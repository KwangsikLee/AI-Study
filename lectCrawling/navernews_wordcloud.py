# -*- coding: utf-8 -*-
"""á„‚á…¦á„‹á…µá„‡á…¥á„‚á…²á„‰á…³_wordcloud.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15JgU0zpG3cBi4cwR120i8A-GdwDp18gM
"""

# # êµ¬ê¸€ ì½”ë©ìš© ë‰´ìŠ¤ í¬ë¡¤ë§ + ì›Œë“œí´ë¼ìš°ë“œ ì˜ˆì œ
# # ì½”ë©ì—ì„œëŠ” ëŒ€ë¶€ë¶„ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤

# # ì›Œë“œí´ë¼ìš°ë“œ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ì½”ë©ì—ì„œ í•œ ë²ˆë§Œ ì‹¤í–‰)
# !pip install wordcloud konlpy
# # ë‚˜ëˆ”ê³ ë”• í°íŠ¸ ì„¤ì¹˜ ë° ì„¤ì •
# !apt-get update -qq
# !apt-get install fonts-nanum -qq
# !fc-cache -fv
# !rm ~/.cache/matplotlib -rf

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import requests
from bs4 import BeautifulSoup
import time
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.font_manager as fm

# ì›Œë“œí´ë¼ìš°ë“œ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from wordcloud import WordCloud
from konlpy.tag import Okt, Kkma
from collections import Counter
import re

# í°íŠ¸ ì„¤ì •
# font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'
# fontprop = fm.FontProperties(fname=font_path, size=10)
# plt.rcParams['font.family'] = 'NanumGothic'
# plt.rcParams['axes.unicode_minus'] = False

# í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic'  # macOS
plt.rcParams['axes.unicode_minus'] = False
font_path='/System/Library/Fonts/AppleGothic.ttf'

def crawl_naver_news():
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”ì¸ í˜ì´ì§€ì—ì„œ í—¤ë“œë¼ì¸ì„ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜"""

    print("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # ì›¹ í˜ì´ì§€ URL
    url = "https://news.naver.com"

    # í—¤ë” ì„¤ì • (ì½”ë© í™˜ê²½ì— ë§ê²Œ ì¡°ì •)
    headers = {
        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    try:
        # ì›¹ í˜ì´ì§€ ìš”ì²­
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        # HTML íŒŒì‹±
        soup = BeautifulSoup(response.text, 'html.parser')

        # ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ì¶”ì¶œ
        headlines = []

        # ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ì°¾ê¸°
        selectors = [
            'a.cjs_news_link',
            'a[href*="/article/"]',
            '.hdline_article_tit',
            '.cluster_text_headline'
        ]

        for selector in selectors:
            items = soup.select(selector)
            if items:
                for item in items[:15]:  # ë” ë§ì€ í—¤ë“œë¼ì¸ ìˆ˜ì§‘
                    title = item.get_text().strip()
                    if title and len(title) > 10:
                        headlines.append(title)
                break

        # ê²°ê³¼ ì¶œë ¥
        print(f"\n ì´ {len(headlines)}ê°œì˜ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
        print("=" * 60)

        for i, headline in enumerate(headlines[:10], 1):
            print(f"{i:2d}. {headline}")

        print("=" * 60)

        return headlines

    except requests.RequestException as e:
        print(f" ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
        return []
    except Exception as e:
        print(f" í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return []

def preprocess_text_for_wordcloud(headlines):
    """ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""

    print(" í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    # ëª¨ë“  í—¤ë“œë¼ì¸ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    full_text = ' '.join(headlines)

    # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
    # íŠ¹ìˆ˜ë¬¸ì, ìˆ«ì, ì˜ì–´ ë“± ì œê±° (í•œê¸€ë§Œ ë‚¨ê¸°ê¸°)
    cleaned_text = re.sub(r'[^ê°€-í£\s]', '', full_text)
    print(f"{cleaned_text}")
    # í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
    okt = Okt()
    print(" í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
    # ëª…ì‚¬ë§Œ ì¶”ì¶œ
    nouns = okt.nouns(cleaned_text)
    print(f" ì´ {len(nouns)}ê°œì˜ ëª…ì‚¬ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
    # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ì˜ë¯¸ì—†ëŠ” ë‹¨ì–´ë“¤)
    stopwords = {
        'ê²ƒ', 'ë“±', 'ë°', 'ì´', 'ê·¸', 'ì €', 'ë”', 'ë˜', 'í•œ', 'ë‘', 'ì„¸', 'ë„¤', 'ë‹¤ì„¯',
        'ìˆë‹¤', 'ì—†ë‹¤', 'ë˜ë‹¤', 'í•˜ë‹¤', 'ì´ë‹¤', 'ì•„ë‹ˆë‹¤', 'ê°™ë‹¤', 'ë‹¤ë¥´ë‹¤',
        'ë•Œë¬¸', 'ìœ„í•´', 'í†µí•´', 'ëŒ€í•´', 'ê´€ë ¨', 'ë”°ë¼', 'ì˜í•´', 'ì—ì„œ', 'ì—ê²Œ', 'ìœ¼ë¡œ', 'ë¡œì„œ',
        'ê¸°ì', 'ë‰´ìŠ¤', 'ë³´ë„', 'ì·¨ì¬', 'ì¸í„°ë·°', 'ë°œí‘œ', 'ë°œì–¸', 'ì–¸ë¡ ', 'ë¯¸ë””ì–´',
        'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ì´ë²ˆ', 'ë‹¤ìŒ', 'ì§€ë‚œ', 'ìµœê·¼', 'í˜„ì¬', 'ì•ìœ¼ë¡œ',
        'ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°', 'ê²½ê¸°', 'ê°•ì›',
        'ë‹¨ë…', 'ì†ë³´', 'ê¸´ê¸‰', 'íŠ¹ë³´', 'í™”ì œ', 'ì´ìŠˆ'
    }

    # 2ê¸€ì ì´ìƒì´ê³  ë¶ˆìš©ì–´ê°€ ì•„ë‹Œ ëª…ì‚¬ë§Œ í•„í„°ë§
    filtered_nouns = [
        noun for noun in nouns
        if len(noun) >= 2 and noun not in stopwords
    ]

    print(f" ì´ {len(filtered_nouns)}ê°œì˜ ìœ ì˜ë¯¸í•œ ëª…ì‚¬ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
    # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
    word_freq = Counter(filtered_nouns)

    print(f" ì´ {len(word_freq)}ê°œì˜ ìœ ì˜ë¯¸í•œ ë‹¨ì–´ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
    print(f" ìƒìœ„ 10ê°œ ë‹¨ì–´: {dict(word_freq.most_common(10))}")

    return word_freq

def create_wordcloud(word_freq, headlines_count):
    """ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""

    if not word_freq:
        print(" ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    print(" ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")

    # ì›Œë“œí´ë¼ìš°ë“œ ì„¤ì •
    wordcloud = WordCloud(
        # font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf',  # ì½”ë© í•œê¸€ í°íŠ¸
        font_path=font_path,  # macOS í•œê¸€ í°íŠ¸
        width=1200,
        height=600,
        background_color='white',
        max_words=100,
        relative_scaling=0.3,
        colormap='viridis',
        random_state=42
    ).generate_from_frequencies(word_freq)

    # ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™”
    plt.figure(figsize=(15, 8))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f' Naver News Word Count (Total {headlines_count})',
              fontsize=16, fontweight='bold', pad=20)

    # ë²”ë¡€ ì¶”ê°€
    plt.figtext(0.5, 0.02,
                f'ìƒì„±ì‹œê°„: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")} | '
                f'ì´ í‚¤ì›Œë“œ: {len(word_freq)}ê°œ | ë°ì´í„° ì¶œì²˜: ë„¤ì´ë²„ ë‰´ìŠ¤',
                ha='center', fontsize=10, style='italic')

    plt.tight_layout()
    plt.show()

    return wordcloud

def create_news_dataframe(headlines):
    """ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜"""

    if not headlines:
        print(" ë¶„ì„í•  í—¤ë“œë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    df = pd.DataFrame({
        'ìˆœë²ˆ': range(1, len(headlines) + 1),
        'í—¤ë“œë¼ì¸': headlines,
        'ê¸€ììˆ˜': [len(headline) for headline in headlines],
        'í¬ë¡¤ë§ì‹œê°„': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')] * len(headlines)
    })

    print("\n ë‰´ìŠ¤ ë°ì´í„° ë¶„ì„:")
    print(f"ì´ ë‰´ìŠ¤ ê°œìˆ˜: {len(df)}")
    print(f"í‰ê·  ê¸€ììˆ˜: {df['ê¸€ììˆ˜'].mean():.1f}")
    print(f"ìµœëŒ€ ê¸€ììˆ˜: {df['ê¸€ììˆ˜'].max()}")
    print(f"ìµœì†Œ ê¸€ììˆ˜: {df['ê¸€ììˆ˜'].min()}")

    return df

def visualize_news_data(df):
    """ë‰´ìŠ¤ ë°ì´í„° ì‹œê°í™”"""

    if df is None or df.empty:
        print(" ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ ì„¤ì •
    plt.style.use('default')
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    # 1. í—¤ë“œë¼ì¸ ê¸€ììˆ˜ ë¶„í¬
    ax1.hist(df['ê¸€ììˆ˜'], bins=10, color='skyblue', alpha=0.7, edgecolor='black')
    ax1.set_title(' news headline Char.', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Char. Cnt')
    ax1.set_ylabel('Freq.')
    ax1.grid(True, alpha=0.3)

    # 2. í—¤ë“œë¼ì¸ë³„ ê¸€ììˆ˜
    ax2.bar(df['ìˆœë²ˆ'], df['ê¸€ììˆ˜'], color='lightcoral', alpha=0.7)
    ax2.set_title(' news headline Char. count', fontsize=14, fontweight='bold')
    ax2.set_xlabel('headline No')
    ax2.set_ylabel('Char Count')
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def show_top_keywords(word_freq, top_n=20):
    """ìƒìœ„ í‚¤ì›Œë“œ í‘œ í˜•íƒœë¡œ ì¶œë ¥"""

    if not word_freq:
        return

    print(f"\n ìƒìœ„ {top_n}ê°œ í‚¤ì›Œë“œ:")
    print("=" * 40)

    for i, (word, count) in enumerate(word_freq.most_common(top_n), 1):
        print(f"{i:2d}. {word:<10} ({count}íšŒ)")

    print("=" * 40)

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    print("ğŸš€ êµ¬ê¸€ ì½”ë© ë‰´ìŠ¤ í¬ë¡¤ë§ + ì›Œë“œí´ë¼ìš°ë“œ ì‹¤ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("=" * 70)

    # 1. ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§
    headlines = crawl_naver_news()

    if not headlines:
        print(" í¬ë¡¤ë§ëœ ë‰´ìŠ¤ê°€ ì—†ì–´ ì‹¤ìŠµì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # 2. ë°ì´í„° ë¶„ì„
    df = create_news_dataframe(headlines)

    if df is not None:
        print("\n ë°ì´í„°í”„ë ˆì„ ë¯¸ë¦¬ë³´ê¸°:")
        print(df.head())

        # 3. ê¸°ë³¸ í†µê³„ ì‹œê°í™”
        visualize_news_data(df)

    # 4. ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    word_freq = preprocess_text_for_wordcloud(headlines)

    # 5. ìƒìœ„ í‚¤ì›Œë“œ ì¶œë ¥
    show_top_keywords(word_freq, top_n=20)

    # 6. ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    wordcloud = create_wordcloud(word_freq, len(headlines))

    # 7. íŒŒì¼ ì €ì¥
    if df is not None:
        # CSV íŒŒì¼ë¡œ ì €ì¥
        df.to_csv('news_headlines_with_wordcloud.csv', index=False, encoding='utf-8')
        print("\n 'news_headlines_with_wordcloud.csv' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

        # í‚¤ì›Œë“œ ë¹ˆë„ CSVë¡œ ì €ì¥
        keyword_df = pd.DataFrame(word_freq.most_common(), columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
        keyword_df.to_csv('keyword_frequency.csv', index=False, encoding='utf-8')
        print(" 'keyword_frequency.csv' íŒŒì¼ë¡œ í‚¤ì›Œë“œ ë¹ˆë„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

        print("   (ì½”ë© ì™¼ìª½ íŒŒì¼ íƒ­ì—ì„œ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)")

    # 8. ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€ ì €ì¥
    if wordcloud:
        wordcloud.to_file('news_wordcloud.png')
        print(" 'news_wordcloud.png' íŒŒì¼ë¡œ ì›Œë“œí´ë¼ìš°ë“œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")


# ê°œë³„ ì‹¤í–‰ í•¨ìˆ˜ë“¤
def run_wordcloud_only():
    """ì›Œë“œí´ë¼ìš°ë“œë§Œ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    headlines = crawl_naver_news()
    if headlines:
        word_freq = preprocess_text_for_wordcloud(headlines)
        show_top_keywords(word_freq)
        create_wordcloud(word_freq, len(headlines))

def run_analysis_only():
    """ê¸°ë³¸ ë¶„ì„ë§Œ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜"""
    headlines = crawl_naver_news()
    if headlines:
        df = create_news_dataframe(headlines)
        if df is not None:
            print(df)
            visualize_news_data(df)

def run_OKT():
    """Okt í˜•íƒœì†Œ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸"""
    okt = Kkma()
    sample_text = "ë„¤ì´ë²„ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ í¬ë¡¤ë§í•˜ê³  ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
    nouns = okt.nouns(sample_text)
    print(f"ì¶”ì¶œëœ ëª…ì‚¬: {nouns}")

# ì½”ë©ì—ì„œ ë°”ë¡œ ì‹¤í–‰
if __name__ == "__main__":
    # ì „ì²´ ì‹¤í–‰
    # main()
    run_OKT()

    # ë˜ëŠ” ê°œë³„ ì‹¤í–‰
    # run_wordcloud_only()  # ì›Œë“œí´ë¼ìš°ë“œë§Œ
    # run_analysis_only()   # ê¸°ë³¸ ë¶„ì„ë§Œ