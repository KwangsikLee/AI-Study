# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜¤ê¸°
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
import streamlit as st
import plotly.express as px

def get_naver_news():
    """ë„¤ì´ë²„ ë‰´ìŠ¤ IT/ê³¼í•™ ì„¹ì…˜ì—ì„œ ê¸°ì‚¬ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""

    print("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ IT/ê³¼í•™ ì„¹ì…˜ì—ì„œ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤...")

    # ë„¤ì´ë²„ ë‰´ìŠ¤ IT/ê³¼í•™ ì„¹ì…˜ URL
    url = "https://news.naver.com/section/105"

    # ì›¹ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ìœ„ì¥í•˜ê¸°
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }

    try:
        print(" ë„¤ì´ë²„ ë‰´ìŠ¤ í˜ì´ì§€ì— ì ‘ì† ì¤‘...")
        response = requests.get(url, headers=headers, timeout=15)

        if response.status_code == 200:
            print(" ë„¤ì´ë²„ ë‰´ìŠ¤ ì ‘ì† ì„±ê³µ!")
        else:
            print(f" ë„¤ì´ë²„ ë‰´ìŠ¤ ì ‘ì† ì‹¤íŒ¨ (HTTP ìƒíƒœì½”ë“œ: {response.status_code})")
            return None

        # HTML ë¶„ì„
        soup = BeautifulSoup(response.text, 'html.parser')
        print(" ë‰´ìŠ¤ í˜ì´ì§€ ë‚´ìš©ì„ ë¶„ì„ ì¤‘...")

        # ë‰´ìŠ¤ ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
        news_data = extract_news_articles(soup)

        if news_data and len(news_data) > 0:
            print(f" {len(news_data)}ê°œì˜ ê¸°ì‚¬ë¥¼ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
            return news_data
        else:
            print(" ê¸°ì‚¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("   - ë„¤ì´ë²„ ë‰´ìŠ¤ í˜ì´ì§€ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print("   - ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return None

    except requests.exceptions.Timeout:
        print(" ë„¤ì´ë²„ ë‰´ìŠ¤ ì ‘ì† ì‹œê°„ ì´ˆê³¼")
        print("   - ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ëŠë¦¬ê±°ë‚˜ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤.")
        return None
    except requests.exceptions.ConnectionError:
        print(" ë„¤ì´ë²„ ë‰´ìŠ¤ ì—°ê²° ì˜¤ë¥˜")
        print("   - ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None
    except Exception as e:
        print(f" ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("   - ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        return None
    
def extract_news_articles(soup):
    """ì›¹í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ"""

    print(" ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì°¾ëŠ” ì¤‘...")

    news_list = []

    try:
        # ë„¤ì´ë²„ ë‰´ìŠ¤ì˜ ë‹¤ì–‘í•œ ê¸°ì‚¬ ì„ íƒìë“¤ ì‹œë„
        article_selectors = [
            # 2024ë…„ ê¸°ì¤€ ë„¤ì´ë²„ ë‰´ìŠ¤ ì„ íƒìë“¤
            'div.sa_item',                          # ì„¹ì…˜ ê¸°ì‚¬ ì•„ì´í…œ
            'div.sa_item_flex',                     # í”Œë ‰ìŠ¤ ê¸°ì‚¬ ì•„ì´í…œ
            'li.sa_item',                           # ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ê¸°ì‚¬
            'div.section_article',                   # ì„¹ì…˜ ê¸°ì‚¬
            'div.news_area',                        # ë‰´ìŠ¤ ì˜ì—­
            'div.list_body li',                     # ë¦¬ìŠ¤íŠ¸ ë³¸ë¬¸ì˜ í•­ëª©ë“¤
            'div.cluster_body li',                  # í´ëŸ¬ìŠ¤í„° ë³¸ë¬¸ í•­ëª©ë“¤
            'article',                              # article íƒœê·¸
            '.sa_item_flex',                        # CSS í´ë˜ìŠ¤
        ]

        found_articles = []

        # ê° ì„ íƒìë¡œ ê¸°ì‚¬ ì°¾ê¸° ì‹œë„
        for i, selector in enumerate(article_selectors):
            print(f"   ì„ íƒì {i+1}: '{selector}' ì‹œë„ ì¤‘...")
            articles = soup.select(selector)

            if articles:
                print(f"    '{selector}'ë¡œ {len(articles)}ê°œ ìš”ì†Œ ë°œê²¬!")
                found_articles = articles
                break
            else:
                print(f"    '{selector}'ë¡œ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í•¨")

        # ëª¨ë“  ì„ íƒì ì‹¤íŒ¨ì‹œ ê¸°ì‚¬ ë§í¬ ì§ì ‘ ê²€ìƒ‰
        if not found_articles:
            print(" ë‹¤ë¥¸ ë°©ë²•: ëª¨ë“  ë§í¬ì—ì„œ ê¸°ì‚¬ ë§í¬ ì°¾ëŠ” ì¤‘...")
            all_links = soup.find_all('a', href=True)
            article_links = []

            for link in all_links:
                href = link.get('href', '')
                if '/article/' in href and link.get_text().strip():
                    article_links.append(link)

            if article_links:
                print(f"    {len(article_links)}ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬!")
                found_articles = article_links
            else:
                print("    ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì§€ ëª»í•¨")

        # ë””ë²„ê¹…: í˜ì´ì§€ êµ¬ì¡° ë¶„ì„
        if not found_articles:
            print("\n í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ ì¤‘...")
            print(f"   ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(soup.get_text())}")

            # ì£¼ìš” div í´ë˜ìŠ¤ë“¤ ì°¾ê¸°
            divs_with_class = soup.find_all('div', class_=True)
            class_names = set()
            for div in divs_with_class[:20]:  # ìƒìœ„ 20ê°œë§Œ
                classes = div.get('class', [])
                for cls in classes:
                    if 'sa_' in cls or 'news' in cls or 'article' in cls or 'section' in cls:
                        class_names.add(cls)

            if class_names:
                print(f"   ë°œê²¬ëœ ê´€ë ¨ í´ë˜ìŠ¤: {list(class_names)[:10]}")

            return []

        # ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
        print(f"ğŸ“ {len(found_articles)}ê°œ ìš”ì†Œì—ì„œ ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ ì¤‘...")

        for i, article in enumerate(found_articles[:30]):  # ìµœëŒ€ 30ê°œ
            try:
                news_info = extract_article_info(article, i + 1)
                if news_info:
                    news_list.append(news_info)
                    print(f"    {i+1}ë²ˆì§¸ ê¸°ì‚¬: {news_info['ì œëª©'][:30]}...")

                # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
                if i > 0 and i % 10 == 0:
                    time.sleep(0.3)

            except Exception as e:
                print(f"    {i+1}ë²ˆì§¸ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

        return news_list

    except Exception as e:
        print(f" ê¸°ì‚¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return []    

def extract_article_info(article, index):
    """ê°œë³„ ê¸°ì‚¬ì—ì„œ ì •ë³´ ì¶”ì¶œ"""

    try:
        # ê¸°ì‚¬ ì œëª© ì°¾ê¸°
        title = ""
        title_selectors = [
            'a.sa_text_title',           # ë„¤ì´ë²„ ë‰´ìŠ¤ ì œëª© ë§í¬
            '.sa_text_title',            # ì œëª© í´ë˜ìŠ¤
            '.sa_text_strong',           # ê°•ì¡° í…ìŠ¤íŠ¸
            'strong.sa_text_strong',     # ê°•ì¡° ì œëª©
            'a[href*="/article/"]',      # ê¸°ì‚¬ ë§í¬
            'a',                         # ì¼ë°˜ ë§í¬
            'strong',                    # ê°•ì¡° íƒœê·¸
        ]

        for selector in title_selectors:
            title_element = article.select_one(selector)
            if title_element:
                title = title_element.get_text().strip()
                if len(title) > 10 and len(title) < 200:  # ì ì ˆí•œ ì œëª© ê¸¸ì´
                    print(f"    ì œëª© ì°¾ìŒ: {title[:30]}...")
                    print(f"    ì„ íƒì: {selector}")
                    break

        # ì œëª©ì´ ì—†ìœ¼ë©´ article ìì²´ê°€ ë§í¬ì¸ ê²½ìš° ì²´í¬
        if not title and article.name == 'a':
            title = article.get_text().strip()

        # ê¸°ì‚¬ ë§í¬ ì°¾ê¸°
        link = ""
        if article.name == 'a' and article.get('href'):
            link = article.get('href')
        else:
            link_element = article.find('a', href=lambda x: x and '/article/' in x)
            if link_element:
                link = link_element.get('href')

        # ìƒëŒ€ ë§í¬ë¥¼ ì ˆëŒ€ ë§í¬ë¡œ ë³€í™˜
        if link and link.startswith('/'):
            link = 'https://news.naver.com' + link
        elif link and not link.startswith('http'):
            link = 'https://news.naver.com' + link

        # ì–¸ë¡ ì‚¬ ì •ë³´ ì°¾ê¸°
        press = ""
        press_selectors = [
            '.sa_text_press',            # ì–¸ë¡ ì‚¬ í´ë˜ìŠ¤
            '.press',                    # ì¼ë°˜ ì–¸ë¡ ì‚¬
            '.source',                   # ì¶œì²˜
            '.sa_text_info_left',        # ì™¼ìª½ ì •ë³´
            '.byline',                   # ë°”ì´ë¼ì¸
        ]

        for selector in press_selectors:
            press_element = article.select_one(selector)
            if press_element:
                press = press_element.get_text().strip()
                if press and len(press) < 50:  # ì ì ˆí•œ ì–¸ë¡ ì‚¬ëª… ê¸¸ì´
                    break

        # ì‹œê°„ ì •ë³´ ì°¾ê¸°
        time_info = ""
        time_selectors = [
            '.sa_text_datetime',         # ì‹œê°„ í´ë˜ìŠ¤
            '.time',                     # ì‹œê°„
            '.date',                     # ë‚ ì§œ
            '.sa_text_info_right',       # ì˜¤ë¥¸ìª½ ì •ë³´
            'time',                      # time íƒœê·¸
        ]

        for selector in time_selectors:
            time_element = article.select_one(selector)
            if time_element:
                time_info = time_element.get_text().strip()
                if time_info and ('ì‹œê°„' in time_info or 'ë¶„' in time_info or 'ì¼' in time_info or ':' in time_info):
                    break

        # ê¸°ì‚¬ ìš”ì•½ì´ë‚˜ ë¶€ì œëª© ì°¾ê¸°
        summary = ""
        summary_selectors = [
            '.sa_text_lede',             # ë¦¬ë“œ ë¬¸ì¥
            '.summary',                  # ìš”ì•½
            '.sub_title',                # ë¶€ì œëª©
            '.description',              # ì„¤ëª…
        ]

        for selector in summary_selectors:
            summary_element = article.select_one(selector)
            if summary_element:
                summary = summary_element.get_text().strip()
                if summary and len(summary) > 10:
                    break

        # ìœ íš¨í•œ ê¸°ì‚¬ì¸ì§€ í™•ì¸
        if title and len(title) > 5 and 'ê´‘ê³ ' not in title:
            return {
                'ìˆœë²ˆ': index,
                'ì œëª©': title[:150],  # ì œëª© ê¸¸ì´ ì œí•œ
                'ì–¸ë¡ ì‚¬': press if press else 'ì •ë³´ì—†ìŒ',
                'ì‹œê°„': time_info if time_info else 'ì •ë³´ì—†ìŒ',
                'ìš”ì•½': summary[:200] if summary else 'ìš”ì•½ì—†ìŒ',
                'ë§í¬': link if link else 'ë§í¬ì—†ìŒ'
            }

        print("not valid article:", title)
        return None

    except Exception as e:
        return None


def save_news_data(news_data):
    """ë‰´ìŠ¤ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""

    if not news_data:
        print(" ì €ì¥í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return

    print(f"\n {len(news_data)}ê°œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤...")

    try:
        # DataFrame ìƒì„±
        df = pd.DataFrame(news_data)

        # CSV íŒŒì¼ ì €ì¥
        df.to_csv('naver_news_it.csv', index=False, encoding='utf-8')

        # í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥
        with open('naver_news_it.txt', 'w', encoding='utf-8') as f:
            f.write("=== ë„¤ì´ë²„ ë‰´ìŠ¤ IT/ê³¼í•™ ì„¹ì…˜ ===\n")
            f.write(f"ìˆ˜ì§‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ì´ ê¸°ì‚¬ ìˆ˜: {len(news_data)}ê°œ\n")
            f.write("=" * 60 + "\n\n")

            for article in news_data:
                f.write(f"[{article['ìˆœë²ˆ']}] {article['ì œëª©']}\n")
                f.write(f"ì–¸ë¡ ì‚¬: {article['ì–¸ë¡ ì‚¬']} | ì‹œê°„: {article['ì‹œê°„']}\n")
                if article['ìš”ì•½'] != 'ìš”ì•½ì—†ìŒ':
                    f.write(f"ìš”ì•½: {article['ìš”ì•½']}\n")
                f.write(f"ë§í¬: {article['ë§í¬']}\n")
                f.write("-" * 50 + "\n\n")

        print(" íŒŒì¼ ì €ì¥ ì™„ë£Œ!")
        print("   - naver_news_it.csv (ì—‘ì…€ìš©)")
        print("   - naver_news_it.txt (í…ìŠ¤íŠ¸ìš©)")

        print(df)

    except Exception as e:
        print(f" íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")


def show_news_data(news_data):
    """ë‰´ìŠ¤ ë°ì´í„°ë¥¼ í™”ë©´ì— ì¶œë ¥"""

    if not news_data:
        print(" ì¶œë ¥í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return

    print(f"\n" + "="*80)
    print("  ë„¤ì´ë²„ ë‰´ìŠ¤ IT/ê³¼í•™ ì„¹ì…˜ - ìµœì‹  ê¸°ì‚¬  ")
    print(f"ìˆ˜ì§‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ì´ {len(news_data)}ê°œ ê¸°ì‚¬")
    print("="*80)

    for article in news_data:
        print(f"\n[{article['ìˆœë²ˆ']}] {article['ì œëª©']}")
        print(f"    {article['ì–¸ë¡ ì‚¬']} |  {article['ì‹œê°„']}")

        if article['ìš”ì•½'] != 'ìš”ì•½ì—†ìŒ' and len(article['ìš”ì•½']) > 10:
            print(f"    {article['ìš”ì•½']}")

        if article['ë§í¬'] != 'ë§í¬ì—†ìŒ':
            print(f"    {article['ë§í¬']}")

        print("-" * 70)

    print("\n" + "="*80)

def analyze_news_data(news_data):
    """ë‰´ìŠ¤ ë°ì´í„° ê°„ë‹¨ ë¶„ì„"""

    if not news_data:
        return

    print(f"\n ë‰´ìŠ¤ ë°ì´í„° ë¶„ì„ ê²°ê³¼:")

    # ì–¸ë¡ ì‚¬ë³„ ê¸°ì‚¬ ìˆ˜
    press_count = {}
    for article in news_data:
        press = article.get('ì–¸ë¡ ì‚¬', 'ì•Œìˆ˜ì—†ìŒ')
        if press != 'ì •ë³´ì—†ìŒ':
            press_count[press] = press_count.get(press, 0) + 1

    if press_count:
        print(f"    ì°¸ì—¬ ì–¸ë¡ ì‚¬: {len(press_count)}ê°œ")
        top_press = sorted(press_count.items(), key=lambda x: x[1], reverse=True)[:5]
        for press, count in top_press:
            print(f"      - {press}: {count}ê°œ ê¸°ì‚¬")

    # ì‹œê°„ ì •ë³´ê°€ ìˆëŠ” ê¸°ì‚¬
    time_articles = [a for a in news_data if a.get('ì‹œê°„', 'ì •ë³´ì—†ìŒ') != 'ì •ë³´ì—†ìŒ']
    print(f"    ì‹œê°„ ì •ë³´ ìˆëŠ” ê¸°ì‚¬: {len(time_articles)}ê°œ")

    # ìš”ì•½ì´ ìˆëŠ” ê¸°ì‚¬
    summary_articles = [a for a in news_data if a.get('ìš”ì•½', 'ìš”ì•½ì—†ìŒ') != 'ìš”ì•½ì—†ìŒ']
    print(f"    ìš”ì•½ì´ ìˆëŠ” ê¸°ì‚¬: {len(summary_articles)}ê°œ")

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    """í”„ë¡œê·¸ë¨ì˜ ë©”ì¸ í•¨ìˆ˜"""

    print(" ë„¤ì´ë²„ ë‰´ìŠ¤ IT/ê³¼í•™ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("=" * 60)
    print(" ëŒ€ìƒ ì‚¬ì´íŠ¸: https://news.naver.com/section/105")
    print(" ëª©í‘œ: ì‹¤ì œ IT/ê³¼í•™ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘")
    print("=" * 60)

    # ë‰´ìŠ¤ ë°ì´í„° í¬ë¡¤ë§
    news_data = get_naver_news()

    if news_data:
        # ë°ì´í„° ë¶„ì„
        analyze_news_data(news_data)

        # ë°ì´í„° í™”ë©´ ì¶œë ¥
        show_news_data(news_data)

        # ë°ì´í„° íŒŒì¼ ì €ì¥
        save_news_data(news_data)

        print(f"\n ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f" ì´ {len(news_data)}ê°œì˜ IT/ê³¼í•™ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")

    else:
        print("\n ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨!")


# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_news_crawling():
    """ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""

    print(" ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤...\n")

    news_data = get_naver_news()

    if news_data:
        print(" í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print(f" ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(news_data)}")
        print(" ì²« 3ê°œ ê¸°ì‚¬ ì œëª©:")
        for i, article in enumerate(news_data[:3]):
            print(f"   {i+1}. {article['ì œëª©'][:50]}...")
    else:
        print(" í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")

    return news_data

# í”„ë¡œê·¸ë¨ ì‹¤í–‰
if __name__ == "__main__":
    main()
