{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FXPM4TUE665x"
      },
      "outputs": [],
      "source": [
        "# 네이버 뉴스 IT/과학 섹션 크롤링\n",
        "# 목표: 네이버 뉴스에서 실제 IT/과학 기사 정보만 가져오기\n",
        "\n",
        "# 필요한 라이브러리 가져오기\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9f3TzgdM69bL"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_naver_news():\n",
        "    \"\"\"네이버 뉴스 IT/과학 섹션에서 기사 정보를 가져오는 함수\"\"\"\n",
        "\n",
        "    print(\"📰 네이버 뉴스 IT/과학 섹션에서 기사를 가져옵니다...\")\n",
        "\n",
        "    # 네이버 뉴스 IT/과학 섹션 URL\n",
        "    url = \"https://news.naver.com/section/105\"\n",
        "\n",
        "    # 웹 브라우저처럼 위장하기\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "        'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',\n",
        "        'Connection': 'keep-alive',\n",
        "        'Upgrade-Insecure-Requests': '1',\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        print(\" 네이버 뉴스 페이지에 접속 중...\")\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            print(\" 네이버 뉴스 접속 성공!\")\n",
        "        else:\n",
        "            print(f\" 네이버 뉴스 접속 실패 (HTTP 상태코드: {response.status_code})\")\n",
        "            return None\n",
        "\n",
        "        # HTML 분석\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        print(\" 뉴스 페이지 내용을 분석 중...\")\n",
        "\n",
        "        # 뉴스 기사 정보 추출\n",
        "        news_data = extract_news_articles(soup)\n",
        "\n",
        "        if news_data and len(news_data) > 0:\n",
        "            print(f\" {len(news_data)}개의 기사를 성공적으로 수집했습니다!\")\n",
        "            return news_data\n",
        "        else:\n",
        "            print(\" 기사 정보를 찾을 수 없습니다.\")\n",
        "            print(\"   - 네이버 뉴스 페이지 구조가 변경되었을 수 있습니다.\")\n",
        "            print(\"   - 네트워크 연결을 확인해주세요.\")\n",
        "            return None\n",
        "\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(\" 네이버 뉴스 접속 시간 초과\")\n",
        "        print(\"   - 네트워크 연결이 느리거나 불안정합니다.\")\n",
        "        return None\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\" 네이버 뉴스 연결 오류\")\n",
        "        print(\"   - 인터넷 연결을 확인해주세요.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\" 예상치 못한 오류 발생: {e}\")\n",
        "        print(\"   - 나중에 다시 시도해주세요.\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KAPsLn-M6_t8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_news_articles(soup):\n",
        "    \"\"\"웹페이지에서 뉴스 기사 정보 추출\"\"\"\n",
        "\n",
        "    print(\" 뉴스 기사를 찾는 중...\")\n",
        "\n",
        "    news_list = []\n",
        "\n",
        "    try:\n",
        "        # 네이버 뉴스의 다양한 기사 선택자들 시도\n",
        "        article_selectors = [\n",
        "            # 2024년 기준 네이버 뉴스 선택자들\n",
        "            'div.sa_item',                          # 섹션 기사 아이템\n",
        "            'div.sa_item_flex',                     # 플렉스 기사 아이템\n",
        "            'li.sa_item',                           # 리스트 형태 기사\n",
        "            'div.section_article',                   # 섹션 기사\n",
        "            'div.news_area',                        # 뉴스 영역\n",
        "            'div.list_body li',                     # 리스트 본문의 항목들\n",
        "            'div.cluster_body li',                  # 클러스터 본문 항목들\n",
        "            'article',                              # article 태그\n",
        "            '.sa_item_flex',                        # CSS 클래스\n",
        "        ]\n",
        "\n",
        "        found_articles = []\n",
        "\n",
        "        # 각 선택자로 기사 찾기 시도\n",
        "        for i, selector in enumerate(article_selectors):\n",
        "            print(f\"   선택자 {i+1}: '{selector}' 시도 중...\")\n",
        "            articles = soup.select(selector)\n",
        "\n",
        "            if articles:\n",
        "                print(f\"    '{selector}'로 {len(articles)}개 요소 발견!\")\n",
        "                found_articles = articles\n",
        "                break\n",
        "            else:\n",
        "                print(f\"    '{selector}'로 요소를 찾지 못함\")\n",
        "\n",
        "        # 모든 선택자 실패시 기사 링크 직접 검색\n",
        "        if not found_articles:\n",
        "            print(\" 다른 방법: 모든 링크에서 기사 링크 찾는 중...\")\n",
        "            all_links = soup.find_all('a', href=True)\n",
        "            article_links = []\n",
        "\n",
        "            for link in all_links:\n",
        "                href = link.get('href', '')\n",
        "                if '/article/' in href and link.get_text().strip():\n",
        "                    article_links.append(link)\n",
        "\n",
        "            if article_links:\n",
        "                print(f\"    {len(article_links)}개의 기사 링크 발견!\")\n",
        "                found_articles = article_links\n",
        "            else:\n",
        "                print(\"    기사 링크를 찾지 못함\")\n",
        "\n",
        "        # 디버깅: 페이지 구조 분석\n",
        "        if not found_articles:\n",
        "            print(\"\\n 페이지 구조 분석 중...\")\n",
        "            print(f\"   전체 텍스트 길이: {len(soup.get_text())}\")\n",
        "\n",
        "            # 주요 div 클래스들 찾기\n",
        "            divs_with_class = soup.find_all('div', class_=True)\n",
        "            class_names = set()\n",
        "            for div in divs_with_class[:20]:  # 상위 20개만\n",
        "                classes = div.get('class', [])\n",
        "                for cls in classes:\n",
        "                    if 'sa_' in cls or 'news' in cls or 'article' in cls or 'section' in cls:\n",
        "                        class_names.add(cls)\n",
        "\n",
        "            if class_names:\n",
        "                print(f\"   발견된 관련 클래스: {list(class_names)[:10]}\")\n",
        "\n",
        "            return []\n",
        "\n",
        "        # 기사 정보 추출\n",
        "        print(f\"📝 {len(found_articles)}개 요소에서 기사 정보 추출 중...\")\n",
        "\n",
        "        for i, article in enumerate(found_articles[:30]):  # 최대 30개\n",
        "            try:\n",
        "                news_info = extract_article_info(article, i + 1)\n",
        "                if news_info:\n",
        "                    news_list.append(news_info)\n",
        "                    print(f\"    {i+1}번째 기사: {news_info['title'][:30]}...\")\n",
        "\n",
        "                # 서버 부하 방지를 위한 딜레이\n",
        "                if i > 0 and i % 10 == 0:\n",
        "                    time.sleep(0.3)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    {i+1}번째 기사 처리 중 오류: {e}\")\n",
        "                continue\n",
        "\n",
        "        return news_list\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" 기사 추출 중 오류: {e}\")\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wmNrQ0iu7C3V"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_article_info(article, index):\n",
        "    \"\"\"개별 기사에서 정보 추출\"\"\"\n",
        "\n",
        "    try:\n",
        "        # 기사 제목 찾기\n",
        "        title = \"\"\n",
        "        title_selectors = [\n",
        "            'a.sa_text_title',           # 네이버 뉴스 제목 링크\n",
        "            '.sa_text_title',            # 제목 클래스\n",
        "            '.sa_text_strong',           # 강조 텍스트\n",
        "            'strong.sa_text_strong',     # 강조 제목\n",
        "            'a[href*=\"/article/\"]',      # 기사 링크\n",
        "            'a',                         # 일반 링크\n",
        "            'strong',                    # 강조 태그\n",
        "        ]\n",
        "\n",
        "        for selector in title_selectors:\n",
        "            title_element = article.select_one(selector)\n",
        "            if title_element:\n",
        "                title = title_element.get_text().strip()\n",
        "                if len(title) > 10 and len(title) < 200:  # 적절한 제목 길이\n",
        "                    break\n",
        "\n",
        "        # 제목이 없으면 article 자체가 링크인 경우 체크\n",
        "        if not title and article.name == 'a':\n",
        "            title = article.get_text().strip()\n",
        "\n",
        "        # 기사 링크 찾기\n",
        "        link = \"\"\n",
        "        if article.name == 'a' and article.get('href'):\n",
        "            link = article.get('href')\n",
        "        else:\n",
        "            link_element = article.find('a', href=lambda x: x and '/article/' in x)\n",
        "            if link_element:\n",
        "                link = link_element.get('href')\n",
        "\n",
        "        # 상대 링크를 절대 링크로 변환\n",
        "        if link and link.startswith('/'):\n",
        "            link = 'https://news.naver.com' + link\n",
        "        elif link and not link.startswith('http'):\n",
        "            link = 'https://news.naver.com' + link\n",
        "\n",
        "        # 언론사 정보 찾기\n",
        "        press = \"\"\n",
        "        press_selectors = [\n",
        "            '.sa_text_press',            # 언론사 클래스\n",
        "            '.press',                    # 일반 언론사\n",
        "            '.source',                   # 출처\n",
        "            '.sa_text_info_left',        # 왼쪽 정보\n",
        "            '.byline',                   # 바이라인\n",
        "        ]\n",
        "\n",
        "        for selector in press_selectors:\n",
        "            press_element = article.select_one(selector)\n",
        "            if press_element:\n",
        "                press = press_element.get_text().strip()\n",
        "                if press and len(press) < 50:  # 적절한 언론사명 길이\n",
        "                    break\n",
        "\n",
        "        # 시간 정보 찾기\n",
        "        time_info = \"\"\n",
        "        time_selectors = [\n",
        "            '.sa_text_datetime',         # 시간 클래스\n",
        "            '.time',                     # 시간\n",
        "            '.date',                     # 날짜\n",
        "            '.sa_text_info_right',       # 오른쪽 정보\n",
        "            'time',                      # time 태그\n",
        "        ]\n",
        "\n",
        "        for selector in time_selectors:\n",
        "            time_element = article.select_one(selector)\n",
        "            if time_element:\n",
        "                time_info = time_element.get_text().strip()\n",
        "                if time_info and ('시간' in time_info or '분' in time_info or '일' in time_info or ':' in time_info):\n",
        "                    break\n",
        "\n",
        "        # 기사 요약이나 부제목 찾기\n",
        "        summary = \"\"\n",
        "        summary_selectors = [\n",
        "            '.sa_text_lede',             # 리드 문장\n",
        "            '.summary',                  # 요약\n",
        "            '.sub_title',                # 부제목\n",
        "            '.description',              # 설명\n",
        "        ]\n",
        "\n",
        "        for selector in summary_selectors:\n",
        "            summary_element = article.select_one(selector)\n",
        "            if summary_element:\n",
        "                summary = summary_element.get_text().strip()\n",
        "                if summary and len(summary) > 10:\n",
        "                    break\n",
        "\n",
        "        # 유효한 기사인지 확인\n",
        "        if title and len(title) > 5 and '광고' not in title:\n",
        "            return {\n",
        "                '순번': index,\n",
        "                '제목': title[:150],  # 제목 길이 제한\n",
        "                '언론사': press if press else '정보없음',\n",
        "                '시간': time_info if time_info else '정보없음',\n",
        "                '요약': summary[:200] if summary else '요약없음',\n",
        "                '링크': link if link else '링크없음'\n",
        "            }\n",
        "\n",
        "        print(\"not valid article:\", title)\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "u26F9bLG7GJO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def save_news_data(news_data):\n",
        "    \"\"\"뉴스 데이터를 파일로 저장\"\"\"\n",
        "\n",
        "    if not news_data:\n",
        "        print(\" 저장할 뉴스 데이터가 없습니다\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n {len(news_data)}개 뉴스 기사를 파일로 저장합니다...\")\n",
        "\n",
        "    try:\n",
        "        # DataFrame 생성\n",
        "        df = pd.DataFrame(news_data)\n",
        "\n",
        "        # CSV 파일 저장\n",
        "        df.to_csv('naver_news_it.csv', index=False, encoding='utf-8')\n",
        "\n",
        "        # 텍스트 파일 저장\n",
        "        with open('naver_news_it.txt', 'w', encoding='utf-8') as f:\n",
        "            f.write(\"=== 네이버 뉴스 IT/과학 섹션 ===\\n\")\n",
        "            f.write(f\"수집 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"총 기사 수: {len(news_data)}개\\n\")\n",
        "            f.write(\"=\" * 60 + \"\\n\\n\")\n",
        "\n",
        "            for article in news_data:\n",
        "                f.write(f\"[{article['순번']}] {article['제목']}\\n\")\n",
        "                f.write(f\"언론사: {article['언론사']} | 시간: {article['시간']}\\n\")\n",
        "                if article['요약'] != '요약없음':\n",
        "                    f.write(f\"요약: {article['요약']}\\n\")\n",
        "                f.write(f\"링크: {article['링크']}\\n\")\n",
        "                f.write(\"-\" * 50 + \"\\n\\n\")\n",
        "\n",
        "        print(\" 파일 저장 완료!\")\n",
        "        print(\"   - naver_news_it.csv (엑셀용)\")\n",
        "        print(\"   - naver_news_it.txt (텍스트용)\")\n",
        "\n",
        "        print(df)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" 파일 저장 중 오류: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrxVuhUus3zH",
        "outputId": "4e8c15b4-11f3-48a6-eb0a-e8b628338ef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 네이버 뉴스 IT/과학 크롤링을 시작합니다!\n",
            "============================================================\n",
            " 대상 사이트: https://news.naver.com/section/105\n",
            " 목표: 실제 IT/과학 뉴스 기사 수집\n",
            "============================================================\n",
            "📰 네이버 뉴스 IT/과학 섹션에서 기사를 가져옵니다...\n",
            " 네이버 뉴스 페이지에 접속 중...\n",
            " 네이버 뉴스 접속 성공!\n",
            " 뉴스 페이지 내용을 분석 중...\n",
            " 뉴스 기사를 찾는 중...\n",
            "   선택자 1: 'div.sa_item' 시도 중...\n",
            "    'div.sa_item'로 요소를 찾지 못함\n",
            "   선택자 2: 'div.sa_item_flex' 시도 중...\n",
            "    'div.sa_item_flex'로 44개 요소 발견!\n",
            "📝 44개 요소에서 기사 정보 추출 중...\n",
            "    1번째 기사 처리 중 오류: 'title'\n",
            "    2번째 기사 처리 중 오류: 'title'\n",
            "    3번째 기사 처리 중 오류: 'title'\n",
            "    4번째 기사 처리 중 오류: 'title'\n",
            "    5번째 기사 처리 중 오류: 'title'\n",
            "    6번째 기사 처리 중 오류: 'title'\n",
            "    7번째 기사 처리 중 오류: 'title'\n",
            "    8번째 기사 처리 중 오류: 'title'\n",
            "    9번째 기사 처리 중 오류: 'title'\n",
            "    10번째 기사 처리 중 오류: 'title'\n",
            "    11번째 기사 처리 중 오류: 'title'\n",
            "    12번째 기사 처리 중 오류: 'title'\n",
            "    13번째 기사 처리 중 오류: 'title'\n",
            "    14번째 기사 처리 중 오류: 'title'\n",
            "    15번째 기사 처리 중 오류: 'title'\n",
            "    16번째 기사 처리 중 오류: 'title'\n",
            "    17번째 기사 처리 중 오류: 'title'\n",
            "    18번째 기사 처리 중 오류: 'title'\n",
            "    19번째 기사 처리 중 오류: 'title'\n",
            "    20번째 기사 처리 중 오류: 'title'\n",
            "    21번째 기사 처리 중 오류: 'title'\n",
            "    22번째 기사 처리 중 오류: 'title'\n",
            "    23번째 기사 처리 중 오류: 'title'\n",
            "    24번째 기사 처리 중 오류: 'title'\n",
            "    25번째 기사 처리 중 오류: 'title'\n",
            "    26번째 기사 처리 중 오류: 'title'\n",
            "    27번째 기사 처리 중 오류: 'title'\n",
            "    28번째 기사 처리 중 오류: 'title'\n",
            "    29번째 기사 처리 중 오류: 'title'\n",
            "    30번째 기사 처리 중 오류: 'title'\n",
            " 30개의 기사를 성공적으로 수집했습니다!\n",
            "\n",
            " 뉴스 데이터 분석 결과:\n",
            "    참여 언론사: 18개\n",
            "      - 지디넷코리아: 4개 기사\n",
            "      - 뉴시스: 3개 기사\n",
            "      - 서울경제: 3개 기사\n",
            "      - 디지털데일리: 2개 기사\n",
            "      - 한국경제: 2개 기사\n",
            "    시간 정보 있는 기사: 30개\n",
            "    요약이 있는 기사: 30개\n",
            "\n",
            "================================================================================\n",
            "  네이버 뉴스 IT/과학 섹션 - 최신 기사  \n",
            "수집 시간: 2025-06-30 10:31:59\n",
            "총 30개 기사\n",
            "================================================================================\n",
            "\n",
            "[1] 위메이드맥스, '미드나잇 워커스' 신규 트레일러 공개\n",
            "    지디넷코리아 |  5\n",
            "개의 관련뉴스 더보기\n",
            "    위메이드맥스(각자대표 손면석·이길형)는 원웨이티켓스튜디오(대표 송광호)가 개발 중인 PC·콘솔 PvPvE 익스트랙션 신작 '미드나잇 워커스'의 첫번째 시네마틱 트레일러를 공개했다고 30일 밝혔다. 이번 영상에서는 생\n",
            "    https://n.news.naver.com/mnews/article/092/0002380115\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[2] 배달앱 이용자 절반 이상 2개 이상 플랫폼 사용 '멀티호밍'\n",
            "    SBS |  20\n",
            "개의 관련뉴스 더보기\n",
            "    ▲ 음식 배달 자료화면 음식 배달 플랫폼 이용자의 절반 이상이 2개 이상의 플랫폼을 쓰는 '멀티호밍'을 하는 것으로 조사됐습니다. 과학기술정보통신부는 지난 2024년 부가통신사업 실태조사를 통해 음식 배달 플랫폼 이\n",
            "    https://n.news.naver.com/mnews/article/055/0001270677\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[3] \"AI가 음성 받아쓰고 보정·요약까지\"…SKT, 에이닷 베타 서비스 출시\n",
            "    뉴시스 |  19\n",
            "개의 관련뉴스 더보기\n",
            "    SK텔레콤은 인공지능(AI) 서비스 '에이닷(A.)'에 '노트'와 '브리핑' 등 신규 서비스의 베타 버전을 출시했다고 30일 밝혔다. 이번에 선보이는 노트와 브리핑은 고객들이 생활속에서 자주 필요로 하는 생활 밀착형\n",
            "    https://n.news.naver.com/mnews/article/003/0013332538\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[4] 과기정통부 2차관에 류제명 실장 임명...\"수석·장관과 AI 정책 이끌 적임자\"\n",
            "    디지털데일리 |  20\n",
            "개의 관련뉴스 더보기\n",
            "    이재명 대통이 류제명 네트워크정책실장을 과학기술정보통신부(이하 과기정통부) 2차관으로 임명했다. 29일 강유정 대통령실 대변인은 차관 인사 관련 서면 브리핑을 통해 초기 내각 인선 소식을 전했다. 강 대변인은 \"류제\n",
            "    https://n.news.naver.com/mnews/article/138/0002199598\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[5] 네이버웹툰, 7월부터 프랑스 고속철도 '테제베'에 전용 콘텐츠 제공\n",
            "    서울경제 |  14\n",
            "개의 관련뉴스 더보기\n",
            "    네이버웹툰이 7월 1일부터 프랑스 고속열차 테제베 이누이(TGV INOUI)와 저가 고속철 위고(OUIGO)에 오리지널 웹툰 15편을 전용 콘텐츠로 제공한다. 네이버웹툰은 30일 프랑스 고속열차 전용 엔터테인먼트 플\n",
            "    https://n.news.naver.com/mnews/article/011/0004502926\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[6] KT, 계명대·MS와 지역 디지털 인프라 양성 협력\n",
            "    조선비즈 |  12\n",
            "개의 관련뉴스 더보기\n",
            "    KT가 계명대학교·한국 마이크로소프트와 지역 디지털 인재를 양성하기 위한 인프라 마련에 나선다. KT는 30일 대구 계명대학교에서 디지털 교육·연구 플랫폼 ‘K-MIND 센터’ 설립을 위한 업무협약을 체결했다고 밝혔\n",
            "    https://n.news.naver.com/mnews/article/366/0001088613\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[7] 메타, 오픈AI 핵심 인재 빼간다…\"저커버그가 직접 DM으로 연락\"\n",
            "    한국경제 |  26\n",
            "개의 관련뉴스 더보기\n",
            "    메타가 거액을 주고 오픈 AI 연구원을 연달아 영입하고 있다. 샘 올트먼 오픈AI 최고경영자(CEO)가 메타가 오픈AI 직원들에게 거액을 주겠다며 이직을 제안했지만 아무도 회사를 떠나지 않았다고 말한지 약 일주일 만\n",
            "    https://n.news.naver.com/mnews/article/015/0005151036\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[8] 내년 공휴일 70일...주5일제 근무자 휴일은 118일\n",
            "    YTN |  18\n",
            "개의 관련뉴스 더보기\n",
            "    내년 실제 공휴일은 총 70일로 올해보다 2일이 늘어납니다. 우주항공청이 발표한 2025년도 '월력요항'을 보면 2026년은 일요일이 52일이고 국경일과 설날, 대체공휴일 등 20일의 공휴일을 더해 72일이 되지만,\n",
            "    https://n.news.naver.com/mnews/article/052/0002212498\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[9] LG CNS, 스테이블코인 관련 기술 내재화... 소버린AI 기대감 - IM증권\n",
            "    머니투데이 |  18분전\n",
            "    LG CNS(LG씨엔에스)가 이재명정부가 주도하는 '소버린 AI'(주권 인공지능) 분야에서 수혜를 볼 것이라는 시장의 분석이 나왔다. LG그룹의 LLM(거대언어모델) 엑사원을 활용한 서비스 등으로 경쟁력을 높여갈 것\n",
            "    https://n.news.naver.com/mnews/article/008/0005214489\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[10] “李 대통령 욕하면 카톡 정지?”…‘대화 검열’ 실험하는 사람들\n",
            "    이데일리 |  18분전\n",
            "    카카오가 지난 16일부터 테러·음모·선동 등 극단적인 콘텐츠를 제재하는 내용을 담은 새 운영정책을 시행하는 것과 관련해 불거졌던 검열 논란이 쉽사리 식지 않고 있다. 검열 테스트용 카카오톡 오픈채팅방이 개설되고 ‘채\n",
            "    https://n.news.naver.com/mnews/article/018/0006052701\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[11] 내년 달력 '빨간 날' 70일…실질 휴일은 올해보다 하루 줄어\n",
            "    뉴시스 |  19분전\n",
            "    2026년 공휴일은 올해보다 2일 늘어난 70일이다. 주5일제 근무자 기준 실질 휴일은 118일로 올해보다 하루 줄어든다. 토요일과 겹치는 공휴일이 4일에 달하는 탓이다. 우주항공청은 2026년도(단기 4359년)\n",
            "    https://n.news.naver.com/mnews/article/003/0013332698\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[12] 비서다운 비서 나오나…SKT '에이닷' 신규 서비스 뭐길래\n",
            "    한국경제 |  19분전\n",
            "    SK텔레콤이 실시간으로 회의록을 정리하거나 사용자의 일상을 분석해 정보를 제공하는 생활밀착형 인공지능(AI) 서비스 두 가지를 '에이닷'에 제공한다. SK텔레콤은 30일 AI 서비스 에이닷에 '노트'와 '브리핑' 등\n",
            "    https://n.news.naver.com/mnews/article/015/0005151258\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[13] 내년 '쉬는 날' 올해보다 하루 적은 118일, 설 연휴 5일 가장 길어\n",
            "    아시아경제 |  28분전\n",
            "    2026년 달력에 붉은색으로 표시된 공휴일은 총 70일로 올해보다 2일이 늘어난다. 그러나 주 5일제 근로자들의 총 휴일 수는 118일로 올해보다 하루를 덜 쉬게 된다. 52일의 일요일과 국경일, 설날, 대체공휴일\n",
            "    https://n.news.naver.com/mnews/article/277/0005615040\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[14] 국가 AI 전략 앞세운 中…바이두, '어니 5.0'으로 챗GPT 넘본다\n",
            "    지디넷코리아 |  33분전\n",
            "    바이두가 차세대 거대언어모델(LLM)을 연이어 공개하며 중국 인공지능(AI) 산업이 딥시크에 이어 다시 한 번 전 세계의 주목을 받고 있다. 이는 단순 기술 발표를 넘어 미·중 AI 주도권 경쟁을 심화시킬 것으로 전\n",
            "    https://n.news.naver.com/mnews/article/092/0002380112\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[15] 류제명 과기정통부 2차관 누구…'ICT·AI' 행정 전문가\n",
            "    블로터 |  33분전\n",
            "    이달 29일 신임 과학기술정보통신부 제2차관으로 임명된 류제명 전 과기정통부 네트워크정책실장은 정보통신기술(ICT)과 인공지능(AI) 분야를 두루 경험한 정책 베테랑이다. 민간 출신 전문가인 배경훈 과기정통부 장관\n",
            "    https://n.news.naver.com/mnews/article/293/0000069207\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[16] \"더 이상 안 빼앗긴다\"…오픈AI, 메타에 AI 인재 유출 막으려 보상 체계 조정\n",
            "    지디넷코리아 |  35분전\n",
            "    메타가 최대 1억 달러(약 1천400억원)에 달하는 조건을 제시하며 우수 인공지능(AI) 인재 영입에 혈안이 된 가운데 오픈AI가 제동을 걸고 나섰다. 보상 체계를 재조정해 이탈 인력을 막기 위해 나선 것이다. 30\n",
            "    https://n.news.naver.com/mnews/article/092/0002380111\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[17] SK바이오사이언스, 경북 안동 폐렴구균 백신 생산시설 준공\n",
            "    디지털타임스 |  35분전\n",
            "    SK바이오사이언스는 경북 안동 백신 생산공장 ‘L HOUSE’에서 폐렴구균 백신 생산시설의 증축을 축하하는 준공식을 개최했다고 30일 밝혔다. 증축된 시설은 글로벌 제약사 사노피와 공동 개발 중인 21가 폐렴구균 백\n",
            "    https://n.news.naver.com/mnews/article/029/0002964758\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[18] \"화성 표면 1m 아래에 얼음\"…기지 최적 장소 찾았다 [우주로 간다]\n",
            "    지디넷코리아 |  39분전\n",
            "    붉은 행성 ‘화성’에서 수심 1m 미만 얼음이 존재할 가능성이 높은 지역이 발견됐다고 BGR 등 외신들이 최근 보도했다. 이번 연구 결과는 ‘지구 물리학연구저널-행성편(Journal of Geophysical Res\n",
            "    https://n.news.naver.com/mnews/article/092/0002380108\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[19] “직장인들 신나겠네” 황금연휴 몰려온다…내년 3일 이상만 8번, 휴일 118일\n",
            "    헤럴드경제 |  48분전\n",
            "    - 우주항공청, 2026년 월력요항 발표 내년 2월 설날에는 5일간의 황금연휴가 찾아온다. 또 3일 연속 연휴도 총 8차례나 된다. 우주항공청은 2026년도(단기 4359년) 우리나라 달력 제작의 기준이 되는 202\n",
            "    https://n.news.naver.com/mnews/article/016/0002492153\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[20] 주 5일제 근로자, 내년 118일 쉰다\n",
            "    서울경제 |  50분전\n",
            "    2026년 공휴일은 총 70일로 올해(68일)보다 2일 늘어난다. 주 5일제를 실시하는 기관의 경우 공휴일70일과 52일의 토요일이 더해져 중복되는 날짜를 제외하면 총 118일의 휴일이 주어진다. 우주항공청은 202\n",
            "    https://n.news.naver.com/mnews/article/011/0004502943\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[21] 애플 3년간 ‘비전·글래스’ 쏟아낸다…7종 XR 기기 로드맵 포착\n",
            "    디지털데일리 |  52분전\n",
            "    애플이 향후 3년간 최소 7종의 헤드마운트(Head-Mounted) 기기 출시를 예고하면서, 스마트 글래스를 중심으로 한 차세대 소비자 전자 생태계 주도권을 노리고 있다는 주장이 제기됐다. 30일 궈밍치 대만TF인터\n",
            "    https://n.news.naver.com/mnews/article/138/0002199612\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[22] 배경훈 과기장관 후보 \"SKT 사태 조사 결과 이번주 내 발표할 듯\"\n",
            "    뉴시스 |  1시간전\n",
            "    배경훈 과학기술정보통신부 장관 후보자는 SK텔레콤 사태 민관합동조사에 대해 \"이번주 내로 (결과를) 발표하는 것으로 알고 있다\"고 밝혔다. 배 후보자는 30일 서울 광화문 우체국에 마련된 인사청문회 사무실로 출근하면\n",
            "    https://n.news.naver.com/mnews/article/003/0013332499\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[23] SKT, 해킹 사태 여파로 브랜드가치 급락.. KT, 이동통신 부문 1위\n",
            "    파이낸셜뉴스 |  1시간전\n",
            "    해킹 사태 여파로 SK텔레콤의 브랜드가치가 큰 폭으로 하락한 것으로 나타났다. 30일 브랜드가치 평가회사 브랜드스탁의 '2025년 2분기 대한민국 100대 브랜드' 발표에 따르면 SKT의 순위는 40위로, 전 분기의\n",
            "    https://n.news.naver.com/mnews/article/014/0005369864\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[24] 출시 15주년 맞은 배민, 주문수 65억건·거래액 153조원 돌파\n",
            "    헤럴드경제 |  1시간전\n",
            "    월간 주문 건수 9년 만에 50배 성장 입점 업체 매출도 가파르게 증가 25일 전사 행사 열고 새 미션 발표 “대체 불가능한 플랫폼으로 거듭날 것” 김범석 우아한형제들 대표이사 [우아한형제들 제공] [헤럴드경제=권제\n",
            "    https://n.news.naver.com/mnews/article/016/0002492111\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[25] 오픈AI, 메타 인력 빼가기에 \"임원 24시간 대기\"로 정면 대응\n",
            "    서울경제 |  1시간전\n",
            "    최근 메타의 ‘인력 빼가기’에 핵심 연구자들을 잃은 오픈AI가 ‘정면 대응’에 나섰다. 메타를 비롯한 외부 영입 제안이 온다면 샘 올트먼 최고경영자(CEO)를 포함한 경영진이 24시간 응대해 보상을 조정해주겠다는 것\n",
            "    https://n.news.naver.com/mnews/article/011/0004502920\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[26] 여행 후 샤워해도 소용없다…안 닦은 캐리어, 변기보다 세균 58배\n",
            "    디지털타임스 |  1시간전\n",
            "    여행지에서 돌아온 뒤 캐리어를 침대나 거실에 그대로 올리는 행동이 세균을 집 안까지 퍼뜨릴 수 있다는 경고가 나왔다. 영국 데일리메일에 따르면 현지의 한 여행 보험사와 미생물학자 에이미 메이 포인터 연구팀이 공동으로\n",
            "    https://n.news.naver.com/mnews/article/029/0002964733\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[27] ‘1.8조 기술수출’ 에이비온, 법차손 리스크까지 두 토끼 잡았다\n",
            "    이데일리 |  1시간전\n",
            "    이 기사는 2025년06월27일 09시05분에 팜이데일리 프리미엄 콘텐츠로 선공개 되었습니다. 표적항암제 및 항체치료제 개발사 에이비온(203400)이 전임상 단계의 신약후보물질로 최대 1조8000억원 규모의 기술수\n",
            "    https://n.news.naver.com/mnews/article/018/0006052594\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[28] 사상 최고치 찍자, 젠슨 황과 엔비디아 임원진 10억달러 주식 현금화\n",
            "    조선일보 |  1시간전\n",
            "    지난 1년간 엔비디아 임원진들이 10억 달러 규모의 주식을 매도했다. 이 가운데 절반 이상은 주가가 고공행진한 최근 몇 주간 집중적으로 이뤄졌다. 29일(현지 시각) 영국 파이낸셜타임스(FT)에 따르면 젠슨 황 엔비\n",
            "    https://n.news.naver.com/mnews/article/023/0003914048\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[29] 첫돌 맞은 ‘로드나인’… “‘비정상의 정상화’, 여전히 현재 진행형”\n",
            "    국민일보 |  1시간전\n",
            "    “‘비정상의 정상화’라는 슬로건을 걸었지만 지난 1년간 어려움이 많았습니다. 아직 완전히 정상화가 됐다고 말하긴 어렵지만, 지금도 목표를 향해 나아가는 과정에 있다고 생각합니다.” 김효재 엔엑스쓰리게임즈 PD는 지난\n",
            "    https://n.news.naver.com/mnews/article/005/0001786426\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[30] '로드나인' 김효재 PD \"'비정상의 정상화'로 많은 고통…부족한 모습 많았다\"\n",
            "    아이뉴스24 |  1시간전\n",
            "    \"지난 1년 '비정상의 정상화' 때문에 많은 고통을 겪었습니다. 부족한 모습도 많이 보여드렸지만, 탄탄한 서비스를 이어갈 수 있다는 확신을 가진 1년이었습니다.\" 스마일게이트의 MMORPG '로드나인' 개발을 총괄하\n",
            "    https://n.news.naver.com/mnews/article/031/0000944330\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "\n",
            " 30개 뉴스 기사를 파일로 저장합니다...\n",
            " 파일 저장 완료!\n",
            "   - naver_news_it.csv (엑셀용)\n",
            "   - naver_news_it.txt (텍스트용)\n",
            "    순번                                             제목     언론사  \\\n",
            "0    1                  위메이드맥스, '미드나잇 워커스' 신규 트레일러 공개  지디넷코리아   \n",
            "1    2              배달앱 이용자 절반 이상 2개 이상 플랫폼 사용 '멀티호밍'     SBS   \n",
            "2    3       \"AI가 음성 받아쓰고 보정·요약까지\"…SKT, 에이닷 베타 서비스 출시     뉴시스   \n",
            "3    4   과기정통부 2차관에 류제명 실장 임명...\"수석·장관과 AI 정책 이끌 적임자\"  디지털데일리   \n",
            "4    5          네이버웹툰, 7월부터 프랑스 고속철도 '테제베'에 전용 콘텐츠 제공    서울경제   \n",
            "5    6                   KT, 계명대·MS와 지역 디지털 인프라 양성 협력    조선비즈   \n",
            "6    7          메타, 오픈AI 핵심 인재 빼간다…\"저커버그가 직접 DM으로 연락\"    한국경제   \n",
            "7    8                 내년 공휴일 70일...주5일제 근무자 휴일은 118일     YTN   \n",
            "8    9   LG CNS, 스테이블코인 관련 기술 내재화... 소버린AI 기대감 - IM증권   머니투데이   \n",
            "9   10            “李 대통령 욕하면 카톡 정지?”…‘대화 검열’ 실험하는 사람들    이데일리   \n",
            "10  11             내년 달력 '빨간 날' 70일…실질 휴일은 올해보다 하루 줄어     뉴시스   \n",
            "11  12               비서다운 비서 나오나…SKT '에이닷' 신규 서비스 뭐길래    한국경제   \n",
            "12  13       내년 '쉬는 날' 올해보다 하루 적은 118일, 설 연휴 5일 가장 길어   아시아경제   \n",
            "13  14        국가 AI 전략 앞세운 中…바이두, '어니 5.0'으로 챗GPT 넘본다  지디넷코리아   \n",
            "14  15               류제명 과기정통부 2차관 누구…'ICT·AI' 행정 전문가     블로터   \n",
            "15  16  \"더 이상 안 빼앗긴다\"…오픈AI, 메타에 AI 인재 유출 막으려 보상 체계 조정  지디넷코리아   \n",
            "16  17               SK바이오사이언스, 경북 안동 폐렴구균 백신 생산시설 준공  디지털타임스   \n",
            "17  18        \"화성 표면 1m 아래에 얼음\"…기지 최적 장소 찾았다 [우주로 간다]  지디넷코리아   \n",
            "18  19    “직장인들 신나겠네” 황금연휴 몰려온다…내년 3일 이상만 8번, 휴일 118일   헤럴드경제   \n",
            "19  20                          주 5일제 근로자, 내년 118일 쉰다    서울경제   \n",
            "20  21           애플 3년간 ‘비전·글래스’ 쏟아낸다…7종 XR 기기 로드맵 포착  디지털데일리   \n",
            "21  22         배경훈 과기장관 후보 \"SKT 사태 조사 결과 이번주 내 발표할 듯\"     뉴시스   \n",
            "22  23       SKT, 해킹 사태 여파로 브랜드가치 급락.. KT, 이동통신 부문 1위  파이낸셜뉴스   \n",
            "23  24           출시 15주년 맞은 배민, 주문수 65억건·거래액 153조원 돌파   헤럴드경제   \n",
            "24  25           오픈AI, 메타 인력 빼가기에 \"임원 24시간 대기\"로 정면 대응    서울경제   \n",
            "25  26           여행 후 샤워해도 소용없다…안 닦은 캐리어, 변기보다 세균 58배  디지털타임스   \n",
            "26  27           ‘1.8조 기술수출’ 에이비온, 법차손 리스크까지 두 토끼 잡았다    이데일리   \n",
            "27  28         사상 최고치 찍자, 젠슨 황과 엔비디아 임원진 10억달러 주식 현금화    조선일보   \n",
            "28  29         첫돌 맞은 ‘로드나인’… “‘비정상의 정상화’, 여전히 현재 진행형”    국민일보   \n",
            "29  30   '로드나인' 김효재 PD \"'비정상의 정상화'로 많은 고통…부족한 모습 많았다\"  아이뉴스24   \n",
            "\n",
            "                 시간                                                 요약  \\\n",
            "0    5\\n개의 관련뉴스 더보기  위메이드맥스(각자대표 손면석·이길형)는 원웨이티켓스튜디오(대표 송광호)가 개발 중인...   \n",
            "1   20\\n개의 관련뉴스 더보기  ▲ 음식 배달 자료화면 음식 배달 플랫폼 이용자의 절반 이상이 2개 이상의 플랫폼을...   \n",
            "2   19\\n개의 관련뉴스 더보기  SK텔레콤은 인공지능(AI) 서비스 '에이닷(A.)'에 '노트'와 '브리핑' 등 신...   \n",
            "3   20\\n개의 관련뉴스 더보기  이재명 대통이 류제명 네트워크정책실장을 과학기술정보통신부(이하 과기정통부) 2차관으...   \n",
            "4   14\\n개의 관련뉴스 더보기  네이버웹툰이 7월 1일부터 프랑스 고속열차 테제베 이누이(TGV INOUI)와 저가...   \n",
            "5   12\\n개의 관련뉴스 더보기  KT가 계명대학교·한국 마이크로소프트와 지역 디지털 인재를 양성하기 위한 인프라 마...   \n",
            "6   26\\n개의 관련뉴스 더보기  메타가 거액을 주고 오픈 AI 연구원을 연달아 영입하고 있다. 샘 올트먼 오픈AI ...   \n",
            "7   18\\n개의 관련뉴스 더보기  내년 실제 공휴일은 총 70일로 올해보다 2일이 늘어납니다. 우주항공청이 발표한 2...   \n",
            "8              18분전  LG CNS(LG씨엔에스)가 이재명정부가 주도하는 '소버린 AI'(주권 인공지능) ...   \n",
            "9              18분전  카카오가 지난 16일부터 테러·음모·선동 등 극단적인 콘텐츠를 제재하는 내용을 담은...   \n",
            "10             19분전  2026년 공휴일은 올해보다 2일 늘어난 70일이다. 주5일제 근무자 기준 실질 휴...   \n",
            "11             19분전  SK텔레콤이 실시간으로 회의록을 정리하거나 사용자의 일상을 분석해 정보를 제공하는 ...   \n",
            "12             28분전  2026년 달력에 붉은색으로 표시된 공휴일은 총 70일로 올해보다 2일이 늘어난다....   \n",
            "13             33분전  바이두가 차세대 거대언어모델(LLM)을 연이어 공개하며 중국 인공지능(AI) 산업이...   \n",
            "14             33분전  이달 29일 신임 과학기술정보통신부 제2차관으로 임명된 류제명 전 과기정통부 네트워...   \n",
            "15             35분전  메타가 최대 1억 달러(약 1천400억원)에 달하는 조건을 제시하며 우수 인공지능(...   \n",
            "16             35분전  SK바이오사이언스는 경북 안동 백신 생산공장 ‘L HOUSE’에서 폐렴구균 백신 생...   \n",
            "17             39분전  붉은 행성 ‘화성’에서 수심 1m 미만 얼음이 존재할 가능성이 높은 지역이 발견됐다...   \n",
            "18             48분전  - 우주항공청, 2026년 월력요항 발표 내년 2월 설날에는 5일간의 황금연휴가 찾...   \n",
            "19             50분전  2026년 공휴일은 총 70일로 올해(68일)보다 2일 늘어난다. 주 5일제를 실시...   \n",
            "20             52분전  애플이 향후 3년간 최소 7종의 헤드마운트(Head-Mounted) 기기 출시를 예...   \n",
            "21             1시간전  배경훈 과학기술정보통신부 장관 후보자는 SK텔레콤 사태 민관합동조사에 대해 \"이번주...   \n",
            "22             1시간전  해킹 사태 여파로 SK텔레콤의 브랜드가치가 큰 폭으로 하락한 것으로 나타났다. 30...   \n",
            "23             1시간전  월간 주문 건수 9년 만에 50배 성장 입점 업체 매출도 가파르게 증가 25일 전사...   \n",
            "24             1시간전  최근 메타의 ‘인력 빼가기’에 핵심 연구자들을 잃은 오픈AI가 ‘정면 대응’에 나섰...   \n",
            "25             1시간전  여행지에서 돌아온 뒤 캐리어를 침대나 거실에 그대로 올리는 행동이 세균을 집 안까지...   \n",
            "26             1시간전  이 기사는 2025년06월27일 09시05분에 팜이데일리 프리미엄 콘텐츠로 선공개 ...   \n",
            "27             1시간전  지난 1년간 엔비디아 임원진들이 10억 달러 규모의 주식을 매도했다. 이 가운데 절...   \n",
            "28             1시간전  “‘비정상의 정상화’라는 슬로건을 걸었지만 지난 1년간 어려움이 많았습니다. 아직 ...   \n",
            "29             1시간전  \"지난 1년 '비정상의 정상화' 때문에 많은 고통을 겪었습니다. 부족한 모습도 많이...   \n",
            "\n",
            "                                                   링크  \n",
            "0   https://n.news.naver.com/mnews/article/092/000...  \n",
            "1   https://n.news.naver.com/mnews/article/055/000...  \n",
            "2   https://n.news.naver.com/mnews/article/003/001...  \n",
            "3   https://n.news.naver.com/mnews/article/138/000...  \n",
            "4   https://n.news.naver.com/mnews/article/011/000...  \n",
            "5   https://n.news.naver.com/mnews/article/366/000...  \n",
            "6   https://n.news.naver.com/mnews/article/015/000...  \n",
            "7   https://n.news.naver.com/mnews/article/052/000...  \n",
            "8   https://n.news.naver.com/mnews/article/008/000...  \n",
            "9   https://n.news.naver.com/mnews/article/018/000...  \n",
            "10  https://n.news.naver.com/mnews/article/003/001...  \n",
            "11  https://n.news.naver.com/mnews/article/015/000...  \n",
            "12  https://n.news.naver.com/mnews/article/277/000...  \n",
            "13  https://n.news.naver.com/mnews/article/092/000...  \n",
            "14  https://n.news.naver.com/mnews/article/293/000...  \n",
            "15  https://n.news.naver.com/mnews/article/092/000...  \n",
            "16  https://n.news.naver.com/mnews/article/029/000...  \n",
            "17  https://n.news.naver.com/mnews/article/092/000...  \n",
            "18  https://n.news.naver.com/mnews/article/016/000...  \n",
            "19  https://n.news.naver.com/mnews/article/011/000...  \n",
            "20  https://n.news.naver.com/mnews/article/138/000...  \n",
            "21  https://n.news.naver.com/mnews/article/003/001...  \n",
            "22  https://n.news.naver.com/mnews/article/014/000...  \n",
            "23  https://n.news.naver.com/mnews/article/016/000...  \n",
            "24  https://n.news.naver.com/mnews/article/011/000...  \n",
            "25  https://n.news.naver.com/mnews/article/029/000...  \n",
            "26  https://n.news.naver.com/mnews/article/018/000...  \n",
            "27  https://n.news.naver.com/mnews/article/023/000...  \n",
            "28  https://n.news.naver.com/mnews/article/005/000...  \n",
            "29  https://n.news.naver.com/mnews/article/031/000...  \n",
            "\n",
            " 뉴스 크롤링 완료!\n",
            " 총 30개의 IT/과학 뉴스를 수집했습니다!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def show_news_data(news_data):\n",
        "    \"\"\"뉴스 데이터를 화면에 출력\"\"\"\n",
        "\n",
        "    if not news_data:\n",
        "        print(\" 출력할 뉴스 데이터가 없습니다\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(\"  네이버 뉴스 IT/과학 섹션 - 최신 기사  \")\n",
        "    print(f\"수집 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"총 {len(news_data)}개 기사\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for article in news_data:\n",
        "        print(f\"\\n[{article['순번']}] {article['제목']}\")\n",
        "        print(f\"    {article['언론사']} |  {article['시간']}\")\n",
        "\n",
        "        if article['요약'] != '요약없음' and len(article['요약']) > 10:\n",
        "            print(f\"    {article['요약']}\")\n",
        "\n",
        "        if article['링크'] != '링크없음':\n",
        "            print(f\"    {article['링크']}\")\n",
        "\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "def analyze_news_data(news_data):\n",
        "    \"\"\"뉴스 데이터 간단 분석\"\"\"\n",
        "\n",
        "    if not news_data:\n",
        "        return\n",
        "\n",
        "    print(f\"\\n 뉴스 데이터 분석 결과:\")\n",
        "\n",
        "    # 언론사별 기사 수\n",
        "    press_count = {}\n",
        "    for article in news_data:\n",
        "        press = article.get('언론사', '알수없음')\n",
        "        if press != '정보없음':\n",
        "            press_count[press] = press_count.get(press, 0) + 1\n",
        "\n",
        "    if press_count:\n",
        "        print(f\"    참여 언론사: {len(press_count)}개\")\n",
        "        top_press = sorted(press_count.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "        for press, count in top_press:\n",
        "            print(f\"      - {press}: {count}개 기사\")\n",
        "\n",
        "    # 시간 정보가 있는 기사\n",
        "    time_articles = [a for a in news_data if a.get('시간', '정보없음') != '정보없음']\n",
        "    print(f\"    시간 정보 있는 기사: {len(time_articles)}개\")\n",
        "\n",
        "    # 요약이 있는 기사\n",
        "    summary_articles = [a for a in news_data if a.get('요약', '요약없음') != '요약없음']\n",
        "    print(f\"    요약이 있는 기사: {len(summary_articles)}개\")\n",
        "\n",
        "# 메인 실행 함수\n",
        "def main():\n",
        "    \"\"\"프로그램의 메인 함수\"\"\"\n",
        "\n",
        "    print(\" 네이버 뉴스 IT/과학 크롤링을 시작합니다!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\" 대상 사이트: https://news.naver.com/section/105\")\n",
        "    print(\" 목표: 실제 IT/과학 뉴스 기사 수집\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 뉴스 데이터 크롤링\n",
        "    news_data = get_naver_news()\n",
        "\n",
        "    if news_data:\n",
        "        # 데이터 분석\n",
        "        analyze_news_data(news_data)\n",
        "\n",
        "        # 데이터 화면 출력\n",
        "        show_news_data(news_data)\n",
        "\n",
        "        # 데이터 파일 저장\n",
        "        save_news_data(news_data)\n",
        "\n",
        "        print(f\"\\n 뉴스 크롤링 완료!\")\n",
        "        print(f\" 총 {len(news_data)}개의 IT/과학 뉴스를 수집했습니다!\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\n 뉴스 크롤링 실패!\")\n",
        "\n",
        "\n",
        "# 간단한 테스트 함수\n",
        "def test_news_crawling():\n",
        "    \"\"\"뉴스 크롤링 테스트\"\"\"\n",
        "\n",
        "    print(\" 네이버 뉴스 크롤링을 테스트합니다...\\n\")\n",
        "\n",
        "    news_data = get_naver_news()\n",
        "\n",
        "    if news_data:\n",
        "        print(\" 크롤링 테스트 성공!\")\n",
        "        print(f\" 수집된 기사 수: {len(news_data)}\")\n",
        "        print(\" 첫 3개 기사 제목:\")\n",
        "        for i, article in enumerate(news_data[:3]):\n",
        "            print(f\"   {i+1}. {article['제목'][:50]}...\")\n",
        "    else:\n",
        "        print(\" 크롤링 테스트 실패!\")\n",
        "\n",
        "    return news_data\n",
        "\n",
        "# 프로그램 실행\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
