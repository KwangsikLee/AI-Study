{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c4b691",
   "metadata": {},
   "source": [
    "# ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§ í›„ ë‰´ìŠ¤ ì±—ë´‡\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b182a",
   "metadata": {},
   "source": [
    "#í¬ë¡¤ë§ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "332eed1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "\n",
      " ì´ 427ê°œì˜ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!\n",
      "============================================================\n",
      " 1. ì½”ìŠ¤í”¼ 081 ë‚´ë¦° 315156ì— ë§ˆê° í•œêµ­ê²½ì œTV ë‚´ìš©ì‘ì„±ì „\n",
      " 2. ì½”ìŠ¤í”¼ 2572í¬ì¸íŠ¸ë‚´ë¦° 315156 ë§ˆê° ë¨¸ë‹ˆS ë‚´ìš©ì‘ì„±ì „\n",
      " 3. ì •ì„±í˜¸ ë²•ë¬´ì¥ê´€ ê±´ì§„ë²•ì‚¬ ê´€ë´‰ê¶Œ ë ì§€ ë¶„ì‹¤ ê°ì°° ì§€ì‹œ ë§¤ê²½ì´ì½”ë…¸ë¯¸ ë‚´ìš©ì‘ì„±ì „\n",
      " 4. ì´ì¤€ì„ ê¹€ê±´í¬ íŠ¹ê²€ ì¶œì„í¬ë Œì‹ ì°¸ì—¬ JTBC ë‚´ìš©ì‘ì„±ì „\n",
      " 5. í•œìˆ˜ì›ì›¨ìŠ¤íŒ…í•˜ìš°ìŠ¤ ê³„ì•½ ë…¼ë€ì— ëŒ€í†µë ¹ì‹¤ ì‚°ì—…ë¶€ ì§„ìƒ ë³´ê³  ì§€ì‹œ CJBì²­ì£¼ë°©ì†¡\n",
      " 6. ëŒ€ê²€ ê±´ì§„ë²•ì‚¬ ê´€ë´‰ê¶Œ ë ì§€ ë¶„ì‹¤ ê°ì°° ì°©ìˆ˜ KBS ë‚´ìš©ì‘ì„±ì „\n",
      " 7. ì •ì„±í˜¸ ë²•ë¬´ ê±´ì§„ ëˆë‹¤ë°œ ë ì§€ ë¶„ì‹¤í•œ ë‚¨ë¶€ì§€ê²€ ê°ì°° ë“± ì§€ì‹œ\n",
      " 8. ëŒ€í†µë ¹ì‹¤ ë¶ˆê³µì • ê³„ì•½ ë…¼ë€ ì²´ì½” ì›ì „ ìˆ˜ì¶œ ì§„ìƒ íŒŒì•… ì§€ì‹œ\n",
      " 9. ê¹€ì˜ê²¸ ì²­ë‹´ë™ ìˆ ìë¦¬ ì˜í˜¹ í•œë™í›ˆ ì†í•´ë°°ìƒ íŒ¨ì†Œì— í•­ì†Œ\n",
      "10. ë¶€ë¶€ ê³„ì—„ ì±…ì„ ì†Œì†¡ ì‹œë¯¼ë“¤ ì„œì´ˆë™ ìíƒ ê°€ì••ë¥˜ ì‹ ì²­\n",
      "============================================================\n",
      "\n",
      " ë‰´ìŠ¤ ë°ì´í„° ë¶„ì„:\n",
      "ì´ ë‰´ìŠ¤ ê°œìˆ˜: 427\n",
      "í‰ê·  ê¸€ììˆ˜: 34.1\n",
      "ìµœëŒ€ ê¸€ììˆ˜: 89\n",
      "ìµœì†Œ ê¸€ììˆ˜: 15\n",
      "\n",
      " 'news_headlines.csv' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"í•œê¸€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)\"\"\"\n",
    "    if text == \"\":\n",
    "        return \"\"\n",
    "\n",
    "    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ì–´, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)\n",
    "    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', '', str(text))\n",
    "\n",
    "    # '...' (ë§ˆì¹¨í‘œ 3ê°œ)ë¥¼ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´\n",
    "    text = re.sub(r'\\.{3,}', ' ', text)\n",
    "\n",
    "    # 2ê°œ ì´ìƒì˜ ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ë³€ê²½\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def crawl_naver_news():\n",
    "    \"\"\"ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”ì¸ í˜ì´ì§€ì—ì„œ í—¤ë“œë¼ì¸ì„ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "\n",
    "    print(\"ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "    # ì›¹ í˜ì´ì§€ URL\n",
    "    url = \"https://news.naver.com\"\n",
    "\n",
    "    # í—¤ë” ì„¤ì • (ì½”ë© í™˜ê²½ì— ë§ê²Œ ì¡°ì •)\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # ì›¹ í˜ì´ì§€ ìš”ì²­\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # HTML íŒŒì‹±\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ì¶”ì¶œ\n",
    "        headlines = []\n",
    "\n",
    "        # ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ì°¾ê¸°\n",
    "        selectors = [\n",
    "            'a.cjs_news_link',\n",
    "            'a[href*=\"/article/\"]',\n",
    "            '.hdline_article_tit',\n",
    "            '.cluster_text_headline'\n",
    "        ]\n",
    "\n",
    "        for selector in selectors:\n",
    "            items = soup.select(selector)\n",
    "            if items:\n",
    "                for item in items[:-1]:  # ë” ë§ì€ í—¤ë“œë¼ì¸ ìˆ˜ì§‘\n",
    "                    title = item.get_text().strip()\n",
    "                    if title and len(title) > 10:\n",
    "                        title = preprocess_text(title)\n",
    "                        headlines.append(title)\n",
    "                break\n",
    "\n",
    "        # ê²°ê³¼ ì¶œë ¥\n",
    "        print(f\"\\n ì´ {len(headlines)}ê°œì˜ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        for i, headline in enumerate(headlines[:10], 1):\n",
    "            print(f\"{i:2d}. {headline}\")\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        return headlines\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\" ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\" í¬ë¡¤ë§ ì˜¤ë¥˜: {e}\")\n",
    "        return []\n",
    "    \n",
    "\n",
    "\n",
    "def create_news_dataframe(headlines):\n",
    "    \"\"\"ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "\n",
    "    if not headlines:\n",
    "        print(\" ë¶„ì„í•  í—¤ë“œë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "    # ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "    df = pd.DataFrame({\n",
    "        'ìˆœë²ˆ': range(1, len(headlines) + 1),\n",
    "        'í—¤ë“œë¼ì¸': headlines,\n",
    "        'ê¸€ììˆ˜': [len(headline) for headline in headlines],\n",
    "        'í¬ë¡¤ë§ì‹œê°„': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')] * len(headlines)\n",
    "    })\n",
    "\n",
    "    print(\"\\n ë‰´ìŠ¤ ë°ì´í„° ë¶„ì„:\")\n",
    "    print(f\"ì´ ë‰´ìŠ¤ ê°œìˆ˜: {len(df)}\")\n",
    "    print(f\"í‰ê·  ê¸€ììˆ˜: {df['ê¸€ììˆ˜'].mean():.1f}\")\n",
    "    print(f\"ìµœëŒ€ ê¸€ììˆ˜: {df['ê¸€ììˆ˜'].max()}\")\n",
    "    print(f\"ìµœì†Œ ê¸€ììˆ˜: {df['ê¸€ììˆ˜'].min()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "headlines = crawl_naver_news()\n",
    "df = create_news_dataframe(headlines)\n",
    "\n",
    "# íŒŒì¼ ì €ì¥\n",
    "if df is not None:\n",
    "    # CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "    df.to_csv('news_headlines.csv', index=False, encoding='utf-8')\n",
    "    print(\"\\n 'news_headlines.csv' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bdb860",
   "metadata": {},
   "source": [
    "# Chat Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f7c3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kwangsiklee/miniforge3/envs/hf_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import urllib.request\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\n",
    "train_data = pd.read_csv('ChatBotData.csv')\n",
    "train_data.head(50)\n",
    "\n",
    "train_data['embedding'] = train_data.apply(lambda row: model.encode(row.Q), axis = 1)\n",
    "\n",
    "\n",
    "def cos_sim(A, B):\n",
    "  return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "def return_answer(question):\n",
    "    embedding = model.encode(question)\n",
    "    train_data['score'] = train_data.apply(lambda x: cos_sim(x['embedding'], embedding), axis=1)\n",
    "    return train_data.loc[train_data['score'].idxmax()]['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604a7f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì €ëŠ” ìœ„ë¡œë´‡ì…ë‹ˆë‹¤.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_answer('ë„ˆëŠ” ëˆ„êµ¬ë‹ˆ?')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
