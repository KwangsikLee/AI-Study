{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c4b691",
   "metadata": {},
   "source": [
    "# 뉴스 기사 크롤링 후 뉴스 챗봇\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b182a",
   "metadata": {},
   "source": [
    "#크롤링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "332eed1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📰 네이버 뉴스 크롤링을 시작합니다...\n",
      "\n",
      " 총 427개의 뉴스 헤드라인을 찾았습니다!\n",
      "============================================================\n",
      " 1. 코스피 081 내린 315156에 마감 한국경제TV 내용작성전\n",
      " 2. 코스피 2572포인트내린 315156 마감 머니S 내용작성전\n",
      " 3. 정성호 법무장관 건진법사 관봉권 띠지 분실 감찰 지시 매경이코노미 내용작성전\n",
      " 4. 이준석 김건희 특검 출석포렌식 참여 JTBC 내용작성전\n",
      " 5. 한수원웨스팅하우스 계약 논란에 대통령실 산업부 진상 보고 지시 CJB청주방송\n",
      " 6. 대검 건진법사 관봉권 띠지 분실 감찰 착수 KBS 내용작성전\n",
      " 7. 정성호 법무 건진 돈다발 띠지 분실한 남부지검 감찰 등 지시\n",
      " 8. 대통령실 불공정 계약 논란 체코 원전 수출 진상 파악 지시\n",
      " 9. 김의겸 청담동 술자리 의혹 한동훈 손해배상 패소에 항소\n",
      "10. 부부 계엄 책임 소송 시민들 서초동 자택 가압류 신청\n",
      "============================================================\n",
      "\n",
      " 뉴스 데이터 분석:\n",
      "총 뉴스 개수: 427\n",
      "평균 글자수: 34.1\n",
      "최대 글자수: 89\n",
      "최소 글자수: 15\n",
      "\n",
      " 'news_headlines.csv' 파일로 저장되었습니다!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"한글 텍스트 전처리 (개선된 버전)\"\"\"\n",
    "    if text == \"\":\n",
    "        return \"\"\n",
    "\n",
    "    # 특수문자 제거 (한글, 영어, 숫자, 공백만 유지)\n",
    "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', str(text))\n",
    "\n",
    "    # '...' (마침표 3개)를 공백으로 대체\n",
    "    text = re.sub(r'\\.{3,}', ' ', text)\n",
    "\n",
    "    # 2개 이상의 연속된 공백을 하나의 공백으로 변경\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def crawl_naver_news():\n",
    "    \"\"\"네이버 뉴스 메인 페이지에서 헤드라인을 크롤링하는 함수\"\"\"\n",
    "\n",
    "    print(\"📰 네이버 뉴스 크롤링을 시작합니다...\")\n",
    "\n",
    "    # 웹 페이지 URL\n",
    "    url = \"https://news.naver.com\"\n",
    "\n",
    "    # 헤더 설정 (코랩 환경에 맞게 조정)\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # 웹 페이지 요청\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # HTML 파싱\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 뉴스 헤드라인 추출\n",
    "        headlines = []\n",
    "\n",
    "        # 여러 가지 방법으로 뉴스 헤드라인 찾기\n",
    "        selectors = [\n",
    "            'a.cjs_news_link',\n",
    "            'a[href*=\"/article/\"]',\n",
    "            '.hdline_article_tit',\n",
    "            '.cluster_text_headline'\n",
    "        ]\n",
    "\n",
    "        for selector in selectors:\n",
    "            items = soup.select(selector)\n",
    "            if items:\n",
    "                for item in items[:-1]:  # 더 많은 헤드라인 수집\n",
    "                    title = item.get_text().strip()\n",
    "                    if title and len(title) > 10:\n",
    "                        title = preprocess_text(title)\n",
    "                        headlines.append(title)\n",
    "                break\n",
    "\n",
    "        # 결과 출력\n",
    "        print(f\"\\n 총 {len(headlines)}개의 뉴스 헤드라인을 찾았습니다!\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        for i, headline in enumerate(headlines[:10], 1):\n",
    "            print(f\"{i:2d}. {headline}\")\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        return headlines\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\" 네트워크 오류: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\" 크롤링 오류: {e}\")\n",
    "        return []\n",
    "    \n",
    "\n",
    "\n",
    "def create_news_dataframe(headlines):\n",
    "    \"\"\"뉴스 헤드라인을 데이터프레임으로 변환\"\"\"\n",
    "\n",
    "    if not headlines:\n",
    "        print(\" 분석할 헤드라인이 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    # 데이터프레임 생성\n",
    "    df = pd.DataFrame({\n",
    "        '순번': range(1, len(headlines) + 1),\n",
    "        '헤드라인': headlines,\n",
    "        '글자수': [len(headline) for headline in headlines],\n",
    "        '크롤링시간': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')] * len(headlines)\n",
    "    })\n",
    "\n",
    "    print(\"\\n 뉴스 데이터 분석:\")\n",
    "    print(f\"총 뉴스 개수: {len(df)}\")\n",
    "    print(f\"평균 글자수: {df['글자수'].mean():.1f}\")\n",
    "    print(f\"최대 글자수: {df['글자수'].max()}\")\n",
    "    print(f\"최소 글자수: {df['글자수'].min()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "headlines = crawl_naver_news()\n",
    "df = create_news_dataframe(headlines)\n",
    "\n",
    "# 파일 저장\n",
    "if df is not None:\n",
    "    # CSV 파일로 저장\n",
    "    df.to_csv('news_headlines.csv', index=False, encoding='utf-8')\n",
    "    print(\"\\n 'news_headlines.csv' 파일로 저장되었습니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bdb860",
   "metadata": {},
   "source": [
    "# Chat Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f7c3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kwangsiklee/miniforge3/envs/hf_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import urllib.request\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\n",
    "train_data = pd.read_csv('ChatBotData.csv')\n",
    "train_data.head(50)\n",
    "\n",
    "train_data['embedding'] = train_data.apply(lambda row: model.encode(row.Q), axis = 1)\n",
    "\n",
    "\n",
    "def cos_sim(A, B):\n",
    "  return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "def return_answer(question):\n",
    "    embedding = model.encode(question)\n",
    "    train_data['score'] = train_data.apply(lambda x: cos_sim(x['embedding'], embedding), axis=1)\n",
    "    return train_data.loc[train_data['score'].idxmax()]['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604a7f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'저는 위로봇입니다.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_answer('너는 누구니?')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
