{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uNrASrLBlCK-"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 1) ì„¤ì¹˜\n",
        "# ============================================\n",
        "!pip -q install langchain langchain-community langchain-text-splitters faiss-cpu sentence-transformers pypdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVr1HFIAJ_Xb"
      },
      "source": [
        "**MultiVectorRetriever**\n",
        "\n",
        "ì›ë¬¸ì—ì„œ\n",
        "   > childë¥¼ ì—¬ëŸ¬ ë¶€ë¶„(ì˜ˆ, ëª©ì°¨, ë³¸ë¬¸, í‚¤ì›Œë“œ ë“±) ì—ì„œ ì°¾ì•„ì„œ\n",
        "     \n",
        "    parent ì²­í¬ì— ì—°ê²°í•´. ë¶€ëª¨ chunk ë‚´ìš©ì„ ê²°ê³¼ë¡œ ë„ì¶œ\n",
        "\n",
        "    Parent <> Child ì—°ê²°ì€ \"doc_id\"ë¡œ ë˜ì–´ ìˆìŒ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzjySWNZfRGH",
        "outputId": "9363e885-9c21-4168-9051-6faa809bc8dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… PDF ready: 2020_ê²½ì œê¸ˆìœµìš©ì–´ 700ì„ _ê²Œì‹œ.pdf\n",
            "ğŸ“„ ì „ì²´ í˜ì´ì§€ ìˆ˜: 371\n",
            "ğŸ§© ë¶€ëª¨ ì²­í¬ ìˆ˜: 466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â³ ë¶€ëª¨-ìì‹ ë§¤í•‘ ì¤‘...\n",
            "âœ… ìƒ‰ì¸ ì™„ë£Œ\n",
            "ë¶€ëª¨ ë¬¸ì„œ ìˆ˜: 466 | ìì‹ ì²­í¬ ìˆ˜: 1378\n",
            "\n",
            "ğŸ” Q: ì¸í”Œë ˆì´ì…˜ì˜ ì •ì˜ì™€ ì›ì¸, ê·¸ë¦¬ê³  ê¸°ì¤€ê¸ˆë¦¬ì™€ì˜ ê´€ê³„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n",
            "--- Top-3 ë¶€ëª¨ ë¬¸ì„œ ---\n",
            " 1. p.334 | 318 ê²½ì œê¸ˆìœµìš©ì–´ 700ì„  ì—¬ ë§¤ë§¤í•  ë•Œ ì´ìš©ëœë‹¤. ê³¼ê±°ì— í”„ë¡œê·¸ë¨ë§¤ë§¤ëŠ” ì§€ìˆ˜ì°¨ìµê±°ë˜ ìœ„ì£¼ë¡œ ì´ë£¨ì–´ì¡Œìœ¼ë‚˜ ìµœê·¼ì— ëŠ” ë¹„ì°¨ìµê±°ë˜ê°€ ëŒ€ë¶€ë¶„(2016ë…„ ìƒë°˜ê¸° ì¤‘ ê±°ë˜ëŒ€ê¸ˆ ê¸°ì¤€ìœ¼ë¡œ 98.3%)ì„ ì°¨ì§€í•˜ê³  ìˆë‹¤.  ì´ëŠ” ê³µëª¨í€ë“œ ë° ì—°ê¸°ê¸ˆ(2010ë…„)ê³¼ ìš°ì •ì‚¬ì—…ë³¸ë¶€(2013ë…„)ì— ëŒ€í•œ ì¦ê¶Œê±°...\n",
            " 2. p.131 | 115 ã…  ë¬¼ê°€ì§€ìˆ˜ ì‹œì¥ì—ì„œ ê±°ë˜ë˜ëŠ” ì—¬ëŸ¬ ê°€ì§€ ìƒí’ˆê³¼ ì„œë¹„ìŠ¤ì˜ ê°€ê²©ì„ ê²½ì œìƒí™œì—ì„œ ì°¨ì§€í•˜ëŠ” ì¤‘ìš”ë„ ë¥¼ ê³ ë ¤í•˜ì—¬ í‰ê· í•œ ì¢…í•©ì ì¸ ê°€ê²©ìˆ˜ì¤€ì„ ë¬¼ê°€ë¼ê³  í•˜ëŠ”ë°, ì´ ê°™ì€ ë¬¼ê°€ì˜ ë³€í™”ë¥¼  í•œ ëˆˆì— ì•Œì•„ë³¼ ìˆ˜ ìˆë„ë¡ ê¸°ì¤€ì—°ë„ì˜ ë¬¼ê°€ìˆ˜ì¤€ì„ 100ìœ¼ë¡œ ë†“ê³  ë¹„êµë˜ëŠ” ë‹¤ë¥¸ ì‹œì ì˜  ë¬¼ê°€ë¥¼ ì§€ìˆ˜ì˜ ...\n",
            " 3. p.82 | 66 ê²½ì œê¸ˆìœµìš©ì–´ 700ì„  í•µì‹¬ì§€í‘œì¤‘ í•˜ë‚˜ì´ë‹¤. ê¸°ëŒ€ì¸í”Œë ˆì´ì…˜ì€ ì„ê¸ˆí˜‘ìƒ, ê°€ê²©ì„¤ì •, íˆ¬ìê²°ì • ë“± ê²½ì œì£¼ì²´ì˜  ì˜ì‚¬ê²°ì •ì— ë°˜ì˜ë˜ë©´ì„œ ìµœì¢…ì ìœ¼ë¡œ ì‹¤ì œ ì¸í”Œë ˆì´ì…˜ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤. êµ¬ì²´ì ì¸ ê²½ë¡œë¥¼  ì‚´í´ë³´ë©´ ê¸°ëŒ€ì¸í”Œë ˆì´ì…˜ ìƒìŠ¹ ì‹œ ê°€ê³„ëŠ” êµ¬ë§¤ë ¥ í•˜ë½ì„ ìš°ë ¤í•˜ì—¬ ëª…ëª©ì„ê¸ˆ ìƒìŠ¹ì„  ìš”êµ¬í•˜ê²Œ ë˜...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3960166090.py:124: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  hits = retriever.get_relevant_documents(q)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ” Q: í™˜ìœ¨ ë³€ë™ì´ ë¬¼ê°€ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ë‚˜ìš”?\n",
            "--- Top-3 ë¶€ëª¨ ë¬¸ì„œ ---\n",
            " 1. p.197 | 181 ã……  ì¸í”Œë ˆì´ì…˜ì••ë ¥ì„ ë†’ì¼ ê°€ëŠ¥ì„±ì´ í° ìƒí™©ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ë¬¼ê°€ì•ˆì •ì„ ì£¼ëœ  ì •ì±…ëª©í‘œë¡œ í•˜ëŠ” ì¤‘ì•™ì€í–‰ì˜ ê²½ìš° ì‹¤ì—…ë¥ ê°­ì€ ë…¸ë™ì‹œì¥ì„ í†µí•œ ì¸í”Œë ˆì´ì…˜ ìƒìŠ¹ì••ë ¥ì„  í‰ê°€í•˜ê±°ë‚˜, í†µí™”ì •ì±… ê¸°ì¡°ê°€ ë…¸ë™ì‹œì¥ ìˆ˜ê¸‰ë¶ˆê· í˜• í•´ì†Œì— ì–´ëŠ ì •ë„ ë¶€í•©í•˜ëŠ”ì§€ ë“±ì„  ì‚´í´ë³´ëŠ”ë° ìœ ìš©í•˜ê²Œ í™œìš©í• ...\n",
            " 2. p.131 | 115 ã…  ë¬¼ê°€ì§€ìˆ˜ ì‹œì¥ì—ì„œ ê±°ë˜ë˜ëŠ” ì—¬ëŸ¬ ê°€ì§€ ìƒí’ˆê³¼ ì„œë¹„ìŠ¤ì˜ ê°€ê²©ì„ ê²½ì œìƒí™œì—ì„œ ì°¨ì§€í•˜ëŠ” ì¤‘ìš”ë„ ë¥¼ ê³ ë ¤í•˜ì—¬ í‰ê· í•œ ì¢…í•©ì ì¸ ê°€ê²©ìˆ˜ì¤€ì„ ë¬¼ê°€ë¼ê³  í•˜ëŠ”ë°, ì´ ê°™ì€ ë¬¼ê°€ì˜ ë³€í™”ë¥¼  í•œ ëˆˆì— ì•Œì•„ë³¼ ìˆ˜ ìˆë„ë¡ ê¸°ì¤€ì—°ë„ì˜ ë¬¼ê°€ìˆ˜ì¤€ì„ 100ìœ¼ë¡œ ë†“ê³  ë¹„êµë˜ëŠ” ë‹¤ë¥¸ ì‹œì ì˜  ë¬¼ê°€ë¥¼ ì§€ìˆ˜ì˜ ...\n",
            " 3. p.332 | 316 ê²½ì œê¸ˆìœµìš©ì–´ 700ì„  ììœ ë³€ë™í™˜ìœ¨ì œë„ì¤‘ ì–´ëŠ í™˜ìœ¨ì œë„ë¥¼ ì±„íƒí•˜ëŠëƒì— ë”°ë¼ êµ¬ë¶„ëœë‹¤. ì¦‰ í‰ê°€ì ˆìƒ(ë˜ëŠ” í‰ê°€ ì ˆí•˜)ì€ ê³ ì •í™˜ìœ¨ì œë„í•˜ì—ì„œ ì •ë¶€ê°€ ì •ì±…ì  ëª©ì  ë“±ìœ¼ë¡œ ìêµ­í†µí™”ì˜ ëŒ€ì™¸ê°€ì¹˜ì¸ í™˜ìœ¨ì„  ì¸ìœ„ì ìœ¼ë¡œ ì¼ì‹œì— ì¡°ì •í•˜ì˜€ì„ ê²½ìš° ì‚¬ìš©ë˜ë©°, ì ˆìƒ(ë˜ëŠ” ì ˆí•˜)ì€ ì¼ë°˜ì ìœ¼ë¡œ ììœ ë³€ë™í™˜ ìœ¨...\n",
            "\n",
            "ğŸ” Q: ìŠ¤íƒœê·¸í”Œë ˆì´ì…˜ì˜ ì˜ë¯¸ì™€ ì •ì±… ëŒ€ì‘ìƒì˜ ì–´ë ¤ì›€ì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
            "--- Top-3 ë¶€ëª¨ ë¬¸ì„œ ---\n",
            " 1. p.185 | 169 ã……  ìŠ¤íƒœê·¸í”Œë ˆì´ì…˜ ì œ2ì°¨ ì„¸ê³„ëŒ€ì „ ì´ì „ê¹Œì§€ ê²½ê¸°ì¹¨ì²´ê¸°ì—ëŠ” ë¬¼ê°€ê°€ í•˜ë½í•˜ê³  ê²½ê¸°í˜¸í™©ê¸°ì—ëŠ” ë¬¼ê°€ê°€  ìƒìŠ¹í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì  í˜„ìƒì´ì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì œ2ì°¨ ì„¸ê³„ëŒ€ì „ ì´í›„ íŠ¹íˆ 1970ë…„ëŒ€ ë“¤ì–´  ë‘ ë²ˆì˜ ìœ ê°€íŒŒë™ì„ ê²ªìœ¼ë©´ì„œ ì‹¤ì—…ì´ ëŠ˜ì–´ë‚˜ëŠ” ì¹¨ì²´ê¸°ì—ë„ ì¸í”Œë ˆì´ì…˜ì´ ì§€ì†ë˜ëŠ” í˜„ìƒ ì´ ë‚˜íƒ€ë‚˜ê¸°...\n",
            " 2. p.158 | ë¶€ì •ì ì¸ ì˜í–¥ì„ ì¤„ ê°€ëŠ¥ì„±ì´ ìˆë‹¤. ë˜í•œ ì‚¬ì „ì  ì •ì±…ë°©í–¥ì˜ ì œì‹œë‚´ìš©ì´ ì§€ë‚˜ì¹˜ê²Œ ë³µì¡ í•˜ê³  ì´í•´í•˜ê¸° ì–´ë ¤ìš¸ ê²½ìš° ì˜¤íˆë ¤ ì •ì±…ìš´ì˜ ë°©ì‹ì˜ ëª…ë£Œì„±ì„ ì œì•½í•˜ê³  ì •ì±… ë¶ˆí™•ì‹¤ì„±ì´  ë‹¤ì‹œ ë†’ì•„ì§€ëŠ” ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ë„ ìˆë‹¤ .  ì—°ê´€ê²€ìƒ‰ì–´ : ì–‘ì ì™„í™”ì •ì±…, ì œë¡œê¸ˆë¦¬ì •ì±…, í†µí™”ì •ì±… ì»¤ë®¤ë‹ˆì¼€ì´ì…˜...\n",
            " 3. p.125 | 109 ã…  ë§¤ì¶œì•¡ì˜ì—…ì´ìµë¥  î‡ î­ë§¤ì¶œì•¡ ì˜ì—…ì†ìµ Ã—î€´î€½î€½ î‡î„ î­ë§¤ì¶œì•¡ ë§¤ì¶œì´ì†ìµ îƒ Ã— î­ë§¤ì¶œì´ì†ìµ ì˜ì—…ì†ìµ îƒîƒ î…Ã—î€´î€½î€½      * ë§¤ì¶œì´ì†ìµ = ë§¤ì¶œì•¡-ë§¤ì¶œì›ê°€     ** ì˜ì—…ì†ìµ = ë§¤ì¶œì´ì†ìµ - íŒë§¤ë¹„ì™€ê´€ë¦¬ë¹„ ë¨¸ë‹ˆë§ˆì¼“í€ë“œ(MMF) ë¨¸ë‹ˆë§ˆì¼“í€ë“œ(MMF; Money Mark...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================\n",
        "# 2) PDF ë‹¤ìš´ë¡œë“œ (ê²½ì œê¸ˆìœµìš©ì–´ 700ì„ )\n",
        "# ============================================\n",
        "import os, requests, urllib.request\n",
        "\n",
        "url = \"https://github.com/chatgpt-kr/openai-api-tutorial/raw/main/ch07/2020_%EA%B2%BD%EC%A0%9C%EA%B8%88%EC%9C%B5%EC%9A%A9%EC%96%B4%20700%EC%84%A0_%EA%B2%8C%EC%8B%9C.pdf\"\n",
        "\n",
        "\n",
        "pdf_path = \"2020_ê²½ì œê¸ˆìœµìš©ì–´ 700ì„ _ê²Œì‹œ.pdf\"\n",
        "\n",
        "# ìš°ì„  urlretrieve ì‹œë„\n",
        "try:\n",
        "    urllib.request.urlretrieve(url, filename=pdf_path)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ë¹„ì–´ìˆìœ¼ë©´ requestsë¡œ ì¬ì‹œë„\n",
        "if not os.path.exists(pdf_path) or os.path.getsize(pdf_path) == 0:\n",
        "    r = requests.get(url, stream=True, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    with open(pdf_path, \"wb\") as f:\n",
        "        for chunk in r.iter_content(chunk_size=8192):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "print(\"âœ… PDF ready:\", pdf_path)\n",
        "\n",
        "# ============================================\n",
        "# 3) PDF ë¡œë“œ & Parent/Child ì²­í¬ ë¶„í• \n",
        "#    (community ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì‚¬ìš©)\n",
        "# ============================================\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "docs = loader.load()\n",
        "print(f\"ğŸ“„ ì „ì²´ í˜ì´ì§€ ìˆ˜: {len(docs)}\")\n",
        "\n",
        "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=120)\n",
        "child_splitter  = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=40)\n",
        "\n",
        "parent_docs = parent_splitter.split_documents(docs)\n",
        "print(f\"ğŸ§© ë¶€ëª¨ ì²­í¬ ìˆ˜: {len(parent_docs)}\")\n",
        "\n",
        "# ============================================\n",
        "# 4) í•œêµ­ì–´ ì„ë² ë”© ì¤€ë¹„ (community ë„¤ì„ìŠ¤í˜ì´ìŠ¤)\n",
        "# ============================================\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n",
        "\n",
        "# ============================================\n",
        "# 5) ë¹ˆ FAISS ì¸ë±ìŠ¤ + InMemoryStore ìƒì„±\n",
        "#    (community ë„¤ì„ìŠ¤í˜ì´ìŠ¤ + dependable_faiss_import)\n",
        "# ============================================\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.vectorstores.faiss import dependable_faiss_import\n",
        "from langchain.docstore import InMemoryDocstore  # ì—¬ê¸°ëŠ” core ê²½ë¡œ ìœ ì§€\n",
        "\n",
        "faiss = dependable_faiss_import()\n",
        "dim = len(embedding.embed_query(\"í…ŒìŠ¤íŠ¸\"))  # ì„ë² ë”© ì°¨ì› í™•ì¸\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "vectorstore = FAISS(embedding.embed_query, index, InMemoryDocstore(), {})\n",
        "\n",
        "# ë¶€ëª¨ ë¬¸ì„œ ì €ì¥ì†Œ (KV ìŠ¤í† ì–´)\n",
        "try:\n",
        "    from langchain.storage import InMemoryStore   # ìµœì‹ \n",
        "except ImportError:\n",
        "    class InMemoryStore(dict):                    # í´ë°±\n",
        "        def mset(self, items):\n",
        "            for k, v in items:\n",
        "                self[k] = v\n",
        "docstore = InMemoryStore()\n",
        "\n",
        "# ============================================\n",
        "# 6) MultiVectorRetriever ìƒì„±\n",
        "#    (ìµœì‹ : langchain.retrievers, í´ë°±: community)\n",
        "# ============================================\n",
        "try:\n",
        "    from langchain.retrievers import MultiVectorRetriever\n",
        "except ImportError:\n",
        "    from langchain_community.retrievers import MultiVectorRetriever\n",
        "\n",
        "from uuid import uuid4\n",
        "from langchain.schema import Document\n",
        "\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=docstore,\n",
        "    id_key=\"doc_id\",\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# 7) ë¶€ëª¨-ìì‹ ë§¤í•‘ (Child ì²­í¬ë§Œ ìƒ‰ì¸)\n",
        "# ============================================\n",
        "print(\" ë¶€ëª¨-ìì‹ ë§¤í•‘ ì¤‘...\")\n",
        "\n",
        "parent_id_to_doc = []\n",
        "child_texts, child_metas = [], []\n",
        "\n",
        "for pdoc in parent_docs:\n",
        "    pid = str(uuid4())\n",
        "\n",
        "    # ë¶€ëª¨ ë¬¸ì„œ ì €ì¥\n",
        "    parent_doc = Document(page_content=pdoc.page_content, metadata=pdoc.metadata | {\"doc_id\": pid})\n",
        "    parent_id_to_doc.append((pid, parent_doc))\n",
        "\n",
        "    # ìì‹ ì²­í¬ ìƒì„± & ë²¡í„° ìƒ‰ì¸ìš© ë°ì´í„° ìˆ˜ì§‘\n",
        "    child_chunks = child_splitter.split_documents([pdoc])\n",
        "    for c in child_chunks:\n",
        "        child_texts.append(c.page_content)\n",
        "        child_metas.append({\"doc_id\": pid, **(c.metadata or {})})\n",
        "\n",
        "# ë¶€ëª¨ ì €ì¥\n",
        "docstore.mset(parent_id_to_doc)\n",
        "\n",
        "# ìì‹ ë²¡í„° ìƒ‰ì¸\n",
        "if child_texts:\n",
        "    retriever.vectorstore.add_texts(texts=child_texts, metadatas=child_metas)\n",
        "\n",
        "print(\"âœ… ìƒ‰ì¸ ì™„ë£Œ\")\n",
        "print(f\"ë¶€ëª¨ ë¬¸ì„œ ìˆ˜: {len(parent_id_to_doc)} | ìì‹ ì²­í¬ ìˆ˜: {len(child_texts)}\")\n",
        "\n",
        "# ============================================\n",
        "# 8) ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
        "# ============================================\n",
        "def ask(q, k=3):\n",
        "    hits = retriever.get_relevant_documents(q)\n",
        "    print(f\"\\nğŸ” Q: {q}\\n--- Top-{min(k,len(hits))} ë¶€ëª¨ ë¬¸ì„œ ---\")\n",
        "    for i, d in enumerate(hits[:k], 1):\n",
        "        page = d.metadata.get(\"page\", \"?\")\n",
        "        preview = d.page_content[:160].replace(\"\\n\",\" \")\n",
        "        print(f\"{i:>2}. p.{page} | {preview}...\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸\n",
        "ask(\"ì¸í”Œë ˆì´ì…˜ì˜ ì •ì˜ì™€ ì›ì¸, ê·¸ë¦¬ê³  ê¸°ì¤€ê¸ˆë¦¬ì™€ì˜ ê´€ê³„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\")\n",
        "ask(\"í™˜ìœ¨ ë³€ë™ì´ ë¬¼ê°€ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ë‚˜ìš”?\")\n",
        "ask(\"ìŠ¤íƒœê·¸í”Œë ˆì´ì…˜ì˜ ì˜ë¯¸ì™€ ì •ì±… ëŒ€ì‘ìƒì˜ ì–´ë ¤ì›€ì€ ë¬´ì—‡ì¸ê°€ìš”?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# ì¸í„°í˜ì´ìŠ¤ ìƒì„±\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(label=\"ê²½ì œê¸ˆìœµìš©ì–´ ì±—ë´‡\") # ì±—ë´‡ ë ˆì´ë¸”ì„ ì¢Œì¸¡ ìƒë‹¨ì— êµ¬ì„±\n",
        "    msg = gr.Textbox(label=\"ì§ˆë¬¸í•´ì£¼ì„¸ìš”!\")  # í•˜ë‹¨ì˜ ì±„íŒ…ì°½ ë ˆì´ë¸”\n",
        "    clear = gr.Button(\"ëŒ€í™” ì´ˆê¸°í™”\")  # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼\n",
        "\n",
        "    # ì±—ë´‡ì˜ ë‹µë³€ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
        "    def respond(message, chat_history):\n",
        "      result = qa_chain(message, \n",
        "                        callbacks=[ConsoleCallbackHandler()])\n",
        "      bot_message = result['result']\n",
        "\n",
        "      # ì±„íŒ… ê¸°ë¡ì— ì‚¬ìš©ìì˜ ë©”ì‹œì§€ì™€ ë´‡ì˜ ì‘ë‹µì„ ì¶”ê°€\n",
        "      chat_history.append((message, bot_message))\n",
        "      return \"\", chat_history\n",
        "\n",
        "    # ì‚¬ìš©ìì˜ ì…ë ¥ì„ ì œì¶œ(submit)í•˜ë©´ respond í•¨ìˆ˜ê°€ í˜¸ì¶œ\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "    # 'ì´ˆê¸°í™”' ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ ì±„íŒ… ê¸°ë¡ì„ ì´ˆê¸°í™”\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "# ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
