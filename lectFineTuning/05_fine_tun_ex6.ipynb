{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”’ W&B ì™„ì „ ë¹„í™œì„±í™” (ë¡œê·¸ì¸/í‚¤ ìš”ì²­ ë°©ì§€)\n",
        "!pip -q uninstall -y wandb\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "os.environ[\"WANDB_SILENT\"] = \"true\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdp33l6oVKeT",
        "outputId": "f0ffebe7-9846-4319-d5bd-8d6419a4202e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping wandb as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "3UdAQ0QU732y",
        "outputId": "6be7263e-8dcd-41cf-ed54-d7b9c8d10a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,179,648 || all params: 126,345,984 || trainable%: 0.9337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-45762981.py:191: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 51200, 'bos_token_id': 51200, 'pad_token_id': 51200}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "í•™ìŠµ ì‹œì‘...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [216/216 00:27, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>4.494700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.566000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>3.081300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.483600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.927000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.733000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.514700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.325500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.271600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.306300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "í•™ìŠµ ì¢…ë£Œ.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì €ì¥: ./lora_adapter_ko_sft\n",
            "\n",
            "=== ë°ëª¨ ===\n",
            "Q: í•œ ì¤„ ìš”ì•½: ë°°í„°ë¦¬ ìˆ˜ëª…ì´ ê¸¸ì–´ ì‚¬ìš©ì ë§Œì¡±ë„ê°€ ë†’ì•„ì¡Œë‹¤.\n",
            "A: ë°°í„°ë¦¬ ìˆ˜ëª… ì—°ì¥\n",
            "ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì„±ëŠ¥ì´ ê°œì„ ëë‹¤.\n",
            "ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì„±ëŠ¥ì´ í–¥ìƒëë‹¤.\n",
            "ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì„±ëŠ¥ì´ í–¥ìƒëë‹¤.\n",
            "ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì„±ëŠ¥ì´ í–¥ìƒëë‹¤.\n",
            "ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì„±ëŠ¥ì´ í–¥ìƒëë‹¤.\n",
            "ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì„±ëŠ¥ì´ í–¥ìƒëë‹¤.\n",
            "ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì„±ëŠ¥ì´ í–¥ìƒëë‹¤.\n",
            "ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì„±ëŠ¥ì´ í–¥ìƒëë‹¤.\n",
            "ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì„±ëŠ¥ì´ í–¥ìƒëë‹¤.\n",
            "ê°„ê²°í•˜ê³ \n",
            "----------------------------------------\n",
            "Q: ì˜ì–´ë¡œ ë²ˆì—­: ë„ì›€ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.\n",
            "A: Thank you very much.Apples are good for health.Apples are good for health.Apples are good for health.Apples are good for health.Apples are good for health\n",
            "----------------------------------------\n",
            "Q: ë©”ì¼ ì²« ë¬¸ì¥(ê³µì†ì²´): íšŒì˜ ë§í¬ ì¬ì „ì†¡ ìš”ì²­\n",
            "A: ì•ˆë…•í•˜ì„¸ìš”, íšŒì˜ ë§í¬ ì¬ì „ì†¡ ìš”ì²­ë“œë¦½ë‹ˆë‹¤. íšŒì˜ ë§í¬ ì¬ì „ì†¡ ìš”ì²­ë“œë¦½ë‹ˆë‹¤. íšŒì˜ ë§í¬ ì¬ì „ì†¡ ìš”ì²­ë“œë¦½ë‹ˆë‹¤. íšŒì˜ ë§í¬ ì¬ì „ì†¡ ìš”ì²­ë“œë¦½ë‹ˆë‹¤. íšŒì˜ ë§í¬ ì¬ì „ì†¡ ìš”ì²­ë“œë¦½ë‹ˆë‹¤. íšŒì˜ ë§í¬ ì¬ì „ì†¡ ìš”ì²­ë“œë¦½ë‹ˆë‹¤. íšŒì˜ ë§í¬ ì¬ì „ì†¡ ìš”ì²­ë“œë¦½ë‹ˆë‹¤. íšŒì˜ ë§í¬ ì¬ì „\n",
            "----------------------------------------\n",
            "Q: í‚¤ì›Œë“œ 3ê°œ ì¶”ì¶œ: ì‹ ê·œ ê¸°ëŠ¥ ì¶œì‹œë¡œ ì‚¬ìš©ì ìœ ì…ì´ ì¦ê°€í–ˆë‹¤.\n",
            "A: â€¢ ì‹ ê·œ ê¸°ëŠ¥ ì¶œì‹œë¡œ ì‚¬ìš©ì ìœ ì…ì´ ì¦ê°€í–ˆë‹¤.â€¢ ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶œì‹œë¡œ ì‚¬ìš©ì ìœ ì…ì´ ì¦ê°€í–ˆë‹¤.â€¢ ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶œì‹œë¡œ ì‚¬ìš©ì ìœ ì…ì´ ì¦ê°€í–ˆë‹¤.â€¢ ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶œì‹œë¡œ ì‚¬ìš©ì ìœ ì…ì´ ì¦ê°€í–ˆë‹¤.â€¢ ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶œì‹œë¡œ ì‚¬ìš©ì ìœ ì…ì´ ì¦ê°€í–ˆë‹¤.â€¢ ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶œì‹œë¡œ ì‚¬ìš©ì ìœ ì…ì´ ì¦ê°€í–ˆë‹¤.â€¢ ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶œì‹œë¡œ ì‚¬ìš©ì ìœ ì…ì´ ì¦ê°€í–ˆë‹¤.â€¢ ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶œì‹œë¡œ ì‚¬ìš©ì ìœ ì…ì´ ì¦ê°€í–ˆë‹¤.\n",
            "----------------------------------------\n",
            "Q: JSONìœ¼ë¡œ ìš”ì•½(í‚¤: title, sentiment): 'ë°°ì†¡ ì§€ì—° ì´ìŠˆê°€ ì‚¬ë¼ì ¸ í‰ì ì´ ì˜¬ëë‹¤.'\n",
            "A: {\"title\":\"positive\":\"positive\"}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}\n",
            "----------------------------------------\n",
            "Q: ë‘ ë¬¸ì¥ ìš”ì•½(30ì ì´ë‚´): ì„œë²„ ê³¼ë¶€í•˜ë¡œ ì¥ì•  ë°œìƒ, ì„ì‹œ í™•ì¥ ì ìš©.\n",
            "A: ì„ì‹œ í™•ì¥ ì ìš©\n",
            "ì„ì‹œ í™•ì¥ ì ìš©\n",
            "ì„ì‹œ í™•ì¥ ì ìš©\n",
            "ì„ì‹œ í™•ì¥ ì ìš©\n",
            "ì„ì‹œ í™•ì¥ ì ìš©\n",
            "ì„ì‹œ í™•ì¥ ì ìš©\n",
            "ì„ì‹œ í™•ì¥ ì ìš©\n",
            "ì„ì‹œ í™•ì¥ ì ìš©\n",
            "ì„ì‹œ í™•ì¥ ì ìš©\n",
            "ì„ì‹œ í™•ì¥ ì ìš©\n",
            "ì„ì‹œ í™•ì¥ ì ìš©\n",
            "ì„ì‹œ í™•ì¥ ì ìš©\n",
            "ì„ì‹œ í™•ì¥ ì ìš©\n",
            "ì„ì‹œ í™•ì¥ ì ìš©\n",
            "ì„ì‹œ í™•ì¥ ì ìš©\n",
            "ì„ì‹œ í™•ì¥ ì ìš©\n",
            "ì„ì‹œ í™•ì¥ ì ìš©\n",
            "ì„ì‹œ í™•ì¥ ì ìš©\n",
            "ì„ì‹œ í™•ì¥ ì ìš©\n",
            "ì„ì‹œ í™•ì¥ ì ìš©\n",
            "----------------------------------------\n",
            "Q: ë¶ˆë¦¿ 3ê°œ: ì—…ë¬´ ì§‘ì¤‘ë ¥ì„ ë†’ì´ëŠ” ì‹¤ì²œ íŒ\n",
            "A: ì—…ë¬´ ì§‘ì¤‘ë ¥ì„ ë†’ì´ëŠ” ì‹¤ì²œ íŒ\n",
            "ì—…ë¬´ ìˆ˜í–‰ì—ì„œ ì„±ê³¼ë¥¼ ë‚´ê¸° ìœ„í•´ ë…¸ë ¥í•˜ëŠ” ìŠµê´€ì„ ê¸¸ëŸ¬ì•¼ í•œë‹¤.</d> #20180913 #ë¯¸ì„¸ë¨¼ì§€ #ì´ˆë¯¸ì„¸ë¨¼ì§€ #ì´ˆë¯¸ì„¸ë¨¼ì§€ê³µí¬ì¦ #ì´ˆë¯¸ì„¸ë¨¼ì§€ê³µí¬ì¦ #ì´ˆë¯¸ì„¸ë¨¼ì§€ê³µí¬ì¦ì˜ˆë°© #ì´ˆë¯¸ì„¸ë¨¼ì§€ê³µí¬\n",
            "----------------------------------------\n",
            "Q: ë¬¸ì¥ì„ ë” ë¶€ë“œëŸ½ê²Œ: ì¼ì • ë³€ê²½ ìš”ì²­ë“œë¦½ë‹ˆë‹¤.\n",
            "A: ëª©í‘œ í™•ì¸í•´ì¤˜. ì¼ì • ë³€ê²½ ìš”ì²­ë“œë¦½ë‹ˆë‹¤.^^ \n",
            "ëª©í‘œ í™•ì¸í•´ì¤˜. ì¼ì • ë³€ê²½ ìš”ì²­ë“œë¦½ë‹ˆë‹¤.^^ \n",
            "ëª©í‘œ í™•ì¸í•´ì¤˜. ì¼ì • ë³€ê²½ ìš”ì²­ë“œë¦½ë‹ˆë‹¤.^^ \n",
            "ëª©í‘œ í™•ì¸í•´ì¤˜. ì¼ì • ë³€ê²½ ìš”ì²­ë“œë¦½ë‹ˆë‹¤.^^ \n",
            "ëª©í‘œ í™•ì¸í•´ì¤˜. ì¼ì • ë³€ê²½ ìš”ì²­ë“œë¦½ë‹ˆë‹¤.^^ \n",
            "ëª©í‘œ í™•ì¸í•´ì¤˜. ì¼ì • ë³€ê²½ ìš”ì²­ë“œë¦½ë‹ˆë‹¤.^^ \n",
            "ëª©í‘œ í™•ì¸\n",
            "----------------------------------------\n",
            "Q: ê°„ë‹¨ ì •ì˜(í•œ ë¬¸ì¥): ê³¼ì†Œì í•©.\n",
            "A: ê³¼ì†Œì†Œì í•©ì€ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¼ì •í•œ\n",
            "----------------------------------------\n",
            "Q: ê°ì • ë¶„ë¥˜(ê¸/ë¶€ì •): 'í¬ì¥ì´ ì—‰ë§ì´ë¼ ì‹¤ë§ìŠ¤ëŸ¬ì› ë‹¤.'\n",
            "A: ë¶€ì •: 'ë¶€ì •'ì´ë‹¤.,\n",
            "ë¶€ì •ë§¥ì´ ì‹¬í•´ì¡Œë‹¤.,\n",
            "ë¶€ì •ë§¥ì´ ì‹¬í•´ì¡Œë‹¤.,\n",
            "ë¶€ì •ë§¥ì´ ì‹¬í•´ì¡Œë‹¤.,\n",
            "ë¶€ì •ë§¥ì´ ì‹¬í•´ì¡Œë‹¤.,\n",
            "ë¶€ì •ë§¥ì´ ì‹¬í•´ì¡Œë‹¤.,\n",
            "ë¶€ì •ë§¥ì´ ì‹¬í•´ì¡Œë‹¤.,\n",
            "ë¶€ì •ë§¥ì´ ì‹¬í•´ì¡Œë‹¤.,\n",
            "ë¶€ì •ë§¥ì´ ì‹¬í•´ì¡Œë‹¤.,\n",
            "ë¶€ì •ë§¥ì´ ì‹¬í•´ì¡Œë‹¤.,\n",
            "ë¶€ì •ë§¥ì´ ì‹¬í•´ì¡Œë‹¤.,\n",
            "ë¶€ì •ë§¥ì´ ì‹¬í•´ì¡Œë‹¤.,\n",
            "ë¶€ì •ë§¥ì´ ì‹¬í•´ì¡Œë‹¤.,\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# LoRA fine-tuning (Korean SFT) â€” Response-only loss, robust to Transformers versions\n",
        "# ================================\n",
        "!pip -q install -U transformers peft accelerate torch\n",
        "\n",
        "import os, random\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, set_seed\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# (ì¤‘ë³µ ë°©ì–´) W&B ë¹„í™œì„±í™”\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "\n",
        "# 1) ê¸°ë³¸ ì„¤ì •\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# 2) ëª¨ë¸/í† í¬ë‚˜ì´ì €\n",
        "# â”€â”€ í•œêµ­ì–´ í’ˆì§ˆ ê¶Œì¥: KoGPT2\n",
        "BASE_MODEL = \"skt/kogpt2-base-v2\"\n",
        "# í•„ìš” ì‹œ distilgpt2ë¡œ ë°”ê¾¸ì‹¤ ìˆ˜ ìˆì–´ìš”(í•œêµ­ì–´ ì„±ëŠ¥ì€ ë‚®ì„ ìˆ˜ ìˆìŒ):\n",
        "# BASE_MODEL = \"distilgpt2\"\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "    tok.pad_token_id = tok.eos_token_id\n",
        "\n",
        "# í…œí”Œë¦¿ íƒœê·¸(í† í° ê²½ê³„ ì•ˆì •)\n",
        "INST_TAG = \"### ì§€ì‹œë¬¸:\"\n",
        "RESP_TAG = \"### ì‘ë‹µ:\"\n",
        "tok.add_special_tokens({\"additional_special_tokens\":[INST_TAG, RESP_TAG]})\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
        "model.resize_token_embeddings(len(tok))\n",
        "model.to(device)\n",
        "\n",
        "# 3) LoRA ì„¤ì • (GPT-2 ê³„ì—´ í•µì‹¬ ì„ í˜•ì¸µë§Œ)\n",
        "#   - c_attn : Q/K/VI í•©ì„± ì„ í˜•ì¸µ\n",
        "#   - c_proj : ì–´í…ì…˜/MLP ì¶œë ¥ ì„ í˜•ì¸µ\n",
        "#   - c_fc   : MLP í™•ì¥ ì„ í˜•ì¸µ(mlp.c_fcë¥¼ ë„“ê²Œ ë§¤ì¹­)\n",
        "lora_cfg = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=8, lora_alpha=16, lora_dropout=0.05,\n",
        "    target_modules=[\"c_attn\",\"c_proj\",\"c_fc\"]\n",
        ")\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# 4) ë‹¤ì–‘í•œ í•œêµ­ì–´ ì§€ì‹œë¬¸ ë°ì´í„°(ì‘ì§€ë§Œ í­ë„“ê²Œ)\n",
        "def ex(prompt, answer):\n",
        "    return f\"{INST_TAG}\\n{prompt}\\n\\n{RESP_TAG}\\n{answer}{tok.eos_token}\"\n",
        "\n",
        "pairs = [\n",
        "    # ìš”ì•½/í•œì¤„ìš”ì•½\n",
        "    (\"í•œ ì¤„ ìš”ì•½: ì—°ì¤€ ë°œí‘œ ì´í›„ ì£¼ì‹ì‹œì¥ì´ ìƒìŠ¹í–ˆë‹¤.\", \"ì—°ì¤€ ë°œí‘œ ì´í›„ ì£¼ê°€ê°€ ìƒìŠ¹í–ˆë‹¤.\"),\n",
        "    (\"í•œ ì¤„ ìš”ì•½(ì´ëª¨ì§€ ê¸ˆì§€, 25ì ì´ë‚´): ë¹„ê°€ ì˜¤ê³  êµí†µ í˜¼ì¡ì´ ì‹¬í•´ì¡Œë‹¤.\", \"ë¹„ë¡œ ì¸í•´ êµí†µ í˜¼ì¡ì´ ì‹¬í•´ì¡Œë‹¤.\"),\n",
        "    # ë²ˆì—­\n",
        "    (\"ì˜ì–´ë¡œ ë²ˆì—­: ì •ë§ ê°ì‚¬í•©ë‹ˆë‹¤.\", \"Thank you very much.\"),\n",
        "    (\"ì˜ì–´ë¡œ ë²ˆì—­: ì‚¬ê³¼ëŠ” ê±´ê°•ì— ì¢‹ë‹¤.\", \"Apples are good for health.\"),\n",
        "    (\"í•œêµ­ì–´ë¡œ ë²ˆì—­: Keep it concise and clear.\", \"ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\"),\n",
        "    # í˜•ì‹/í†¤ ë³€í™˜\n",
        "    (\"ë©”ì¼ ì²« ë¬¸ì¥(ê³µì†ì²´): íšŒì˜ ì¼ì • ì¡°ìœ¨ì„ ì •ì¤‘íˆ ìš”ì²­\", \"ì•ˆë…•í•˜ì„¸ìš”. íšŒì˜ ì¼ì • ê´€ë ¨í•˜ì—¬ ê°€ëŠ¥í•œ ì‹œê°„ì„ ì—¬ì­™ê³ ì ì—°ë½ë“œë¦½ë‹ˆë‹¤.\"),\n",
        "    (\"ë¬¸ì¥ì„ ë” ê³µì†í•˜ê²Œ: ë‚´ì¼ê¹Œì§€ ìë£Œ ì£¼ì„¸ìš”.\", \"ë²ˆê±°ë¡œìš°ì‹œê² ì§€ë§Œ ë‚´ì¼ê¹Œì§€ ìë£Œë¥¼ ê³µìœ í•´ ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?\"),\n",
        "    (\"ë°˜ë§ë¡œ ë°”ê¾¸ê¸°: ì˜¤ëŠ˜ ì¼ì • í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤.\", \"ì˜¤ëŠ˜ ì¼ì • í™•ì¸í•´ì¤˜.\"),\n",
        "    # í‚¤ì›Œë“œ/ì¶”ì¶œ\n",
        "    (\"í‚¤ì›Œë“œ 3ê°œ ì¶”ì¶œ: ì¸ê³µì§€ëŠ¥ì´ ì˜ë£Œ ì˜ìƒ íŒë…ì„ ë³´ì¡°í•´ ì •í™•ë„ë¥¼ ë†’ì˜€ë‹¤.\", \"ì¸ê³µì§€ëŠ¥, ì˜ë£Œ ì˜ìƒ, ì •í™•ë„\"),\n",
        "    (\"í•´ì‹œíƒœê·¸ 3ê°œ ìƒì„±: ì—¬ë¦„ì²  ìˆ˜ë¶„ ì„­ì·¨ì˜ ì¤‘ìš”ì„±\", \"#ì—¬ë¦„ê±´ê°• #ìˆ˜ë¶„ì„­ì·¨ #ì—´ì‚¬ë³‘ì˜ˆë°©\"),\n",
        "    # êµ¬ì¡°í™”(JSON)\n",
        "    (\"JSONìœ¼ë¡œ ìš”ì•½(í‚¤: title, sentiment): 'ì„œë¹„ìŠ¤ ê°œì„  ê³µì§€ì— ê³ ê° ë°˜ì‘ì´ ëŒ€ì²´ë¡œ ê¸ì •ì ì´ë‹¤.'\",\n",
        "     '{\"title\":\"ì„œë¹„ìŠ¤ ê°œì„  ê³µì§€ ë°˜ì‘\",\"sentiment\":\"positive\"}'),\n",
        "    (\"í•­ëª©ë³„ ìš”ì•½(JSON: pros, cons): 'ë°°í„°ë¦¬ ì˜¤ë˜ê°€ë‚˜ ë¬´ê²Œê°€ ì¡°ê¸ˆ ë¬´ê²ë‹¤.'\",\n",
        "     '{\"pros\":[\"ë°°í„°ë¦¬ ì˜¤ë˜ê°\"],\"cons\":[\"ë¬´ê²Œê°€ ë‹¤ì†Œ ë¬´ê±°ì›€\"]}'),\n",
        "    # ê·œì¹™/ì œì•½\n",
        "    (\"ê·œì¹™: í•œ ë¬¸ì¥, ë§ˆì¹¨í‘œë¡œ ëë‚´ê¸° â€” ë”¥ëŸ¬ë‹ì„ ì •ì˜í•´ì¤˜.\", \"ë”¥ëŸ¬ë‹ì€ ë‹¤ì¸µ ì‹ ê²½ë§ìœ¼ë¡œ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ê¸°ê³„í•™ìŠµ ê¸°ë²•ì´ë‹¤.\"),\n",
        "    (\"ë¶ˆë¦¿ 3ê°œë¡œ ì •ë¦¬: ì§‘ì¤‘ë ¥ì„ ë†’ì´ëŠ” ë°©ë²•\", \"â€¢ ë°©í•´ ìš”ì†Œ ì œê±°\\nâ€¢ ì§§ì€ ëª©í‘œ ì„¤ì •\\nâ€¢ ì¼ì •í•œ íœ´ì‹\"),\n",
        "    # ì„¤ëª…/ì •ì˜\n",
        "    (\"í•œ ë¬¸ì¥ ì •ì˜: ë¨¸ì‹ ëŸ¬ë‹.\", \"ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ê¸°ìˆ ì´ë‹¤.\"),\n",
        "    (\"ì´ˆë³´ìì—ê²Œ ì„¤ëª…: ê³¼ì í•©ì´ ë­ì•¼?\", \"ê³¼ì í•©ì€ í•™ìŠµ ë°ì´í„°ì—ë§Œ ì§€ë‚˜ì¹˜ê²Œ ë§ì¶°ì ¸ ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” í˜„ìƒì´ë‹¤.\"),\n",
        "    # ë¶„ë¥˜/íŒì •(ê°„ë‹¨)\n",
        "    (\"ê°ì • ë¶„ë¥˜(ê¸/ë¶€ì • ì¤‘ í•˜ë‚˜): 'ì´ ì œí’ˆ ì •ë§ ë§ˆìŒì— ë“ ë‹¤.'\", \"ê¸ì •\"),\n",
        "    (\"ê°ì • ë¶„ë¥˜(ê¸/ë¶€ì • ì¤‘ í•˜ë‚˜): 'ë°°ì†¡ì´ ë„ˆë¬´ ëŠë ¤ì„œ ì‹¤ë§í–ˆë‹¤.'\", \"ë¶€ì •\"),\n",
        "    # ë³€í™˜/ìš”ì²­\n",
        "    (\"ë¬¸ì¥ì„ ë” ê°„ê²°í•˜ê²Œ: ë³¸ ê±´ê³¼ ê´€ë ¨í•˜ì—¬ ê²€í†  ê²°ê³¼ë¥¼ ì „ë‹¬ë“œë¦½ë‹ˆë‹¤.\", \"ê´€ë ¨ ê²€í†  ê²°ê³¼ë¥¼ ì „ë‹¬ë“œë¦½ë‹ˆë‹¤.\"),\n",
        "    (\"ìˆ«ìë§Œ ì¶”ì¶œ: ì£¼ë¬¸ë²ˆí˜¸ A-001-39\", \"00139\"),\n",
        "    # ì¼ì •/ë¹„ì¦ˆë‹ˆìŠ¤\n",
        "    (\"íšŒì˜ ì•„ì  ë‹¤ 3ê°œ ì œì•ˆ(í•œ ì¤„ì‹): ì˜¨ë¼ì¸ ì„¸ë¯¸ë‚˜ ì¤€ë¹„ íšŒì˜\", \"ëª©í‘œ í™•ì¸\\nì—­í•  ë¶„ë‹´\\níƒ€ì„ë¼ì¸ í™•ì •\"),\n",
        "    (\"ë‚©ê¸° ì—°ì¥ ìš”ì²­ ë©”ì¼ ì²« ë¬¸ì¥(ê³µì†ì²´, í•œ ë¬¸ì¥):\", \"ì•ˆë…•í•˜ì„¸ìš”, ë‚©ê¸° ì¼ì • ê´€ë ¨í•˜ì—¬ ë¶€ë“ì´í•˜ê²Œ ì—°ì¥ì„ ìš”ì²­ë“œë¦¬ê³ ì ì—°ë½ë“œë¦½ë‹ˆë‹¤.\"),\n",
        "    # ìŠ¤íƒ€ì¼/ì¬êµ¬ì„±\n",
        "    (\"ë¬¸ì¥ì„ ë” ëª…í™•í•˜ê²Œ: ë°ì´í„° ì²˜ë¦¬ ì†ë„ê°€ ë¶€ì¡±í•˜ë‹¤.\", \"ë°ì´í„° ì²˜ë¦¬ ì†ë„ê°€ ëŠë ¤ ì„±ëŠ¥ ì €í•˜ê°€ ë°œìƒí•œë‹¤.\"),\n",
        "    (\"ë‘ ë¬¸ì¥ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ: ëª¨ë¸ì€ ì •í™•í•˜ì§€ë§Œ ëŠë¦¬ë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì˜€ë‹¤.\", \"ì •í™•í•˜ì§€ë§Œ ëŠë¦° ëª¨ë¸ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì˜€ë‹¤.\"),\n",
        "]\n",
        "\n",
        "texts = []\n",
        "REPEAT = 20   # ë°ì´í„°ê°€ ëŠ˜ë©´ ì¤„ì´ì„¸ìš”(10~20 ê¶Œì¥)\n",
        "for _ in range(REPEAT):\n",
        "    for p, a in pairs:\n",
        "        texts.append(ex(p, a))\n",
        "\n",
        "# 5) ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ (í† í°í™”ëŠ” collatorì—ì„œ ë°°ì¹˜ë¡œ ì²˜ë¦¬)\n",
        "class TextOnlyDS(Dataset):\n",
        "    def __init__(self, texts): self.texts = texts\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, i): return {\"text\": self.texts[i]}\n",
        "\n",
        "split = int(len(texts)*0.9)\n",
        "train_ds = TextOnlyDS(texts[:split])\n",
        "val_ds   = TextOnlyDS(texts[split:])\n",
        "\n",
        "# 6) Collator: ë°°ì¹˜ í† í°í™” + \"ì‘ë‹µë§Œ ë¡œìŠ¤\" ë§ˆìŠ¤í‚¹ + ì•ˆì „ íŒ¨ë”©\n",
        "RESP_TAG_WITH_NL = RESP_TAG + \"\\n\"\n",
        "resp_ids = tok(RESP_TAG_WITH_NL, add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "def find_subseq(seq, sub):\n",
        "    L, l = len(seq), len(sub)\n",
        "    for i in range(L - l + 1):\n",
        "        if seq[i:i+l] == sub:\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "def response_only_collate(features, tokenizer=tok, max_length=384):\n",
        "    # í…ìŠ¤íŠ¸ ë°°ì¹˜ í† í°í™”\n",
        "    texts = [f[\"text\"] for f in features]\n",
        "    enc = tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    input_ids = enc[\"input_ids\"]\n",
        "    attn = enc[\"attention_mask\"]\n",
        "\n",
        "    # labels ì´ˆê¸°í™”(-100), \"ì‘ë‹µ:\" ì´í›„ë§Œ ì •ë‹µ ë³µì‚¬\n",
        "    labels = torch.full_like(input_ids, -100)\n",
        "    B = input_ids.size(0)\n",
        "    for i in range(B):\n",
        "        ids = input_ids[i].tolist()\n",
        "        pos = find_subseq(ids, resp_ids)\n",
        "        if pos != -1:\n",
        "            start = pos + len(resp_ids)\n",
        "            seq_len = int(attn[i].sum().item())  # pad ì œì™¸\n",
        "            start = min(start, seq_len)\n",
        "            labels[i, start:seq_len] = input_ids[i, start:seq_len]\n",
        "\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attn, \"labels\": labels}\n",
        "\n",
        "def collate(batch):\n",
        "    return response_only_collate(batch)\n",
        "\n",
        "# 7) í•™ìŠµ ì¸ì (ë²„ì „ í˜¸í™˜: ìµœì‹ /êµ¬ë²„ì „ ìë™ fallback)\n",
        "model.config.use_cache = False  # Trainer ê²½ê³  ë°©ì§€\n",
        "def make_args():\n",
        "    # ìµœì‹  ë²„ì „ìš©\n",
        "    try:\n",
        "        return TrainingArguments(\n",
        "            output_dir=\"./out_lora\",\n",
        "            overwrite_output_dir=True,\n",
        "            num_train_epochs=4,\n",
        "            per_device_train_batch_size=4,\n",
        "            gradient_accumulation_steps=2,\n",
        "            learning_rate=1e-4,\n",
        "            lr_scheduler_type=\"cosine\",\n",
        "            weight_decay=0.0,\n",
        "            logging_steps=20,\n",
        "            evaluation_strategy=\"steps\",  # â† ì¼ë¶€ êµ¬ë²„ì „ì—ì„œ ë¯¸ì§€ì›\n",
        "            eval_steps=100,\n",
        "            save_strategy=\"no\",\n",
        "            remove_unused_columns=False,\n",
        "            report_to=\"none\",\n",
        "            bf16=(torch.cuda.is_available() and getattr(torch.cuda, \"is_bf16_supported\", lambda: False)()),\n",
        "            fp16=(torch.cuda.is_available() and not getattr(torch.cuda, \"is_bf16_supported\", lambda: False)()),\n",
        "        )\n",
        "    except TypeError:\n",
        "        # êµ¬ë²„ì „ í˜¸í™˜(ë¬¸ì œë˜ëŠ” ì¸ì ì œê±°)\n",
        "        return TrainingArguments(\n",
        "            output_dir=\"./out_lora\",\n",
        "            overwrite_output_dir=True,\n",
        "            num_train_epochs=4,\n",
        "            per_device_train_batch_size=4,\n",
        "            gradient_accumulation_steps=2,\n",
        "            learning_rate=1e-4,\n",
        "            logging_steps=20,\n",
        "            remove_unused_columns=False,\n",
        "            # ì•„ë˜ ì¸ìë“¤ì€ êµ¬ë²„ì „ì—ì„œ ì—†ì„ ìˆ˜ ìˆì–´ ìƒëµ\n",
        "            # lr_scheduler_type, save_strategy, report_to, evaluation_strategy, eval_steps, bf16/fp16 ë“±\n",
        "        )\n",
        "\n",
        "args = make_args()\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, args=args,\n",
        "    train_dataset=train_ds, eval_dataset=val_ds,\n",
        "    data_collator=collate, tokenizer=tok\n",
        ")\n",
        "\n",
        "print(\"í•™ìŠµ ì‹œì‘...\")\n",
        "trainer.train()\n",
        "print(\"í•™ìŠµ ì¢…ë£Œ.\")\n",
        "\n",
        "# 8) ì–´ëŒ‘í„° ì €ì¥\n",
        "save_dir = \"./lora_adapter_ko_sft\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "model.save_pretrained(save_dir)\n",
        "tok.save_pretrained(save_dir)\n",
        "print(\"ì €ì¥:\", save_dir)\n",
        "\n",
        "# 9) ì¶”ë¡  í•¨ìˆ˜(í•™ìŠµ í…œí”Œë¦¿ê³¼ ë™ì¼í•˜ê²Œ!)\n",
        "@torch.inference_mode()\n",
        "def generate_ko(prompt, max_new_tokens=80, temperature=0.7, top_p=0.9, do_sample=False):\n",
        "    model.eval()\n",
        "    prefix = f\"{INST_TAG}\\n{prompt}\\n\\n{RESP_TAG}\\n\"\n",
        "    inputs = tok(prefix, return_tensors=\"pt\").to(model.device)\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=do_sample, temperature=temperature, top_p=top_p,\n",
        "        pad_token_id=tok.eos_token_id, eos_token_id=tok.eos_token_id\n",
        "    )\n",
        "    txt = tok.decode(out[0], skip_special_tokens=False)\n",
        "    return txt.split(RESP_TAG, 1)[-1].strip() if RESP_TAG in txt else txt.strip()\n",
        "\n",
        "print(\"\\n=== ë°ëª¨ ===\")\n",
        "tests = [\n",
        "    # í•™ìŠµ ë¶„í¬ì™€ ìœ ì‚¬(ì¼ë°˜í™” ì²´í¬)\n",
        "    \"í•œ ì¤„ ìš”ì•½: ë°°í„°ë¦¬ ìˆ˜ëª…ì´ ê¸¸ì–´ ì‚¬ìš©ì ë§Œì¡±ë„ê°€ ë†’ì•„ì¡Œë‹¤.\",\n",
        "    \"ì˜ì–´ë¡œ ë²ˆì—­: ë„ì›€ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.\",\n",
        "    \"ë©”ì¼ ì²« ë¬¸ì¥(ê³µì†ì²´): íšŒì˜ ë§í¬ ì¬ì „ì†¡ ìš”ì²­\",\n",
        "    \"í‚¤ì›Œë“œ 3ê°œ ì¶”ì¶œ: ì‹ ê·œ ê¸°ëŠ¥ ì¶œì‹œë¡œ ì‚¬ìš©ì ìœ ì…ì´ ì¦ê°€í–ˆë‹¤.\",\n",
        "    \"JSONìœ¼ë¡œ ìš”ì•½(í‚¤: title, sentiment): 'ë°°ì†¡ ì§€ì—° ì´ìŠˆê°€ ì‚¬ë¼ì ¸ í‰ì ì´ ì˜¬ëë‹¤.'\",\n",
        "    # ì‚´ì§ ìƒˆë¡œìš´ ìš”ì²­(ì „ì´ ì„±ëŠ¥ ì²´í¬)\n",
        "    \"ë‘ ë¬¸ì¥ ìš”ì•½(30ì ì´ë‚´): ì„œë²„ ê³¼ë¶€í•˜ë¡œ ì¥ì•  ë°œìƒ, ì„ì‹œ í™•ì¥ ì ìš©.\",\n",
        "    \"ë¶ˆë¦¿ 3ê°œ: ì—…ë¬´ ì§‘ì¤‘ë ¥ì„ ë†’ì´ëŠ” ì‹¤ì²œ íŒ\",\n",
        "    \"ë¬¸ì¥ì„ ë” ë¶€ë“œëŸ½ê²Œ: ì¼ì • ë³€ê²½ ìš”ì²­ë“œë¦½ë‹ˆë‹¤.\",\n",
        "    \"ê°„ë‹¨ ì •ì˜(í•œ ë¬¸ì¥): ê³¼ì†Œì í•©.\",\n",
        "    \"ê°ì • ë¶„ë¥˜(ê¸/ë¶€ì •): 'í¬ì¥ì´ ì—‰ë§ì´ë¼ ì‹¤ë§ìŠ¤ëŸ¬ì› ë‹¤.'\",\n",
        "]\n",
        "for p in tests:\n",
        "    print(\"Q:\", p)\n",
        "    print(\"A:\", generate_ko(p))\n",
        "    print(\"-\"*40)\n"
      ]
    }
  ]
}