{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c6ff49e77c9d4d7b8697d68f73ead3f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_599b1fc22d9e43e4aefbe11819bf1974",
              "IPY_MODEL_45f0a0673f1a427db3b02b4c5aadfc5c",
              "IPY_MODEL_4e75c414128c4f6c8f537eb5d936914c"
            ],
            "layout": "IPY_MODEL_43decd5a486e4dbe828a1b771709440b"
          }
        },
        "599b1fc22d9e43e4aefbe11819bf1974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4ac513483704970929415adf31e1034",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_467ce44b73ed42108d450d502968a996",
            "value": "Map:‚Äá100%"
          }
        },
        "45f0a0673f1a427db3b02b4c5aadfc5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00a22867092d4c1fac1424c7296d23c1",
            "max": 240,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ad52d34a39e4bbf94af75fe9de0e0b7",
            "value": 240
          }
        },
        "4e75c414128c4f6c8f537eb5d936914c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31a143661ac549d6a671586e2395c42e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_739bf0ec4618410f894701b53720a920",
            "value": "‚Äá240/240‚Äá[00:00&lt;00:00,‚Äá4378.09‚Äáexamples/s]"
          }
        },
        "43decd5a486e4dbe828a1b771709440b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4ac513483704970929415adf31e1034": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "467ce44b73ed42108d450d502968a996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00a22867092d4c1fac1424c7296d23c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ad52d34a39e4bbf94af75fe9de0e0b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31a143661ac549d6a671586e2395c42e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "739bf0ec4618410f894701b53720a920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747,
          "referenced_widgets": [
            "c6ff49e77c9d4d7b8697d68f73ead3f2",
            "599b1fc22d9e43e4aefbe11819bf1974",
            "45f0a0673f1a427db3b02b4c5aadfc5c",
            "4e75c414128c4f6c8f537eb5d936914c",
            "43decd5a486e4dbe828a1b771709440b",
            "c4ac513483704970929415adf31e1034",
            "467ce44b73ed42108d450d502968a996",
            "00a22867092d4c1fac1424c7296d23c1",
            "6ad52d34a39e4bbf94af75fe9de0e0b7",
            "31a143661ac549d6a671586e2395c42e",
            "739bf0ec4618410f894701b53720a920"
          ]
        },
        "id": "p3iU3JSH5J0M",
        "outputId": "1e466ec2-4130-467d-dca0-e0040482ebc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,179,648 || all params: 126,345,984 || trainable%: 0.9337\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/240 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6ff49e77c9d4d7b8697d68f73ead3f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-173637014.py:155: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 51200, 'bos_token_id': 51200, 'pad_token_id': 51200}.\n",
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [120/120 07:28, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>4.413000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.181700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.125500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.423000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.092000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.942600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.850200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.861500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.843900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.833900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.819800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.821000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Îç∞Î™® Ï∂úÎ†• ===\n",
            "ÏÑúÏö∏ ÎÇ¥Ïùº ÎÇ†Ïî®Î•º Ìïú Ï§ÑÎ°ú ÏöîÏïΩÌï¥Ï§ò. -> ÏÑúÏö∏ÏùÄ ÎßëÍ≥† ÎÇÆÍ∏∞Ïò® 28ÎèÑ, ÎØ∏ÏÑ∏Î®ºÏßÄ Î≥¥ÌÜµÏûÖÎãàÎã§. ^^ „ÄèÍ≥† ÎßêÌï©ÎãàÎã§. ^^ „ÄÇÏÑúÏö∏ÏùÄ ÎßëÍ≥† ÎÇÆÍ∏∞Ïò® 28ÎèÑ, ÎØ∏ÏÑ∏Î®ºÏßÄ Î≥¥ÌÜµÏûÖÎãàÎã§. ^^ „ÄÇÏÑúÏö∏ÏùÄ ÎßëÍ≥† ÎÇÆÍ∏∞Ïò® 28ÎèÑ, ÎØ∏ÏÑ∏Î®ºÏßÄ Î≥¥ÌÜµÏûÖÎãàÎã§. ^^ „ÄÇÏÑúÏö∏ÏùÄ ÎßëÍ≥† ÎÇÆÍ∏∞Ïò® 28ÎèÑ, ÎØ∏ÏÑ∏Î®ºÏßÄ Î≥¥ÌÜµÏûÖÎãàÎã§. ^^ „ÄÇÏÑúÏö∏ÏùÄ ÎßëÍ≥† ÎÇÆÍ∏∞Ïò® 28ÎèÑ, ÎØ∏ÏÑ∏Î®º\n",
            "Ï†ïÏ§ëÌïú ÏùºÏ†ï Ï°∞Ïú® Î©îÏùº Ï≤´ Î¨∏Ïû• Ïç®Ï§ò. -> ÏïàÎÖïÌïòÏÑ∏Ïöî, Í∑ÄÏÇ¨ ÌîÑÎ°úÏ†ùÌä∏Ïùò ÏÑ±Í≥µÏùÑ Í±∞ÎëêÏóàÏäµÎãàÎã§. health.presented.go.kr/spectes are good for health.go.kr/spectes.go.kr/spare good for health.go.kr)ÏóêÏÑú ÌôïÏù∏ÌïòÏÑ∏Ïöî. health.go.kr/spectes.go.kr\n",
            "Ìïú Ï§ÑÎ°ú ÏöîÏïΩ: 'ÎèÑÎ°ú ÌôïÏû• Í≥µÏÇ¨Í∞Ä ÏßÄÏó∞ÎêòÍ≥† ÏûàÎã§.' -> ÎèÑÎ°ú ÌôïÏû• Í≥µÏÇ¨Í∞Ä ÏßÄÏó∞ÎêòÏóàÎã§.\"\n",
            "ÎèÑÎ°ú ÌôïÏû• Í≥µÏÇ¨Í∞Ä ÏßÄÏó∞ÎêòÍ≥† ÏûàÎã§. health. health. health. health. health. health. health. health. health. health. health. health. health. heal\n",
            "Í∞ÑÎã® Î≤àÏó≠: 'Ìè¨ÎèÑÎäî Ìï≠ÏÇ∞Ìôî Ìö®Í≥ºÍ∞Ä ÏûàÎã§' -> ÏòÅÏñ¥ -> Apples are good for health. health. health. health. health. health. health. are good for health. health. health. health. health. health.\n"
          ]
        }
      ],
      "source": [
        "# üß© Causal LM SFT with LoRA on KoGPT2\n",
        "!pip -q install -U transformers datasets peft accelerate sentencepiece\n",
        "\n",
        "import os, random, numpy as np, torch\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# ========== 0) Repro & perf ==========\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "# ========== 1) Tokenizer / Model ==========\n",
        "BASE_MODEL = \"skt/kogpt2-base-v2\"\n",
        "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "\n",
        "# ÌÖúÌîåÎ¶øÏö© ÌäπÏàò ÌÜ†ÌÅ∞(ÌÜ†ÌÅ∞ Í≤ΩÍ≥Ñ ÏïàÏ†ïÌôî)\n",
        "B_INST = \"### Instruction:\"\n",
        "B_RESP = \"### Response:\"\n",
        "SPECIAL_TOKENS = {\"additional_special_tokens\": [B_INST, B_RESP]}\n",
        "tok.add_special_tokens(SPECIAL_TOKENS)\n",
        "\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
        "model.resize_token_embeddings(len(tok))\n",
        "\n",
        "# ========== 2) LoRA ÏÑ§Ï†ï ==========\n",
        "peft_conf = LoraConfig(\n",
        "    r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    # GPT-2 Í≥ÑÏó¥ Ìò∏Ìôò ÌÉÄÍπÉ Î™®Îìà\n",
        "    target_modules=[\"c_attn\", \"c_proj\", \"mlp.c_fc\", \"mlp.c_proj\"]\n",
        ")\n",
        "model = get_peft_model(model, peft_conf)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ========== 3) ÏÜåÌòï ÏßÄÏãúÎ¨∏ Îç∞Ïù¥ÌÑ∞ÏÖã ==========\n",
        "pairs = [\n",
        "    {\"prompt\":\"ÎÇ†Ïî® ÏöîÏïΩ Í∑úÏπô: 1) Ìïú Ï§Ñ 2) Ïù¥Î™®ÏßÄ Í∏àÏßÄ\\nÏÑúÏö∏ Ïò§Îäò ÎÇ†Ïî® ÏïåÎ†§Ï§ò.\",\n",
        "     \"response\":\"ÏÑúÏö∏ÏùÄ ÎßëÍ≥† ÎÇÆÍ∏∞Ïò® 28ÎèÑ, ÎØ∏ÏÑ∏Î®ºÏßÄ Î≥¥ÌÜµÏûÖÎãàÎã§.\"},\n",
        "    {\"prompt\":\"Ìïú Ï§ÑÎ°ú ÏöîÏïΩ: 'ÎåÄÏ§ëÍµêÌÜµ ÏöîÍ∏à Ïù∏ÏÉÅ ÎÖºÏùòÍ∞Ä ÏßÑÌñâ Ï§ëÏù¥Îã§.'\",\n",
        "     \"response\":\"ÎåÄÏ§ëÍµêÌÜµ ÏöîÍ∏à Ïù∏ÏÉÅÏù¥ ÎÖºÏùò Îã®Í≥ÑÏóê ÏûàÎã§.\"},\n",
        "    {\"prompt\":\"Í∞ÑÎã® Î≤àÏó≠: 'ÏÇ¨Í≥ºÎäî Í±¥Í∞ïÏóê Ï¢ãÎã§' -> ÏòÅÏñ¥\",\n",
        "     \"response\":\"Apples are good for health.\"},\n",
        "    {\"prompt\":\"ÎπÑÏ¶àÎãàÏä§ Ïù¥Î©îÏùº Ï≤´ Î¨∏Ïû• Ï†úÏïà(ÌïúÍµ≠Ïñ¥, Í≥µÏÜêÏ≤¥): ÎÇ©Í∏∞ Ïó∞Ïû• ÏöîÏ≤≠\",\n",
        "     \"response\":\"ÏïàÎÖïÌïòÏÑ∏Ïöî, Í∑ÄÏÇ¨ ÌîÑÎ°úÏ†ùÌä∏Ïùò ÎÇ©Í∏∞ ÏùºÏ†ï Í¥ÄÎ†®ÌïòÏó¨ Ï°∞Ïã¨Ïä§ÎüΩÍ≤å Ïó∞Ïû•ÏùÑ ÏöîÏ≤≠ÎìúÎ¶ΩÎãàÎã§.\"},\n",
        "]\n",
        "\n",
        "def format_example(p, r, eos):\n",
        "    return f\"{B_INST}\\n{p}\\n\\n{B_RESP}\\n{r}{eos}\"\n",
        "\n",
        "train_texts = [format_example(d[\"prompt\"], d[\"response\"], tok.eos_token) for d in pairs]\n",
        "\n",
        "# ÏÜåÎüâ Îç∞Ïù¥ÌÑ∞ ‚Üí upsamplingÏúºÎ°ú ÏàòÎ†¥ ÏïàÏ†ïÌôî\n",
        "REPEAT = 60  # ÌïÑÏöî Ïãú 30~200ÏóêÏÑú Ï°∞Ï†à\n",
        "train_texts = train_texts * REPEAT\n",
        "\n",
        "raw_ds = Dataset.from_dict({\"text\": train_texts})\n",
        "\n",
        "# ========== 4) ÌÜ†ÌÅ¨ÎÇòÏù¥Ï¶à + Î†àÏù¥Î∏î ÎßàÏä§ÌÇπ(ÏùëÎãµÎßå loss) ==========\n",
        "def build_features(batch):\n",
        "    texts = batch[\"text\"]\n",
        "    input_ids_list, attn_list, labels_list = [], [], []\n",
        "    # \"### Response:\\n\" ÌÜ†ÌÅ∞ ÏãúÌÄÄÏä§\n",
        "    resp_tag_ids = tok(B_RESP + \"\\n\", add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "    def find_subseq(seq, sub):\n",
        "        L, l = len(seq), len(sub)\n",
        "        for i in range(L - l + 1):\n",
        "            if seq[i:i+l] == sub:\n",
        "                return i\n",
        "        return -1\n",
        "\n",
        "    for t in texts:\n",
        "        enc = tok(t, max_length=512, truncation=True)\n",
        "        input_ids = enc[\"input_ids\"]\n",
        "        attn = enc[\"attention_mask\"]\n",
        "\n",
        "        idx = find_subseq(input_ids, resp_tag_ids)\n",
        "        if idx == -1:\n",
        "            # ÏïàÏ†ÑÏû•Ïπò: ÌÉúÍ∑∏Î•º Î™ª Ï∞æÏúºÎ©¥ Ï†ÑÏ≤¥ -100\n",
        "            labels = [-100] * len(input_ids)\n",
        "        else:\n",
        "            start = idx + len(resp_tag_ids)\n",
        "            labels = [-100] * start + input_ids[start:]\n",
        "\n",
        "        input_ids_list.append(input_ids)\n",
        "        attn_list.append(attn)\n",
        "        labels_list.append(labels)\n",
        "\n",
        "    return {\"input_ids\": input_ids_list, \"attention_mask\": attn_list, \"labels\": labels_list}\n",
        "\n",
        "ds_tok = raw_ds.map(build_features, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# ========== 5) Collator: labelsÎäî ÏàòÎèô Ìå®Îî© ==========\n",
        "@dataclass\n",
        "class ResponseOnlyCollator:\n",
        "    tokenizer: AutoTokenizer\n",
        "    pad_to_multiple_of: int = 8\n",
        "\n",
        "    def __call__(self, features: List[Dict]):\n",
        "        # 1) labelsÎ•º Ïû†Ïãú Î∂ÑÎ¶¨Ìï¥ tokenizer.padÍ∞Ä Í±¥ÎìúÎ¶¨ÏßÄ ÏïäÍ≤å Ìï®\n",
        "        labels_list = [f.pop(\"labels\") for f in features]\n",
        "\n",
        "        # 2) ÏûÖÎ†•Îßå Ìå®Îî©\n",
        "        batch = self.tokenizer.pad(\n",
        "            features,\n",
        "            padding=True,\n",
        "            max_length=None,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # 3) labels ÏàòÎèô Ìå®Îî©(-100) ÌõÑ ÌÖêÏÑúÌôî\n",
        "        max_len = batch[\"input_ids\"].size(1)\n",
        "        padded_labels = []\n",
        "        for lab in labels_list:\n",
        "            if len(lab) < max_len:\n",
        "                lab = lab + [-100] * (max_len - len(lab))\n",
        "            else:\n",
        "                lab = lab[:max_len]\n",
        "            padded_labels.append(lab)\n",
        "        batch[\"labels\"] = torch.tensor(padded_labels, dtype=torch.long)\n",
        "        return batch\n",
        "\n",
        "collator = ResponseOnlyCollator(tok)\n",
        "\n",
        "# ========== 6) ÌïôÏäµ ÏÑ∏ÌåÖ ==========\n",
        "try:\n",
        "    bf16_ok = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "except Exception:\n",
        "    bf16_ok = False\n",
        "fp16_ok = torch.cuda.is_available() and not bf16_ok\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./kogpt2-lora-sft\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1e-4,          # ÏÜåÎüâ Îç∞Ïù¥ÌÑ∞ ‚Üí ÎÇÆÍ≤å\n",
        "    num_train_epochs=8,          # ÌïÑÏöî Ïãú 6~20 ÏÇ¨Ïù¥ÏóêÏÑú Ï°∞Ï†à\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    weight_decay=0.0,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"no\",\n",
        "    bf16=bf16_ok,\n",
        "    fp16=fp16_ok,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=ds_tok,\n",
        "    tokenizer=tok,\n",
        "    data_collator=collator\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# ========== 7) Ï∂îÎ°† Ïú†Ìã∏ (ÌïôÏäµ ÌÖúÌîåÎ¶øÍ≥º ÎèôÏùº) ==========\n",
        "def generate(prompt, max_new_tokens=80, do_sample=False, top_p=0.9, temperature=0.7):\n",
        "    text = f\"{B_INST}\\n{prompt}\\n\\n{B_RESP}\\n\"\n",
        "    inputs = tok(text, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=do_sample,    # ÏÜåÎüâ Îç∞Ïù¥ÌÑ∞ Í≥ºÏ†ÅÌï© ‚Üí Í∏∞Î≥∏ FalseÎ°ú Î≥¥ÏàòÏ†Å ÏÉùÏÑ±\n",
        "            top_p=top_p, temperature=temperature,\n",
        "            pad_token_id=tok.eos_token_id,\n",
        "            eos_token_id=tok.eos_token_id\n",
        "        )\n",
        "    full = tok.decode(out[0], skip_special_tokens=False)\n",
        "    # ÏùëÎãµ Î∂ÄÎ∂ÑÎßå Ï∂îÏ∂ú\n",
        "    if B_RESP in full:\n",
        "        ans = full.split(B_RESP, 1)[-1].strip()\n",
        "    else:\n",
        "        ans = full\n",
        "    return ans.strip()\n",
        "\n",
        "print(\"=== Îç∞Î™® Ï∂úÎ†• ===\")\n",
        "tests = [\n",
        "    \"ÏÑúÏö∏ ÎÇ¥Ïùº ÎÇ†Ïî®Î•º Ìïú Ï§ÑÎ°ú ÏöîÏïΩÌï¥Ï§ò.\",\n",
        "    \"Ï†ïÏ§ëÌïú ÏùºÏ†ï Ï°∞Ïú® Î©îÏùº Ï≤´ Î¨∏Ïû• Ïç®Ï§ò.\",\n",
        "    \"Ìïú Ï§ÑÎ°ú ÏöîÏïΩ: 'ÎèÑÎ°ú ÌôïÏû• Í≥µÏÇ¨Í∞Ä ÏßÄÏó∞ÎêòÍ≥† ÏûàÎã§.'\",\n",
        "    \"Í∞ÑÎã® Î≤àÏó≠: 'Ìè¨ÎèÑÎäî Ìï≠ÏÇ∞Ìôî Ìö®Í≥ºÍ∞Ä ÏûàÎã§' -> ÏòÅÏñ¥\",\n",
        "]\n",
        "for p in tests:\n",
        "    print(p, \"->\", generate(p))\n"
      ]
    }
  ]
}