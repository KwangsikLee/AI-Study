{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c6ff49e77c9d4d7b8697d68f73ead3f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_599b1fc22d9e43e4aefbe11819bf1974",
              "IPY_MODEL_45f0a0673f1a427db3b02b4c5aadfc5c",
              "IPY_MODEL_4e75c414128c4f6c8f537eb5d936914c"
            ],
            "layout": "IPY_MODEL_43decd5a486e4dbe828a1b771709440b"
          }
        },
        "599b1fc22d9e43e4aefbe11819bf1974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4ac513483704970929415adf31e1034",
            "placeholder": "​",
            "style": "IPY_MODEL_467ce44b73ed42108d450d502968a996",
            "value": "Map: 100%"
          }
        },
        "45f0a0673f1a427db3b02b4c5aadfc5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00a22867092d4c1fac1424c7296d23c1",
            "max": 240,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ad52d34a39e4bbf94af75fe9de0e0b7",
            "value": 240
          }
        },
        "4e75c414128c4f6c8f537eb5d936914c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31a143661ac549d6a671586e2395c42e",
            "placeholder": "​",
            "style": "IPY_MODEL_739bf0ec4618410f894701b53720a920",
            "value": " 240/240 [00:00&lt;00:00, 4378.09 examples/s]"
          }
        },
        "43decd5a486e4dbe828a1b771709440b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4ac513483704970929415adf31e1034": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "467ce44b73ed42108d450d502968a996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00a22867092d4c1fac1424c7296d23c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ad52d34a39e4bbf94af75fe9de0e0b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31a143661ac549d6a671586e2395c42e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "739bf0ec4618410f894701b53720a920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747,
          "referenced_widgets": [
            "c6ff49e77c9d4d7b8697d68f73ead3f2",
            "599b1fc22d9e43e4aefbe11819bf1974",
            "45f0a0673f1a427db3b02b4c5aadfc5c",
            "4e75c414128c4f6c8f537eb5d936914c",
            "43decd5a486e4dbe828a1b771709440b",
            "c4ac513483704970929415adf31e1034",
            "467ce44b73ed42108d450d502968a996",
            "00a22867092d4c1fac1424c7296d23c1",
            "6ad52d34a39e4bbf94af75fe9de0e0b7",
            "31a143661ac549d6a671586e2395c42e",
            "739bf0ec4618410f894701b53720a920"
          ]
        },
        "id": "p3iU3JSH5J0M",
        "outputId": "1e466ec2-4130-467d-dca0-e0040482ebc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,179,648 || all params: 126,345,984 || trainable%: 0.9337\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/240 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6ff49e77c9d4d7b8697d68f73ead3f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-173637014.py:155: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 51200, 'bos_token_id': 51200, 'pad_token_id': 51200}.\n",
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [120/120 07:28, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>4.413000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.181700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.125500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.423000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.092000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.942600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.850200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.861500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.843900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.833900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.819800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.821000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 데모 출력 ===\n",
            "서울 내일 날씨를 한 줄로 요약해줘. -> 서울은 맑고 낮기온 28도, 미세먼지 보통입니다. ^^ 』고 말합니다. ^^ 。서울은 맑고 낮기온 28도, 미세먼지 보통입니다. ^^ 。서울은 맑고 낮기온 28도, 미세먼지 보통입니다. ^^ 。서울은 맑고 낮기온 28도, 미세먼지 보통입니다. ^^ 。서울은 맑고 낮기온 28도, 미세먼\n",
            "정중한 일정 조율 메일 첫 문장 써줘. -> 안녕하세요, 귀사 프로젝트의 성공을 거두었습니다. health.presented.go.kr/spectes are good for health.go.kr/spectes.go.kr/spare good for health.go.kr)에서 확인하세요. health.go.kr/spectes.go.kr\n",
            "한 줄로 요약: '도로 확장 공사가 지연되고 있다.' -> 도로 확장 공사가 지연되었다.\"\n",
            "도로 확장 공사가 지연되고 있다. health. health. health. health. health. health. health. health. health. health. health. health. health. heal\n",
            "간단 번역: '포도는 항산화 효과가 있다' -> 영어 -> Apples are good for health. health. health. health. health. health. health. are good for health. health. health. health. health. health.\n"
          ]
        }
      ],
      "source": [
        "# 🧩 Causal LM SFT with LoRA on KoGPT2\n",
        "!pip -q install -U transformers datasets peft accelerate sentencepiece\n",
        "\n",
        "import os, random, numpy as np, torch\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# ========== 0) Repro & perf ==========\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "# ========== 1) Tokenizer / Model ==========\n",
        "BASE_MODEL = \"skt/kogpt2-base-v2\"\n",
        "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "\n",
        "# 템플릿용 특수 토큰(토큰 경계 안정화)\n",
        "B_INST = \"### Instruction:\"\n",
        "B_RESP = \"### Response:\"\n",
        "SPECIAL_TOKENS = {\"additional_special_tokens\": [B_INST, B_RESP]}\n",
        "tok.add_special_tokens(SPECIAL_TOKENS)\n",
        "\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
        "model.resize_token_embeddings(len(tok))\n",
        "\n",
        "# ========== 2) LoRA 설정 ==========\n",
        "peft_conf = LoraConfig(\n",
        "    r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    # GPT-2 계열 호환 타깃 모듈\n",
        "    target_modules=[\"c_attn\", \"c_proj\", \"mlp.c_fc\", \"mlp.c_proj\"]\n",
        ")\n",
        "model = get_peft_model(model, peft_conf)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ========== 3) 소형 지시문 데이터셋 ==========\n",
        "pairs = [\n",
        "    {\"prompt\":\"날씨 요약 규칙: 1) 한 줄 2) 이모지 금지\\n서울 오늘 날씨 알려줘.\",\n",
        "     \"response\":\"서울은 맑고 낮기온 28도, 미세먼지 보통입니다.\"},\n",
        "    {\"prompt\":\"한 줄로 요약: '대중교통 요금 인상 논의가 진행 중이다.'\",\n",
        "     \"response\":\"대중교통 요금 인상이 논의 단계에 있다.\"},\n",
        "    {\"prompt\":\"간단 번역: '사과는 건강에 좋다' -> 영어\",\n",
        "     \"response\":\"Apples are good for health.\"},\n",
        "    {\"prompt\":\"비즈니스 이메일 첫 문장 제안(한국어, 공손체): 납기 연장 요청\",\n",
        "     \"response\":\"안녕하세요, 귀사 프로젝트의 납기 일정 관련하여 조심스럽게 연장을 요청드립니다.\"},\n",
        "]\n",
        "\n",
        "def format_example(p, r, eos):\n",
        "    return f\"{B_INST}\\n{p}\\n\\n{B_RESP}\\n{r}{eos}\"\n",
        "\n",
        "train_texts = [format_example(d[\"prompt\"], d[\"response\"], tok.eos_token) for d in pairs]\n",
        "\n",
        "# 소량 데이터 → upsampling으로 수렴 안정화\n",
        "REPEAT = 60  # 필요 시 30~200에서 조절\n",
        "train_texts = train_texts * REPEAT\n",
        "\n",
        "raw_ds = Dataset.from_dict({\"text\": train_texts})\n",
        "\n",
        "# ========== 4) 토크나이즈 + 레이블 마스킹(응답만 loss) ==========\n",
        "def build_features(batch):\n",
        "    texts = batch[\"text\"]\n",
        "    input_ids_list, attn_list, labels_list = [], [], []\n",
        "    # \"### Response:\\n\" 토큰 시퀀스\n",
        "    resp_tag_ids = tok(B_RESP + \"\\n\", add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "    def find_subseq(seq, sub):\n",
        "        L, l = len(seq), len(sub)\n",
        "        for i in range(L - l + 1):\n",
        "            if seq[i:i+l] == sub:\n",
        "                return i\n",
        "        return -1\n",
        "\n",
        "    for t in texts:\n",
        "        enc = tok(t, max_length=512, truncation=True)\n",
        "        input_ids = enc[\"input_ids\"]\n",
        "        attn = enc[\"attention_mask\"]\n",
        "\n",
        "        idx = find_subseq(input_ids, resp_tag_ids)\n",
        "        if idx == -1:\n",
        "            # 안전장치: 태그를 못 찾으면 전체 -100\n",
        "            labels = [-100] * len(input_ids)\n",
        "        else:\n",
        "            start = idx + len(resp_tag_ids)\n",
        "            labels = [-100] * start + input_ids[start:]\n",
        "\n",
        "        input_ids_list.append(input_ids)\n",
        "        attn_list.append(attn)\n",
        "        labels_list.append(labels)\n",
        "\n",
        "    return {\"input_ids\": input_ids_list, \"attention_mask\": attn_list, \"labels\": labels_list}\n",
        "\n",
        "ds_tok = raw_ds.map(build_features, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# ========== 5) Collator: labels는 수동 패딩 ==========\n",
        "@dataclass\n",
        "class ResponseOnlyCollator:\n",
        "    tokenizer: AutoTokenizer\n",
        "    pad_to_multiple_of: int = 8\n",
        "\n",
        "    def __call__(self, features: List[Dict]):\n",
        "        # 1) labels를 잠시 분리해 tokenizer.pad가 건드리지 않게 함\n",
        "        labels_list = [f.pop(\"labels\") for f in features]\n",
        "\n",
        "        # 2) 입력만 패딩\n",
        "        batch = self.tokenizer.pad(\n",
        "            features,\n",
        "            padding=True,\n",
        "            max_length=None,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # 3) labels 수동 패딩(-100) 후 텐서화\n",
        "        max_len = batch[\"input_ids\"].size(1)\n",
        "        padded_labels = []\n",
        "        for lab in labels_list:\n",
        "            if len(lab) < max_len:\n",
        "                lab = lab + [-100] * (max_len - len(lab))\n",
        "            else:\n",
        "                lab = lab[:max_len]\n",
        "            padded_labels.append(lab)\n",
        "        batch[\"labels\"] = torch.tensor(padded_labels, dtype=torch.long)\n",
        "        return batch\n",
        "\n",
        "collator = ResponseOnlyCollator(tok)\n",
        "\n",
        "# ========== 6) 학습 세팅 ==========\n",
        "try:\n",
        "    bf16_ok = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "except Exception:\n",
        "    bf16_ok = False\n",
        "fp16_ok = torch.cuda.is_available() and not bf16_ok\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./kogpt2-lora-sft\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1e-4,          # 소량 데이터 → 낮게\n",
        "    num_train_epochs=8,          # 필요 시 6~20 사이에서 조절\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    weight_decay=0.0,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"no\",\n",
        "    bf16=bf16_ok,\n",
        "    fp16=fp16_ok,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=ds_tok,\n",
        "    tokenizer=tok,\n",
        "    data_collator=collator\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# ========== 7) 추론 유틸 (학습 템플릿과 동일) ==========\n",
        "def generate(prompt, max_new_tokens=80, do_sample=False, top_p=0.9, temperature=0.7):\n",
        "    text = f\"{B_INST}\\n{prompt}\\n\\n{B_RESP}\\n\"\n",
        "    inputs = tok(text, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=do_sample,    # 소량 데이터 과적합 → 기본 False로 보수적 생성\n",
        "            top_p=top_p, temperature=temperature,\n",
        "            pad_token_id=tok.eos_token_id,\n",
        "            eos_token_id=tok.eos_token_id\n",
        "        )\n",
        "    full = tok.decode(out[0], skip_special_tokens=False)\n",
        "    # 응답 부분만 추출\n",
        "    if B_RESP in full:\n",
        "        ans = full.split(B_RESP, 1)[-1].strip()\n",
        "    else:\n",
        "        ans = full\n",
        "    return ans.strip()\n",
        "\n",
        "print(\"=== 데모 출력 ===\")\n",
        "tests = [\n",
        "    \"서울 내일 날씨를 한 줄로 요약해줘.\",\n",
        "    \"정중한 일정 조율 메일 첫 문장 써줘.\",\n",
        "    \"한 줄로 요약: '도로 확장 공사가 지연되고 있다.'\",\n",
        "    \"간단 번역: '포도는 항산화 효과가 있다' -> 영어\",\n",
        "]\n",
        "for p in tests:\n",
        "    print(p, \"->\", generate(p))\n"
      ]
    }
  ]
}