#!/usr/bin/env python3
"""
CollegeQASystem - êµ¬ì¶•ëœ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì‚¬ìš©í•œ ì§ˆë¬¸ë‹µë³€ ì „ë‹´ í´ë˜ìŠ¤

Author: kwangsiklee  
Version: 0.1.0
"""

import json
from pathlib import Path
from typing import Dict, Any

# LangChain imports
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate


class CollegeQASystem:
    """êµ¬ì¶•ëœ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì‚¬ìš©í•œ ì§ˆë¬¸ë‹µë³€ ì „ë‹´ í´ë˜ìŠ¤"""
    
    def __init__(self, vector_db_dir: str):
        self.vector_db_dir = Path(vector_db_dir)
        
        # LLM ì„¤ì • (í•„ìš”ì‹œ ì§€ì—° ë¡œë”©)
        self.llm = None
        self.embeddings = None
        
        # ë²¡í„° ìŠ¤í† ì–´ì™€ QA ì²´ì¸
        self.vector_store = None
        self.qa_chain = None
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.setup_prompt_template()
        
        print(f"CollegeQASystem ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"ë²¡í„° DB ë””ë ‰í† ë¦¬: {self.vector_db_dir}")
    
    def initialize_llm_components(self):
        """LLM êµ¬ì„±ìš”ì†Œ ì§€ì—° ì´ˆê¸°í™”"""
        if self.llm is None:
            print("ğŸ¤– LLM êµ¬ì„±ìš”ì†Œ ì´ˆê¸°í™” ì¤‘...")
            
            # OpenAI ì„¤ì •
            self.llm = ChatOpenAI(
                model="gpt-4o-mini",
                temperature=0.2,
                max_tokens=800
            )
            
            # ì„ë² ë”© ëª¨ë¸
            self.embeddings = OpenAIEmbeddings()
    
    def setup_prompt_template(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •"""
        self.prompt_template = PromptTemplate(
            input_variables=["context", "question"],
            template="""ë‹¹ì‹ ì€ ê³ ë“±í•™ìƒë“¤ì˜ ëŒ€í•™êµ ì „ê³µ ì„ íƒì„ ë„ì™€ì£¼ëŠ” ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

ì•„ë˜ ëŒ€í•™êµ í•™ê³¼ ì•ˆë‚´ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.

ì°¸ê³  ìë£Œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:
1. ê³ ë“±í•™ìƒì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”
2. êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”  
3. ì§„ë¡œì™€ ê´€ë ¨ëœ ì¡°ì–¸ì„ í¬í•¨í•´ì£¼ì„¸ìš”
4. ì°¸ê³  ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ ì¼ë°˜ì ì¸ ì •ë³´ë¡œ ë³´ì™„í•´ì£¼ì„¸ìš”
5. ì¹œê·¼í•˜ê³  ê²©ë ¤í•˜ëŠ” í†¤ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”
6. ì°¸ê³  ìë£Œì— ìˆëŠ” ë‹µë³€ê³¼ ì—†ëŠ” ë‹µë³€ì„ êµ¬ë¶„í•´ì„œ í‘œí˜„í•´ì£¼ì„¸ìš”
ë‹µë³€:"""
        )
    
    def load_vector_store(self):
        """ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ"""
        try:
            print("ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
            
            # LLM êµ¬ì„±ìš”ì†Œ ì´ˆê¸°í™”
            self.initialize_llm_components()
            
            self.vector_store = FAISS.load_local(
                str(self.vector_db_dir),
                embeddings=self.embeddings,
                allow_dangerous_deserialization=True
            )
            
            # QA ì²´ì¸ ì„¤ì •
            self.setup_qa_chain()
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_path = self.vector_db_dir / "metadata.json"
            if metadata_path.exists():
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                print(f"âœ… ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ!")
                print(f"   ìƒì„±ì¼: {metadata.get('created_at', 'Unknown')}")
                print(f"   ë¬¸ì„œ ìˆ˜: {metadata.get('total_documents', 'Unknown')}")
            else:
                print("âœ… ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def setup_qa_chain(self):
        """QA ì²´ì¸ ì„¤ì •"""
        if self.vector_store is None:
            raise ValueError("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • (ìƒìœ„ 3ê°œ ë¬¸ì„œ ê²€ìƒ‰)
        retriever = self.vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}
        )
        
        # QA ì²´ì¸ ìƒì„±
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={
                "prompt": self.prompt_template
            },
            return_source_documents=True
        )
        
        print("âœ… QA ì²´ì¸ ì„¤ì • ì™„ë£Œ")
    
    def query(self, question: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
        if self.qa_chain is None:
            raise ValueError("QA ì²´ì¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•˜ì„¸ìš”.")
        
        try:
            print(f"ğŸ” ì§ˆë¬¸ ì²˜ë¦¬: {question}")
            
            # QA ì²´ì¸ìœ¼ë¡œ ì§ˆë¬¸ ì²˜ë¦¬
            result = self.qa_chain.invoke({"query": question})
            
            # ë‹µë³€ ì¶”ì¶œ
            answer = result.get("result", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            source_docs = result.get("source_documents", [])
            
            # ì†ŒìŠ¤ ì •ë³´ ìƒì„±
            sources = []
            for doc in source_docs:
                metadata = doc.metadata
                source_info = f"{metadata.get('source', 'Unknown')} (í˜ì´ì§€ {metadata.get('page', '?')})"
                if source_info not in sources:
                    sources.append(source_info)
            
            print(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ (ì°¸ê³  ìë£Œ: {len(sources)}ê°œ)")
            
            return {
                "question": question,
                "answer": answer,
                "sources": sources,
                "source_documents": source_docs
            }
            
        except Exception as e:
            error_msg = f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            print(f"âŒ {error_msg}")
            
            return {
                "question": question,
                "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. {error_msg}",
                "sources": [],
                "source_documents": []
            }