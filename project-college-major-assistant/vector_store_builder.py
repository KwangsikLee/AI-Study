#!/usr/bin/env python3
"""
VectorStoreBuilder - PDF ì²˜ë¦¬ ë° ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì „ë‹´ í´ë˜ìŠ¤

Author: kwangsiklee  
Version: 0.1.0
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
import json
from datetime import datetime
import gc

# LangChain imports
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

# a_my_rag_module ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))
from a_my_rag_module import PDFImageExtractor, KoreanOCR


class VectorStoreBuilder:
    """PDF ì²˜ë¦¬ ë° ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì „ë‹´ í´ë˜ìŠ¤"""
    
    def __init__(self, pdf_dir: str, temp_images_dir: str, vector_db_dir: str):
        self.pdf_dir = Path(pdf_dir)
        self.temp_images_dir = Path(temp_images_dir)
        self.vector_db_dir = Path(vector_db_dir)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.temp_images_dir.mkdir(exist_ok=True)
        self.vector_db_dir.mkdir(exist_ok=True)
        
        # PDF ì²˜ë¦¬ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
        self.pdf_extractor = PDFImageExtractor(dpi=100, max_size=2048)
        self.ocr = KoreanOCR()
        
        # ì„ë² ë”© ëª¨ë¸ (í•„ìš”ì‹œ ì§€ì—° ë¡œë”©)
        self.embeddings = None

        # í…ìŠ¤íŠ¸ ë¶„í• ê¸°
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=100,
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
        )
        
        # ë²¡í„° ìŠ¤í† ì–´ (êµ¬ì¶• ì‹œì—ë§Œ ì‚¬ìš©)
        self.vector_store = None
        
        print(f"VectorStoreBuilder ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"PDF ë””ë ‰í† ë¦¬: {self.pdf_dir}")
        print(f"ë²¡í„° DB ë””ë ‰í† ë¦¬: {self.vector_db_dir}")
        
    def force_memory_cleanup(self):
        """ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        gc.collect()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (PyTorch)
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except ImportError:
            pass
    
    def initialize_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ì§€ì—° ì´ˆê¸°í™”"""
        if self.embeddings is None:
            print("ğŸ”¤ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            # ì„ë² ë”© ëª¨ë¸
            self.embeddings = OpenAIEmbeddings()
    
    def vector_store_exists(self) -> bool:
        """ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        faiss_index_path = self.vector_db_dir / "index.faiss"
        faiss_pkl_path = self.vector_db_dir / "index.pkl"
        return faiss_index_path.exists() and faiss_pkl_path.exists()
    
    def build_vector_store(self, progress_callback: Optional[Callable] = None):
        """PDF íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•"""
        try:
            if progress_callback:
                progress_callback("PDF íŒŒì¼ ëª©ë¡ í™•ì¸ ì¤‘...")
            
            # PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            pdf_files = list(self.pdf_dir.glob("*.pdf"))
            
            if not pdf_files:
                raise ValueError(f"PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.pdf_dir}")
            
            print(f"ë°œê²¬ëœ PDF íŒŒì¼: {len(pdf_files)}ê°œ")
            
            all_documents = []
            
            # ìƒ˜í”Œë¡œ ì²˜ìŒ 1ê°œ íŒŒì¼ë§Œ ì²˜ë¦¬ (MVP)
            # sample_files = pdf_files[:1]
            sample_files = [self.pdf_dir / "01-ê²½ì˜ëŒ€í•™.pdf"]
            for i, pdf_file in enumerate(sample_files):
                try:
                    if progress_callback:
                        progress_callback(f"ì²˜ë¦¬ ì¤‘: {pdf_file.name} ({i+1}/{len(sample_files)})")
                    
                    print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {pdf_file.name}")
                    
                    # 1. PDFì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ
                    image_paths = self.pdf_extractor.extract_images_from_pdf(
                        str(pdf_file),
                        str(self.temp_images_dir),
                        split_large_pages=True
                    )
                    
                    print(f"  ğŸ“· ì¶”ì¶œëœ ì´ë¯¸ì§€: {len(image_paths)}ê°œ")
                    
                    check_memory = True # = self.check_memory_threshold()
                    if check_memory:
                        print("âš ï¸ ë©”ëª¨ë¦¬ ì„ê³„ê°’ ì´ˆê³¼ - ê°•ì œ ì •ë¦¬ ë° ëŒ€ê¸°")
                        self.force_memory_cleanup()
                        import time
                        time.sleep(2)  # ë©”ëª¨ë¦¬ ì•ˆì •í™” ëŒ€ê¸°
                    
                    # 2. OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    pdf_texts = []
                    for img_path in image_paths:
                        try:
                            text = self.ocr.extract_text(img_path)
                            if text.strip():  # ë¹ˆ í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ
                                pdf_texts.append(text.strip())
                        except Exception as e:
                            print(f"    âš ï¸ OCR ì‹¤íŒ¨: {img_path} - {e}")
                            continue
                    
                    print(f"  ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¸”ë¡: {len(pdf_texts)}ê°œ")
                    
                    # 3. í…ìŠ¤íŠ¸ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
                    for j, text in enumerate(pdf_texts):
                        if len(text) > 50:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                            # í…ìŠ¤íŠ¸ ë¶„í• 
                            chunks = self.text_splitter.split_text(text)
                            
                            for k, chunk in enumerate(chunks):
                                doc = Document(
                                    page_content=chunk,
                                    metadata={
                                        "source": pdf_file.name,
                                        "page": j + 1,
                                        "chunk": k + 1,
                                        "processed_at": datetime.now().isoformat()
                                    }
                                )
                                all_documents.append(doc)
                    
                    print(f"  âœ… ì™„ë£Œ: {pdf_file.name}")
                    
                except Exception as e:
                    print(f"  âŒ ì˜¤ë¥˜: {pdf_file.name} - {e}")
                    continue
            
            # memory release ocr model
            self.ocr.cleanup_ocr_model()

            if not all_documents:
                raise ValueError("ì²˜ë¦¬ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            print(f"\nğŸ“Š ì´ ë¬¸ì„œ ìˆ˜: {len(all_documents)}ê°œ")
            
            if progress_callback:
                progress_callback(f"ë²¡í„° ì„ë² ë”© ìƒì„± ì¤‘... ({len(all_documents)}ê°œ ë¬¸ì„œ)")
            
            # 4. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
            self.initialize_embedding_model()

            # ë¬¸ì„œë¥¼ ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)
            batch_size = 50  # í•œ ë²ˆì— 50ê°œ ë¬¸ì„œì”©
            
            if len(all_documents) <= batch_size:
                # ë¬¸ì„œê°€ ì ìœ¼ë©´ í•œ ë²ˆì— ì²˜ë¦¬
                self.vector_store = FAISS.from_documents(
                    documents=all_documents,
                    embedding=self.embeddings
                )
            else:
                # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
                print(f"   ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬: {batch_size}ê°œì”©")
                
                # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
                first_batch = all_documents[:batch_size]
                self.vector_store = FAISS.from_documents(
                    documents=first_batch,
                    embedding=self.embeddings
                )
                
                # ë‚˜ë¨¸ì§€ ë°°ì¹˜ë“¤ì„ ì¶”ê°€
                for i in range(batch_size, len(all_documents), batch_size):
                    batch = all_documents[i:i + batch_size]
                    batch_vector_store = FAISS.from_documents(
                        documents=batch,
                        embedding=self.embeddings
                    )
                    
                    # ë²¡í„° ìŠ¤í† ì–´ ë³‘í•©
                    self.vector_store.merge_from(batch_vector_store)
                    
                    # ë°°ì¹˜ ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
                    batch_vector_store = None
                    gc.collect()
                    
                    print(f"   âœ… ë°°ì¹˜ {i//batch_size + 1} ì™„ë£Œ")

            # 5. ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
            self.vector_store.save_local(str(self.vector_db_dir))
            
            # 6. ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                "created_at": datetime.now().isoformat(),
                "total_documents": len(all_documents),
                "processed_pdfs": [f.name for f in sample_files],
                "vector_db_path": str(self.vector_db_dir)
            }
            
            metadata_path = self.vector_db_dir / "metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            if progress_callback:
                progress_callback(f"ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ! (ë¬¸ì„œ {len(all_documents)}ê°œ)")
            
            print(f"âœ… ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ!")
            print(f"   ì €ì¥ ìœ„ì¹˜: {self.vector_db_dir}")
            print(f"   ì´ ë¬¸ì„œ: {len(all_documents)}ê°œ")
            
        except Exception as e:
            error_msg = f"ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì‹¤íŒ¨: {e}"
            print(f"âŒ {error_msg}")
            if progress_callback:
                progress_callback(error_msg)
            raise
    
    def initialize_vector_db(self, force_rebuild: bool = False, progress_callback: Optional[Callable] = None):
        """ë²¡í„° DB ì´ˆê¸°í™” - ë…ë¦½ ì‹¤í–‰ ê°€ëŠ¥í•œ ì´ˆê¸°í™” í•¨ìˆ˜"""
        try:
            if progress_callback:
                progress_callback("ë²¡í„° DB ì´ˆê¸°í™” ì‹œì‘...")
            
            print("ğŸ”„ ë²¡í„° DB ì´ˆê¸°í™” ì‹œì‘...")
            
            # ê¸°ì¡´ ë²¡í„° DB í™•ì¸
            if self.vector_store_exists() and not force_rebuild:
                if progress_callback:
                    progress_callback("ê¸°ì¡´ ë²¡í„° DB ë°œê²¬ - ê²€ì¦ ì¤‘...")
                
                print("ğŸ“ ê¸°ì¡´ ë²¡í„° DB ë°œê²¬ - ê²€ì¦ ì‹œë„...")
                try:
                    self.initialize_embedding_model()
                    # ë²¡í„° ìŠ¤í† ì–´ ì¡´ì¬ í™•ì¸ë§Œ ìˆ˜í–‰
                    _ = FAISS.load_local(
                        str(self.vector_db_dir),
                        embeddings=self.embeddings,
                        allow_dangerous_deserialization=True
                    )
                    
                    if progress_callback:
                        progress_callback("âœ… ê¸°ì¡´ ë²¡í„° DB ê²€ì¦ ì™„ë£Œ!")
                    
                    print("âœ… ê¸°ì¡´ ë²¡í„° DB ê²€ì¦ ì™„ë£Œ!")
                    return True, "ê¸°ì¡´ ë²¡í„° DB ê²€ì¦ ì™„ë£Œ."
                
                except Exception as e:
                    if progress_callback:
                        progress_callback(f"ê¸°ì¡´ DB ê²€ì¦ ì‹¤íŒ¨ - ìƒˆë¡œ êµ¬ì¶•: {e}")
                    
                    print(f"âš ï¸ ê¸°ì¡´ DB ê²€ì¦ ì‹¤íŒ¨ - ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤: {e}")
                    force_rebuild = True
            
            # ìƒˆ ë²¡í„° DB êµ¬ì¶• ë˜ëŠ” ê°•ì œ ì¬êµ¬ì¶•
            if not self.vector_store_exists() or force_rebuild:
                if force_rebuild:
                    if progress_callback:
                        progress_callback("ê¸°ì¡´ ë²¡í„° DB ì‚­ì œ í›„ ìƒˆë¡œ êµ¬ì¶•...")
                    
                    print("ğŸ—‘ï¸ ê¸°ì¡´ ë²¡í„° DB ì‚­ì œ í›„ ìƒˆë¡œ êµ¬ì¶•...")
                    # ê¸°ì¡´ ë²¡í„° DB íŒŒì¼ ì‚­ì œ
                    import shutil
                    if self.vector_db_dir.exists():
                        shutil.rmtree(self.vector_db_dir)
                        self.vector_db_dir.mkdir(exist_ok=True)
                
                if progress_callback:
                    progress_callback("ìƒˆ ë²¡í„° DB êµ¬ì¶• ì‹œì‘...")
                
                print("ğŸ—ï¸ ìƒˆ ë²¡í„° DB êµ¬ì¶• ì‹œì‘...")
                self.build_vector_store(progress_callback)
                
                if progress_callback:
                    progress_callback("âœ… ìƒˆ ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ!")
                
                print("âœ… ìƒˆ ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ!")
                return True, "ìƒˆ ë²¡í„° DBë¥¼ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤."
            
        except Exception as e:
            error_msg = f"ë²¡í„° DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}"
            print(f"âŒ {error_msg}")
            
            if progress_callback:
                progress_callback(f"âŒ {error_msg}")
            
            return False, error_msg
    
    def get_vector_store_info(self) -> Dict[str, Any]:
        """ë²¡í„° ìŠ¤í† ì–´ ì •ë³´ ë°˜í™˜"""
        info = {
            "exists": self.vector_store_exists(),
            "initialized": self.vector_store is not None,
            "vector_db_path": str(self.vector_db_dir)
        }
        
        # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶”ê°€
        metadata_path = self.vector_db_dir / "metadata.json"
        if metadata_path.exists():
            try:
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                info.update(metadata)
            except Exception as e:
                info["metadata_error"] = str(e)
        
        return info