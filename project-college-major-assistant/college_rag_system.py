#!/usr/bin/env python3
"""
ëŒ€í•™ í•™ê³¼ ì„ íƒ ë„ìš°ë¯¸ RAG ì‹œìŠ¤í…œ êµ¬í˜„
PDF ì´ë¯¸ì§€ ì¶”ì¶œ â†’ OCR â†’ ë²¡í„° ì„ë² ë”© â†’ ê²€ìƒ‰ â†’ LLM ë‹µë³€ ìƒì„±

Author: kwangsiklee  
Version: 0.1.0
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
import pickle
import json
from datetime import datetime

# LangChain imports
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter

# a_my_rag_module ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))
from a_my_rag_module import PDFImageExtractor, KoreanOCR, MultiEmbeddingManager


class CollegeRAGSystem:
    """ëŒ€í•™êµ í•™ê³¼ ì•ˆë‚´ ìë£Œ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, pdf_dir: str, temp_images_dir: str, vector_db_dir: str):
        self.pdf_dir = Path(pdf_dir)
        self.temp_images_dir = Path(temp_images_dir)
        self.vector_db_dir = Path(vector_db_dir)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.temp_images_dir.mkdir(exist_ok=True)
        self.vector_db_dir.mkdir(exist_ok=True)
        
        # êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
        self.pdf_extractor = PDFImageExtractor(dpi=150, max_size=2048)
        self.ocr = KoreanOCR()
        
        # OpenAI ì„¤ì •
        self.llm = ChatOpenAI(
            model= "gpt-4o-mini",  #"gpt-3.5-turbo",
            temperature=0.2,
            max_tokens=1000
        )
        
        # ì„ë² ë”© ëª¨ë¸
        self.embeddings = OpenAIEmbeddings()
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸°
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=100,
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
        )
        
        # ë²¡í„° ìŠ¤í† ì–´
        self.vector_store = None
        self.qa_chain = None
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.setup_prompt_template()
        
        print(f"CollegeRAGSystem ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"PDF ë””ë ‰í† ë¦¬: {self.pdf_dir}")
        print(f"ë²¡í„° DB ë””ë ‰í† ë¦¬: {self.vector_db_dir}")
    
    def setup_prompt_template(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •"""
        self.prompt_template = PromptTemplate(
            input_variables=["context", "question"],
            template="""ë‹¹ì‹ ì€ ê³ ë“±í•™ìƒë“¤ì˜ ëŒ€í•™êµ ì „ê³µ ì„ íƒì„ ë„ì™€ì£¼ëŠ” ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

ì•„ë˜ ëŒ€í•™êµ í•™ê³¼ ì•ˆë‚´ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.

ì°¸ê³  ìë£Œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:
1. ê³ ë“±í•™ìƒì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”
2. êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”  
3. ì§„ë¡œì™€ ê´€ë ¨ëœ ì¡°ì–¸ì„ í¬í•¨í•´ì£¼ì„¸ìš”
4. ì°¸ê³  ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ ì¼ë°˜ì ì¸ ì •ë³´ë¡œ ë³´ì™„í•´ì£¼ì„¸ìš”
5. ì¹œê·¼í•˜ê³  ê²©ë ¤í•˜ëŠ” í†¤ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”

ë‹µë³€:"""
        )
    
    def vector_store_exists(self) -> bool:
        """ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        faiss_index_path = self.vector_db_dir / "index.faiss"
        faiss_pkl_path = self.vector_db_dir / "index.pkl"
        return faiss_index_path.exists() and faiss_pkl_path.exists()
    
    def build_vector_store(self, progress_callback: Optional[Callable] = None):
        """PDF íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•"""
        try:
            if progress_callback:
                progress_callback("PDF íŒŒì¼ ëª©ë¡ í™•ì¸ ì¤‘...")
            
            # PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            pdf_files = list(self.pdf_dir.glob("*.pdf"))
            
            if not pdf_files:
                raise ValueError(f"PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.pdf_dir}")
            
            print(f"ë°œê²¬ëœ PDF íŒŒì¼: {len(pdf_files)}ê°œ")
            
            all_documents = []
            
            # ìƒ˜í”Œë¡œ ì²˜ìŒ 1ê°œ íŒŒì¼ë§Œ ì²˜ë¦¬ (MVP)
            sample_files = pdf_files[:1]
            
            for i, pdf_file in enumerate(sample_files):
                try:
                    if progress_callback:
                        progress_callback(f"ì²˜ë¦¬ ì¤‘: {pdf_file.name} ({i+1}/{len(sample_files)})")
                    
                    print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {pdf_file.name}")
                    
                    # 1. PDFì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ
                    image_paths = self.pdf_extractor.extract_images_from_pdf(
                        str(pdf_file),
                        str(self.temp_images_dir),
                        split_large_pages=True
                    )
                    
                    print(f"  ğŸ“· ì¶”ì¶œëœ ì´ë¯¸ì§€: {len(image_paths)}ê°œ")
                    
                    # 2. OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    pdf_texts = []
                    for img_path in image_paths:
                        try:
                            text = self.ocr.extract_text(img_path)
                            if text.strip():  # ë¹ˆ í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ
                                pdf_texts.append(text.strip())
                        except Exception as e:
                            print(f"    âš ï¸ OCR ì‹¤íŒ¨: {img_path} - {e}")
                            continue
                    
                    print(f"  ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¸”ë¡: {len(pdf_texts)}ê°œ")
                    
                    # 3. í…ìŠ¤íŠ¸ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
                    for j, text in enumerate(pdf_texts):
                        if len(text) > 50:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                            # í…ìŠ¤íŠ¸ ë¶„í• 
                            chunks = self.text_splitter.split_text(text)
                            
                            for k, chunk in enumerate(chunks):
                                doc = Document(
                                    page_content=chunk,
                                    metadata={
                                        "source": pdf_file.name,
                                        "page": j + 1,
                                        "chunk": k + 1,
                                        "processed_at": datetime.now().isoformat()
                                    }
                                )
                                all_documents.append(doc)
                    
                    print(f"  âœ… ì™„ë£Œ: {pdf_file.name}")
                    
                except Exception as e:
                    print(f"  âŒ ì˜¤ë¥˜: {pdf_file.name} - {e}")
                    continue
            
            if not all_documents:
                raise ValueError("ì²˜ë¦¬ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            print(f"\nğŸ“Š ì´ ë¬¸ì„œ ìˆ˜: {len(all_documents)}ê°œ")
            
            if progress_callback:
                progress_callback(f"ë²¡í„° ì„ë² ë”© ìƒì„± ì¤‘... ({len(all_documents)}ê°œ ë¬¸ì„œ)")
            
            # 4. ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
            self.vector_store = FAISS.from_documents(
                documents=all_documents,
                embedding=self.embeddings
            )
            
            # 5. ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
            self.vector_store.save_local(str(self.vector_db_dir))
            
            # 6. ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                "created_at": datetime.now().isoformat(),
                "total_documents": len(all_documents),
                "processed_pdfs": [f.name for f in sample_files],
                "vector_db_path": str(self.vector_db_dir)
            }
            
            metadata_path = self.vector_db_dir / "metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            # 7. QA ì²´ì¸ ì„¤ì •
            self.setup_qa_chain()
            
            if progress_callback:
                progress_callback(f"ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ! (ë¬¸ì„œ {len(all_documents)}ê°œ)")
            
            print(f"âœ… ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ!")
            print(f"   ì €ì¥ ìœ„ì¹˜: {self.vector_db_dir}")
            print(f"   ì´ ë¬¸ì„œ: {len(all_documents)}ê°œ")
            
        except Exception as e:
            error_msg = f"ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì‹¤íŒ¨: {e}"
            print(f"âŒ {error_msg}")
            if progress_callback:
                progress_callback(error_msg)
            raise
    
    def load_vector_store(self):
        """ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ"""
        try:
            print("ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
            
            self.vector_store = FAISS.load_local(
                str(self.vector_db_dir),
                embeddings=self.embeddings,
                allow_dangerous_deserialization=True
            )
            
            # QA ì²´ì¸ ì„¤ì •
            self.setup_qa_chain()
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_path = self.vector_db_dir / "metadata.json"
            if metadata_path.exists():
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                print(f"âœ… ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ!")
                print(f"   ìƒì„±ì¼: {metadata.get('created_at', 'Unknown')}")
                print(f"   ë¬¸ì„œ ìˆ˜: {metadata.get('total_documents', 'Unknown')}")
            else:
                print("âœ… ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def setup_qa_chain(self):
        """QA ì²´ì¸ ì„¤ì •"""
        if self.vector_store is None:
            raise ValueError("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • (ìƒìœ„ 3ê°œ ë¬¸ì„œ ê²€ìƒ‰)
        retriever = self.vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}
        )
        
        # QA ì²´ì¸ ìƒì„±
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={
                "prompt": self.prompt_template
            },
            return_source_documents=True
        )
        
        print("âœ… QA ì²´ì¸ ì„¤ì • ì™„ë£Œ")
    
    def query(self, question: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
        if self.qa_chain is None:
            raise ValueError("QA ì²´ì¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•˜ì„¸ìš”.")
        
        try:
            print(f"ğŸ” ì§ˆë¬¸ ì²˜ë¦¬: {question}")
            
            # QA ì²´ì¸ìœ¼ë¡œ ì§ˆë¬¸ ì²˜ë¦¬
            result = self.qa_chain.invoke({"query": question})
            
            # ë‹µë³€ ì¶”ì¶œ
            answer = result.get("result", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            source_docs = result.get("source_documents", [])
            
            # ì†ŒìŠ¤ ì •ë³´ ìƒì„±
            sources = []
            for doc in source_docs:
                metadata = doc.metadata
                source_info = f"{metadata.get('source', 'Unknown')} (í˜ì´ì§€ {metadata.get('page', '?')})"
                if source_info not in sources:
                    sources.append(source_info)
            
            print(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ (ì°¸ê³  ìë£Œ: {len(sources)}ê°œ)")
            
            return {
                "question": question,
                "answer": answer,
                "sources": sources,
                "source_documents": source_docs
            }
            
        except Exception as e:
            error_msg = f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            print(f"âŒ {error_msg}")
            
            return {
                "question": question,
                "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. {error_msg}",
                "sources": [],
                "source_documents": []
            }
    
    def get_vector_store_info(self) -> Dict[str, Any]:
        """ë²¡í„° ìŠ¤í† ì–´ ì •ë³´ ë°˜í™˜"""
        info = {
            "exists": self.vector_store_exists(),
            "initialized": self.vector_store is not None,
            "vector_db_path": str(self.vector_db_dir)
        }
        
        # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶”ê°€
        metadata_path = self.vector_db_dir / "metadata.json"
        if metadata_path.exists():
            try:
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                info.update(metadata)
            except Exception as e:
                info["metadata_error"] = str(e)
        
        return info


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_rag_system():
    """RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ê²½ë¡œ ì„¤ì •
    base_dir = Path(__file__).parent
    pdf_dir = base_dir / "korea_univ_guides"
    temp_images_dir = base_dir / "temp_images"  
    vector_db_dir = base_dir / "vector_db"
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_system = CollegeRAGSystem(
        pdf_dir=str(pdf_dir),
        temp_images_dir=str(temp_images_dir),
        vector_db_dir=str(vector_db_dir)
    )
    
    # ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ë˜ëŠ” ë¡œë“œ
    if not rag_system.vector_store_exists():
        print("ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•...")
        rag_system.build_vector_store()
    else:
        print("ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ...")
        rag_system.load_vector_store()
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_questions = [
        "ì»´í“¨í„°ê³µí•™ê³¼ëŠ” ì–´ë–¤ ê³µë¶€ë¥¼ í•˜ë‚˜ìš”?",
        "ê²½ì˜í•™ê³¼ì˜ ì£¼ìš” ê³¼ëª©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì˜ëŒ€ ì§„í•™ì„ ìœ„í•´ ì–´ë–¤ ì¤€ë¹„ê°€ í•„ìš”í•œê°€ìš”?"
    ]
    
    print("\nğŸ¤– ì§ˆë¬¸-ë‹µë³€ í…ŒìŠ¤íŠ¸:")
    for question in test_questions:
        print(f"\nâ“ {question}")
        result = rag_system.query(question)
        print(f"ğŸ’¬ {result['answer']}")
        if result['sources']:
            print(f"ğŸ“š ì°¸ê³  ìë£Œ: {', '.join(result['sources'])}")


if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    from dotenv import load_dotenv
    load_dotenv()
    test_rag_system()