{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy4xQSINk8K-"
      },
      "source": [
        "# 환경설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4_lJst0k4Wh",
        "outputId": "6548613d-aa41-4480-94af-2ad82986444e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (2.5.1)\n",
            "Requirement already satisfied: torchvision in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (3.10.3)\n",
            "Requirement already satisfied: seaborn in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (0.13.2)\n",
            "Requirement already satisfied: pandas in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (2.3.1)\n",
            "Requirement already satisfied: numpy in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (1.26.4)\n",
            "Requirement already satisfied: Pillow in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (11.3.0)\n",
            "Requirement already satisfied: scikit-learn in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (1.7.1)\n",
            "Requirement already satisfied: filelock in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from torch) (2025.7.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <EB3FF92A-5EB1-3EE8-AF8B-5923C1265422> /Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/kwangsiklee/miniforge3/envs/llmenv/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 모든 라이브러리가 성공적으로 로드되었습니다!\n",
            "PyTorch 버전: 2.5.1\n",
            "CUDA 사용 가능: False\n"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리 설치 (코랩에서 실행)\n",
        "!pip install torch torchvision matplotlib seaborn pandas numpy Pillow scikit-learn\n",
        "\n",
        "# 기본 라이브러리 import\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import io\n",
        "import requests\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import time\n",
        "import os\n",
        "\n",
        "# 시드 설정 (재현 가능한 결과를 위해)\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\" 모든 라이브러리가 성공적으로 로드되었습니다!\")\n",
        "print(f\"PyTorch 버전: {torch.__version__}\")\n",
        "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzfCN-wklHcm"
      },
      "source": [
        "메모리 효율적인 데이터셋"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGLlrKchlIgt",
        "outputId": "9b72c046-ef6f-4188-a4a5-c1a41826ab36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 메모리 효율성 테스트:\n",
            "\n",
            "1 일반 데이터셋 (모든 데이터 메모리에 로드):\n",
            " 메모리 효율적 데이터셋 생성:\n",
            "   - 데이터 크기: 1,000\n",
            "   - 특성 차원: 100\n",
            "   - 캐시 크기: 1000\n",
            "   - 메모리 사용량: 라벨만 저장 (~0.0MB)\n",
            "   생성 시간: 0.007초\n",
            "\n",
            "2 메모리 효율적 데이터셋 (지연 로딩):\n",
            " 메모리 효율적 데이터셋 생성:\n",
            "   - 데이터 크기: 1,000\n",
            "   - 특성 차원: 100\n",
            "   - 캐시 크기: 100\n",
            "   - 메모리 사용량: 라벨만 저장 (~0.0MB)\n",
            "   생성 시간: 0.000초\n",
            "\n",
            " 캐시 효과 테스트:\n",
            "   첫 번째 접근 (캐시 미스): 0.085초\n",
            "   두 번째 접근 (캐시 히트): 0.000초\n",
            "   속도 향상: 612.5x\n",
            "\n",
            " 캐시 통계:\n",
            "   cache_size: 50\n",
            "   max_cache_size: 100\n",
            "   cache_utilization: 50.0%\n"
          ]
        }
      ],
      "source": [
        "class MemoryEfficientDataset(Dataset):\n",
        "    \"\"\"메모리 효율적인 데이터셋 (지연 로딩)\"\"\"\n",
        "\n",
        "    def __init__(self, data_size=10000, feature_dim=100, cache_size=1000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_size: 전체 데이터 크기\n",
        "            feature_dim: 특성 차원\n",
        "            cache_size: 캐시할 데이터 수\n",
        "        \"\"\"\n",
        "        self.data_size = data_size\n",
        "        self.feature_dim = feature_dim\n",
        "        self.cache_size = cache_size\n",
        "\n",
        "        # 데이터를 실제로 생성하지 않고 메타데이터만 저장\n",
        "        self.labels = torch.randint(0, 10, (data_size,))\n",
        "\n",
        "        # LRU 캐시 구현\n",
        "        self.cache = {}\n",
        "        self.access_order = []\n",
        "\n",
        "        print(f\" 메모리 효율적 데이터셋 생성:\")\n",
        "        print(f\"   - 데이터 크기: {data_size:,}\")\n",
        "        print(f\"   - 특성 차원: {feature_dim}\")\n",
        "        print(f\"   - 캐시 크기: {cache_size}\")\n",
        "        print(f\"   - 메모리 사용량: 라벨만 저장 (~{data_size * 4 / 1024**2:.1f}MB)\")\n",
        "\n",
        "\n",
        "    def _generate_data(self, idx):\n",
        "        \"\"\"데이터 생성 (실제로는 파일에서 로드)\"\"\"\n",
        "        # 시뮬레이션: 실제로는 파일 I/O나 복잡한 계산\n",
        "        torch.manual_seed(idx)  # 일관된 데이터 생성을 위해\n",
        "        data = torch.randn(self.feature_dim)\n",
        "        time.sleep(0.001)  # I/O 지연 시뮬레이션\n",
        "        return data\n",
        "\n",
        "    def _update_cache(self, idx, data):\n",
        "        \"\"\"LRU 캐시 업데이트\"\"\"\n",
        "        # 캐시 크기 초과 시 가장 오래된 데이터 제거\n",
        "        if len(self.cache) >= self.cache_size:\n",
        "            oldest_idx = self.access_order.pop(0)\n",
        "            del self.cache[oldest_idx]\n",
        "\n",
        "        # 새 데이터 캐시에 추가\n",
        "        self.cache[idx] = data\n",
        "        self.access_order.append(idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 캐시 확인\n",
        "        if idx in self.cache:\n",
        "            # 캐시 히트: 접근 순서 업데이트\n",
        "            self.access_order.remove(idx)\n",
        "            self.access_order.append(idx)\n",
        "            data = self.cache[idx]\n",
        "        else:\n",
        "            # 캐시 미스: 데이터 생성 및 캐시 저장\n",
        "            data = self._generate_data(idx)\n",
        "            self._update_cache(idx, data)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        return data, label\n",
        "\n",
        "    def get_cache_stats(self):\n",
        "        \"\"\"캐시 통계 반환\"\"\"\n",
        "        return {\n",
        "            'cache_size': len(self.cache),\n",
        "            'max_cache_size': self.cache_size,\n",
        "            'cache_utilization': len(self.cache) / self.cache_size * 100\n",
        "        }\n",
        "\n",
        "# 메모리 효율성 테스트\n",
        "print(\" 메모리 효율성 테스트:\")\n",
        "\n",
        "# 일반 데이터셋 vs 메모리 효율적 데이터셋 비교\n",
        "print(\"\\n1 일반 데이터셋 (모든 데이터 메모리에 로드):\")\n",
        "start_time = time.time()\n",
        "# normal_dataset = NumberDataset(num_samples=1000, input_dim=100)\n",
        "normal_dataset = MemoryEfficientDataset(data_size=1000, feature_dim=100, cache_size=1000) # Using MemoryEfficientDataset for comparison\n",
        "normal_creation_time = time.time() - start_time\n",
        "print(f\"   생성 시간: {normal_creation_time:.3f}초\")\n",
        "\n",
        "print(\"\\n2 메모리 효율적 데이터셋 (지연 로딩):\")\n",
        "start_time = time.time()\n",
        "efficient_dataset = MemoryEfficientDataset(data_size=1000, feature_dim=100, cache_size=100)\n",
        "efficient_creation_time = time.time() - start_time\n",
        "print(f\"   생성 시간: {efficient_creation_time:.3f}초\")\n",
        "\n",
        "# 캐시 효과 테스트\n",
        "print(f\"\\n 캐시 효과 테스트:\")\n",
        "\n",
        "# 첫 번째 접근 (캐시 미스)\n",
        "start_time = time.time()\n",
        "for i in range(50):\n",
        "    _ = efficient_dataset[i]\n",
        "first_access_time = time.time() - start_time\n",
        "\n",
        "# 두 번째 접근 (캐시 히트)\n",
        "start_time = time.time()\n",
        "for i in range(50):\n",
        "    _ = efficient_dataset[i]\n",
        "second_access_time = time.time() - start_time\n",
        "\n",
        "print(f\"   첫 번째 접근 (캐시 미스): {first_access_time:.3f}초\")\n",
        "print(f\"   두 번째 접근 (캐시 히트): {second_access_time:.3f}초\")\n",
        "print(f\"   속도 향상: {first_access_time / second_access_time:.1f}x\")\n",
        "\n",
        "# 캐시 통계\n",
        "cache_stats = efficient_dataset.get_cache_stats()\n",
        "print(f\"\\n 캐시 통계:\")\n",
        "for key, value in cache_stats.items():\n",
        "    if 'utilization' in key:\n",
        "        print(f\"   {key}: {value:.1f}%\")\n",
        "    else:\n",
        "        print(f\"   {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqVHTrr1lAhu"
      },
      "source": [
        "# 성능 최적화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lqmugIElCvM",
        "outputId": "5e6b2641-e38a-487a-d2ab-feec8e63764f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " DataLoader 성능 벤치마크\n",
            "==================================================\n",
            " 메모리 효율적 데이터셋 생성:\n",
            "   - 데이터 크기: 5,000\n",
            "   - 특성 차원: 100\n",
            "   - 캐시 크기: 500\n",
            "   - 메모리 사용량: 라벨만 저장 (~0.0MB)\n",
            "\n",
            " 테스트 중: 기본설정\n",
            "   ⏱ 시간: 1.099초\n",
            "    처리량: 582.2 샘플/초\n",
            "\n",
            " 테스트 중: 큰배치\n",
            "   ⏱ 시간: 4.544초\n",
            "    처리량: 563.3 샘플/초\n",
            "\n",
            " 테스트 중: 멀티워커\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "               ^^^^^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^\n",
            "^^^^  File \"/Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
            "^^^^^^^^^^^^^^^\n",
            "  File \"/Users/kwangsiklee/miniforge3/envs/llmenv/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "      self = reduction.pickle.load(from_parent) \n",
            "        ^^^^^^^^^^^^^^^^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^\n",
            "^^^^^AttributeError^^: ^Can't get attribute 'MemoryEfficientDataset' on <module '__main__' (built-in)>^^\n",
            "^^^^^^^^^^^^^^^^\n",
            "AttributeError: Can't get attribute 'MemoryEfficientDataset' on <module '__main__' (built-in)>\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "DataLoader worker (pid(s) 80008, 80009) exited unexpectedly",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/llmenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1243\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1242\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1244\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/llmenv/lib/python3.11/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    112\u001b[39m timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/llmenv/lib/python3.11/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/llmenv/lib/python3.11/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/llmenv/lib/python3.11/multiprocessing/connection.py:948\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m    947\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/llmenv/lib/python3.11/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/llmenv/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001b[39m, in \u001b[36m_set_SIGCHLD_handler.<locals>.handler\u001b[39m\u001b[34m(signum, frame)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhandler\u001b[39m(signum, frame):\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[32m     72\u001b[39m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid 80009) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# 성능 벤치마크 실행\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m benchmark_results = \u001b[43mbenchmark_dataloader_settings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mbenchmark_dataloader_settings\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     39\u001b[39m start_time = time.time()\n\u001b[32m     40\u001b[39m batch_count = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 간단한 연산 시뮬레이션\u001b[39;49;00m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/llmenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/llmenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1448\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1447\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1451\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/llmenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1412\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1408\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1409\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1410\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1411\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1412\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1413\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1414\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/llmenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1256\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1255\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1257\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1258\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1260\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 80008, 80009) exited unexpectedly"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def benchmark_dataloader_settings():\n",
        "    \"\"\"DataLoader 설정별 성능 벤치마크\"\"\"\n",
        "\n",
        "    print(\" DataLoader 성능 벤치마크\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 테스트용 큰 데이터셋 생성\n",
        "    # large_dataset = NumberDataset(num_samples=5000, input_dim=100, num_classes=10)\n",
        "    # Use MemoryEfficientDataset instead of NumberDataset\n",
        "    large_dataset = MemoryEfficientDataset(data_size=5000, feature_dim=100, cache_size=500)\n",
        "\n",
        "\n",
        "    # 다양한 설정 테스트\n",
        "    test_configs = [\n",
        "        {'name': '기본설정', 'batch_size': 32, 'num_workers': 0, 'pin_memory': False},\n",
        "        {'name': '큰배치', 'batch_size': 128, 'num_workers': 0, 'pin_memory': False},\n",
        "        {'name': '멀티워커', 'batch_size': 32, 'num_workers': 2, 'pin_memory': False},\n",
        "        {'name': '핀메모리', 'batch_size': 32, 'num_workers': 0, 'pin_memory': True},\n",
        "        {'name': '최적화', 'batch_size': 64, 'num_workers': 2, 'pin_memory': True}\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for config in test_configs:\n",
        "        print(f\"\\n 테스트 중: {config['name']}\")\n",
        "\n",
        "        # DataLoader 생성\n",
        "        dataloader = DataLoader(\n",
        "            large_dataset,\n",
        "            batch_size=config['batch_size'],\n",
        "            shuffle=True,\n",
        "            num_workers=config['num_workers'],\n",
        "            pin_memory=config['pin_memory'] and torch.cuda.is_available()\n",
        "        )\n",
        "\n",
        "        # 시간 측정\n",
        "        start_time = time.time()\n",
        "        batch_count = 0\n",
        "\n",
        "        for batch_data, batch_labels in dataloader:\n",
        "            # 간단한 연산 시뮬레이션\n",
        "            _ = batch_data.mean()\n",
        "            _ = batch_labels.sum()\n",
        "\n",
        "            batch_count += 1\n",
        "            if batch_count >= 20:  # 20개 배치만 테스트\n",
        "                break\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        throughput = (batch_count * config['batch_size']) / elapsed_time\n",
        "\n",
        "        result = {\n",
        "            'name': config['name'],\n",
        "            'time': elapsed_time,\n",
        "            'throughput': throughput,\n",
        "            'config': config\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        print(f\"   ⏱ 시간: {elapsed_time:.3f}초\")\n",
        "        print(f\"    처리량: {throughput:.1f} 샘플/초\")\n",
        "\n",
        "    # 결과 비교\n",
        "    print(f\"\\n 성능 비교 결과:\")\n",
        "    print(\"-\" * 60)\n",
        "    baseline = results[0]['throughput']\n",
        "\n",
        "    for result in results:\n",
        "        speedup = result['throughput'] / baseline\n",
        "        print(f\"{result['name']:10} | {result['time']:6.3f}초 | {result['throughput']:8.1f} 샘플/초 | {speedup:4.2f}x\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# 성능 벤치마크 실행\n",
        "benchmark_results = benchmark_dataloader_settings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88wl7spElKaw"
      },
      "source": [
        "GPU 최적화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8edjtHmlLW3",
        "outputId": "0d8b9e74-7f8b-40c1-fae6-01be217f76fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " GPU 최적화 테스트:\n",
            " 메모리 효율적 데이터셋 생성:\n",
            "   - 데이터 크기: 1,000\n",
            "   - 특성 차원: 50\n",
            "   - 캐시 크기: 1000\n",
            "   - 메모리 사용량: 라벨만 저장 (~0.0MB)\n",
            " GPU 최적화 로더 생성:\n",
            "   - Device: cuda\n",
            "   - Pin memory: True\n",
            "   - Workers: 0\n",
            "\n",
            "⏱️ 성능 비교:\n",
            "   일반 로더: 0.519초\n",
            "   GPU 최적화: 0.337초\n",
            "   성능 향상: 1.54x\n",
            "\n",
            " GPU 메모리 사용량:\n",
            "   할당된 메모리: 0.0MB\n",
            "   캐시된 메모리: 2.0MB\n"
          ]
        }
      ],
      "source": [
        "class GPUOptimizedLoader:\n",
        "    \"\"\"GPU 최적화된 데이터 로더\"\"\"\n",
        "\n",
        "    def __init__(self, dataset, batch_size=32, shuffle=True):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.dataloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=2 if not torch.cuda.is_available() else 0,  # GPU 사용시 워커 줄임\n",
        "            pin_memory=torch.cuda.is_available(),  # GPU 사용시 pin memory 활성화\n",
        "            persistent_workers=True if not torch.cuda.is_available() else False\n",
        "        )\n",
        "\n",
        "        print(f\" GPU 최적화 로더 생성:\")\n",
        "        print(f\"   - Device: {self.device}\")\n",
        "        print(f\"   - Pin memory: {torch.cuda.is_available()}\")\n",
        "        print(f\"   - Workers: {self.dataloader.num_workers}\")\n",
        "\n",
        "    def __iter__(self):\n",
        "        for batch_data, batch_labels in self.dataloader:\n",
        "            # GPU로 비동기 전송\n",
        "            batch_data = batch_data.to(self.device, non_blocking=True)\n",
        "            batch_labels = batch_labels.to(self.device, non_blocking=True)\n",
        "\n",
        "            yield batch_data, batch_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataloader)\n",
        "\n",
        "# GPU 최적화 테스트\n",
        "if torch.cuda.is_available():\n",
        "    print(\" GPU 최적화 테스트:\")\n",
        "\n",
        "    test_dataset = MemoryEfficientDataset(data_size=1000, feature_dim=50)\n",
        "\n",
        "    # 일반 로더\n",
        "    normal_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    # GPU 최적화 로더\n",
        "    gpu_loader = GPUOptimizedLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    # 성능 비교\n",
        "    print(\"\\n⏱ 성능 비교:\")\n",
        "\n",
        "    # 일반 로더 테스트\n",
        "    start_time = time.time()\n",
        "    for i, (data, labels) in enumerate(normal_loader):\n",
        "        data = data.cuda()\n",
        "        labels = labels.cuda()\n",
        "        _ = data.mean()  # 간단한 연산\n",
        "        if i >= 10:\n",
        "            break\n",
        "    normal_time = time.time() - start_time\n",
        "\n",
        "    # GPU 최적화 로더 테스트\n",
        "    start_time = time.time()\n",
        "    for i, (data, labels) in enumerate(gpu_loader):\n",
        "        _ = data.mean()  # 간단한 연산\n",
        "        if i >= 10:\n",
        "            break\n",
        "    optimized_time = time.time() - start_time\n",
        "\n",
        "    print(f\"   일반 로더: {normal_time:.3f}초\")\n",
        "    print(f\"   GPU 최적화: {optimized_time:.3f}초\")\n",
        "    print(f\"   성능 향상: {normal_time / optimized_time:.2f}x\")\n",
        "\n",
        "else:\n",
        "    print(\" GPU를 사용할 수 없어 CPU에서 테스트합니다.\")\n",
        "    print(\"   코랩에서 GPU 런타임을 선택하면 GPU 최적화를 테스트할 수 있습니다.\")\n",
        "\n",
        "# 메모리 사용량 모니터링 (GPU가 있는 경우)\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\n GPU 메모리 사용량:\")\n",
        "    print(f\"   할당된 메모리: {torch.cuda.memory_allocated() / 1024**2:.1f}MB\")\n",
        "    print(f\"   캐시된 메모리: {torch.cuda.memory_reserved() / 1024**2:.1f}MB\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llmenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
