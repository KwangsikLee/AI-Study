{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1fuGy_FrxWs"
   },
   "source": [
    "시간당 내부 평균온도: 4도~40도\n",
    "\n",
    "시간당 내부 평균습도: 0% ~ 100%\n",
    "\n",
    "시간당 내부 평균 co2 농도 : 0ppm ~ 1200 ppm\n",
    "\n",
    "시간당 평균 EC : 0 ~ 8\n",
    "\n",
    "시간당 분무량 : 0 ~ 3000 / 일간 누적 분무량 0 ~ 72,000\n",
    "\n",
    "시간당 백색광량 : 0 ~ 120,000 / 일간 누적 백색광량 0 ~ 2,880,000\n",
    "\n",
    "시간당 적색광량 : 0 ~ 120,000 / 일간 누적 적색광량 0 ~ 2,880,000\n",
    "\n",
    "시간당 청색광량 : 0 ~ 120,000 / 일간 누적 청색광량 0 ~ 2,880,000\n",
    "\n",
    "시간당 총광량 : 0 ~ 120,000 / 일간 누적 총광량 0 ~ 2,880,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ONkSAYeCrsV6",
    "outputId": "68daafb0-8424-46a5-a9f6-728f44733a96"
   },
   "outputs": [],
   "source": [
    "# 나눔고딕 폰트 설치 및 설정\n",
    "!apt-get update -qq\n",
    "!apt-get install fonts-nanum -qq\n",
    "!fc-cache -fv\n",
    "!rm ~/.cache/matplotlib -rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 폰트 설정\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
    "# fontprop = fm.FontProperties(fname=font_path, size=10)\n",
    "# plt.rcParams['font.family'] = 'NanumGothic'\n",
    "# plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 폰트 설정\n",
    "font_path='/System/Library/Fonts/AppleGothic.ttf'\n",
    "fontprop = fm.FontProperties(fname=font_path, size=10)\n",
    "plt.rcParams['font.family'] = 'AppleGothic'  # macOS\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gcI_wcjqsCW4",
    "outputId": "2ce463e5-7ca6-4381-d232-ccddcc75b175"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 데이터 shape: (18816, 17)\n",
      "타겟 데이터 shape: (784, 3)\n",
      "입력 데이터 컬럼: ['DAT', 'obs_time', '내부온도관측치', '내부습도관측치', 'co2관측치', 'ec관측치', '시간당분무량', '일간누적분무량', '시간당백색광량', '일간누적백색광량', '시간당적색광량', '일간누적적색광량', '시간당청색광량', '일간누적청색광량', '시간당총광량', '일간누적총광량', 'CASE']\n",
      "타겟 데이터 컬럼: ['DAT', 'predicted_weight_g', 'CASE']\n",
      "\n",
      "hour 컬럼 추가 후 X shape: (18816, 18)\n",
      "hour 컬럼 unique values: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "\n",
      "필터링된 입력 데이터 shape: (588, 17)\n",
      "병합된 데이터 shape: (18816, 19)\n",
      "병합된 데이터 첫 5행:\n",
      "   DAT obs_time    내부온도관측치    내부습도관측치      co2관측치     ec관측치  시간당분무량  일간누적분무량  \\\n",
      "0    0    00:00  25.300000  81.835000  536.016667  1.407439     0.0      0.0   \n",
      "1    0    01:00  25.680357  81.264286  528.696429  1.409003   126.0    126.0   \n",
      "2    0    02:00  25.273333  81.471666  532.833333  1.406913     0.0    126.0   \n",
      "3    0    03:00  25.355000  81.398334  545.566667  1.406689   126.0    252.0   \n",
      "4    0    04:00  25.391667  81.483333  558.583333  1.411070     0.0    252.0   \n",
      "\n",
      "   시간당백색광량  일간누적백색광량  시간당적색광량  일간누적적색광량  시간당청색광량  일간누적청색광량  시간당총광량  일간누적총광량  \\\n",
      "0      0.0       0.0      0.0       0.0      0.0       0.0     0.0      0.0   \n",
      "1      0.0       0.0      0.0       0.0      0.0       0.0     0.0      0.0   \n",
      "2      0.0       0.0      0.0       0.0      0.0       0.0     0.0      0.0   \n",
      "3      0.0       0.0      0.0       0.0      0.0       0.0     0.0      0.0   \n",
      "4      0.0       0.0      0.0       0.0      0.0       0.0     0.0      0.0   \n",
      "\n",
      "      CASE  hour  predicted_weight_g  \n",
      "0  CASE_01     0                 NaN  \n",
      "1  CASE_01     1                 NaN  \n",
      "2  CASE_01     2                 NaN  \n",
      "3  CASE_01     3                 NaN  \n",
      "4  CASE_01     4                 NaN  \n"
     ]
    }
   ],
   "source": [
    "# 필수 라이브러리 설치\n",
    "# !pip install xgboost lightgbm scikit-learn tensorflow matplotlib seaborn --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# 1. 모든 CSV 파일 읽어오기 및 합치기\n",
    "def load_all_csv_files(folder_path):\n",
    "    \"\"\"폴더 내 모든 CSV 파일을 읽어서 하나의 DataFrame으로 합침\"\"\"\n",
    "    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    dataframes = []\n",
    "    \n",
    "    for file in sorted(csv_files):\n",
    "        df = pd.read_csv(file)\n",
    "        # 파일명에서 CASE 번호 추출 후 컬럼으로 추가\n",
    "        case_num = os.path.basename(file).replace('.csv', '')\n",
    "        df['CASE'] = case_num\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# train_input 폴더의 모든 CSV 파일 읽기\n",
    "train_input_path = './상추_생육_예측/train_input'\n",
    "X = load_all_csv_files(train_input_path)\n",
    "\n",
    "# train_target 폴더의 모든 CSV 파일 읽기  \n",
    "train_target_path = './상추_생육_예측/train_target'\n",
    "y_df = load_all_csv_files(train_target_path)\n",
    "\n",
    "print(\"입력 데이터 shape:\", X.shape)\n",
    "print(\"타겟 데이터 shape:\", y_df.shape)\n",
    "print(\"입력 데이터 컬럼:\", X.columns.tolist())\n",
    "print(\"타겟 데이터 컬럼:\", y_df.columns.tolist())\n",
    "\n",
    "if 'obs_time' in X.columns:\n",
    "    # obs_time에서 시간(hh) 추출하여 숫자 컬럼으로 추가\n",
    "    X['time'] = X['obs_time'].str.split(':').str[0].astype(int)\n",
    "\n",
    "# 타겟 컬럼이 여러 개라면, 'predicted_weight_g'만 사용\n",
    "target_col = 'predicted_weight_g'\n",
    "\n",
    "\n",
    "# CASE와 DAT를 기준으로 필터링된 X와 y_df 병합\n",
    "df = pd.merge(X, y_df[['CASE', 'DAT', target_col]], on=['CASE', 'DAT'], how='left')\n",
    "\n",
    "print(\"병합된 데이터 shape:\", df.shape)\n",
    "print(\"병합된 데이터 첫 5행:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JV6oiMoRr91h",
    "outputId": "598628a8-a305-42cf-bae1-242086290cb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 결측값 개수: 672\n",
      "결측값이 있는 컬럼:\n",
      "  - predicted_weight_g: 672개 (3.6%)\n"
     ]
    }
   ],
   "source": [
    "def check_nan():\n",
    "    # 결측값 확인\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_columns = missing_values[missing_values > 0]\n",
    "\n",
    "    print(f\"전체 결측값 개수: {missing_values.sum()}\")\n",
    "    if len(missing_columns) > 0:\n",
    "        print(\"결측값이 있는 컬럼:\")\n",
    "        for col, count in missing_columns.items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"  - {col}: {count}개 ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(\"결측값이 있는 컬럼: 없음\")\n",
    "\n",
    "check_nan()\n",
    "\n",
    "# 2. 결측치/이상치 처리 (간단 예시)\n",
    "# df = df.dropna()  # 결측치 행 제거 (상황에 따라 평균/중앙값 대체 가능)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. 특성과 타겟 분리\n",
    "y = df[target_col]\n",
    "X = df.drop([target_col], axis=1)\n",
    "\n",
    "# 문자형 피처 라벨 인코딩\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for col in X.select_dtypes(include='object').columns:\n",
    "    X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "# 4. 학습/테스트 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. 공통 평가 및 시각화 함수 (회귀)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def print_regression_metrics(y_true, y_pred, model_name=\"Model\"):\n",
    "    print(f\"\\n[{model_name} 평가]\")\n",
    "    print(\"MAE :\", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"RMSE:\", np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    print(\"R2  :\", r2_score(y_true, y_pred))\n",
    "\n",
    "def plot_regression_results(y_true, y_pred, model_name=\"Model\"):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.4)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "    plt.xlabel(\"실제값\")\n",
    "    plt.ylabel(\"예측값\")\n",
    "    plt.title(f\"실제값 vs 예측값: {model_name}\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Residual Plot\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.histplot(residuals, bins=30, kde=True)\n",
    "    plt.title(f\"Residuals Distribution: {model_name}\")\n",
    "    plt.xlabel(\"Residual\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance(model, feature_names, model_name=\"Model\"):\n",
    "    try:\n",
    "        importances = model.feature_importances_\n",
    "        idx = np.argsort(importances)[::-1]\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.bar(range(len(importances)), importances[idx])\n",
    "        plt.xticks(range(len(importances)), np.array(feature_names)[idx], rotation=45)\n",
    "        plt.title(f'Feature Importance: {model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except AttributeError:\n",
    "        print(f\"{model_name}은 feature_importances_ 속성이 없습니다.\")\n",
    "\n",
    "# 6. 모델별 예측 및 평가\n",
    "\n",
    "# (1) RandomForest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=120, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "print_regression_metrics(y_test, rf_pred, \"RandomForest\")\n",
    "plot_regression_results(y_test, rf_pred, \"RandomForest\")\n",
    "plot_feature_importance(rf, X.columns, \"RandomForest\")\n",
    "\n",
    "# (2) XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor(n_estimators=120, random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_pred = xgb.predict(X_test)\n",
    "print_regression_metrics(y_test, xgb_pred, \"XGBoost\")\n",
    "plot_regression_results(y_test, xgb_pred, \"XGBoost\")\n",
    "plot_feature_importance(xgb, X.columns, \"XGBoost\")\n",
    "\n",
    "# (3) LightGBM\n",
    "from lightgbm import LGBMRegressor\n",
    "lgbm = LGBMRegressor(n_estimators=120, random_state=42)\n",
    "lgbm.fit(X_train, y_train)\n",
    "lgbm_pred = lgbm.predict(X_test)\n",
    "print_regression_metrics(y_test, lgbm_pred, \"LightGBM\")\n",
    "plot_regression_results(y_test, lgbm_pred, \"LightGBM\")\n",
    "plot_feature_importance(lgbm, X.columns, \"LightGBM\")\n",
    "\n",
    "# (4) Stacking 앙상블\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "stack = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('rf', rf),\n",
    "        ('xgb', xgb),\n",
    "        ('lgbm', lgbm)\n",
    "    ],\n",
    "    final_estimator=Ridge()\n",
    ")\n",
    "stack.fit(X_train, y_train)\n",
    "stack_pred = stack.predict(X_test)\n",
    "print_regression_metrics(y_test, stack_pred, \"Stacking 앙상블\")\n",
    "plot_regression_results(y_test, stack_pred, \"Stacking 앙상블\")\n",
    "\n",
    "# (5) 딥러닝(MLP)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mae', metrics=['mae'])\n",
    "model.fit(X_train, y_train, validation_split=0.1, epochs=40, batch_size=32, verbose=1)\n",
    "mlp_pred = model.predict(X_test).flatten()\n",
    "print_regression_metrics(y_test, mlp_pred, \"MLP 딥러닝\")\n",
    "plot_regression_results(y_test, mlp_pred, \"MLP 딥러닝\")\n",
    "\n",
    "# (6) 성능 비교표\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['RandomForest', 'XGBoost', 'LightGBM', 'Stacking', 'MLP'],\n",
    "    'MAE': [\n",
    "        mean_absolute_error(y_test, rf_pred),\n",
    "        mean_absolute_error(y_test, xgb_pred),\n",
    "        mean_absolute_error(y_test, lgbm_pred),\n",
    "        mean_absolute_error(y_test, stack_pred),\n",
    "        mean_absolute_error(y_test, mlp_pred)\n",
    "    ],\n",
    "    'RMSE': [\n",
    "        np.sqrt(mean_squared_error(y_test, rf_pred)),\n",
    "        np.sqrt(mean_squared_error(y_test, xgb_pred)),\n",
    "        np.sqrt(mean_squared_error(y_test, lgbm_pred)),\n",
    "        np.sqrt(mean_squared_error(y_test, stack_pred)),\n",
    "        np.sqrt(mean_squared_error(y_test, mlp_pred))\n",
    "    ],\n",
    "    'R2': [\n",
    "        r2_score(y_test, rf_pred),\n",
    "        r2_score(y_test, xgb_pred),\n",
    "        r2_score(y_test, lgbm_pred),\n",
    "        r2_score(y_test, stack_pred),\n",
    "        r2_score(y_test, mlp_pred)\n",
    "    ]\n",
    "})\n",
    "print('\\n\\n[모델별 회귀 성능 비교]')\n",
    "display(results)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
