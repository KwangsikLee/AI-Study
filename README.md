# AI-Study ğŸ“š

AI/ML í•™ìŠµì„ ìœ„í•œ ê°œì¸ ìŠ¤í„°ë”” ì €ì¥ì†Œì…ë‹ˆë‹¤. ê¸°ì´ˆ Pythonë¶€í„° ë°ì´í„° ë¶„ì„, ìµœì‹  ë”¥ëŸ¬ë‹, ìì—°ì–´ì²˜ë¦¬ê¹Œì§€ ì²´ê³„ì ìœ¼ë¡œ í•™ìŠµí•œ ë‚´ìš©ì„ ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.

## ğŸ“‹ í”„ë¡œì íŠ¸ êµ¬ì¡°

### ğŸ ê¸°ì´ˆ í•™ìŠµ ëª¨ë“ˆ
- **lectBasic/**: Python ê¸°ì´ˆ, ë°ì´í„°ë² ì´ìŠ¤, Streamlit ê¸°ë³¸ê¸°
- **lectCrawling/**: ì›¹ í¬ë¡¤ë§ (BeautifulSoup, Selenium)
- **lectAPI/**: API ì—°ë™ ì‹¤ìŠµ (World Bank, Hacker News ë“±)

### ğŸ“Š ë°ì´í„° ë¶„ì„ & ML
- **lectAnalysis/**: NumPy, Pandas ë°ì´í„° ë¶„ì„ ê¸°ì´ˆ
- **lectMLDL/**: ì „í†µì  ë¨¸ì‹ ëŸ¬ë‹ (KNN, ê²°ì •íŠ¸ë¦¬, ì„ í˜•íšŒê·€)
- **lectUnsupervised/**: ë¹„ì§€ë„í•™ìŠµ (í´ëŸ¬ìŠ¤í„°ë§, PCA, t-SNE)

### ğŸ§  ë”¥ëŸ¬ë‹ & AI
- **lectDL_NN/**: PyTorch ê¸°ë°˜ ë”¥ëŸ¬ë‹ (MLP, CNN, RNN/LSTM, ì•™ìƒë¸”)
- **lectPytorch/**: PyTorch ì‹¬í™” í•™ìŠµ
- **lectVision/**: ì»´í“¨í„° ë¹„ì „ (YOLO ê°ì²´íƒì§€)

### ğŸ¤– ìì—°ì–´ì²˜ë¦¬ & ì–¸ì–´ëª¨ë¸
- **lectLanguageModel1/**: NLP ê¸°ì´ˆ (í† í¬ë‚˜ì´ì§•, TF-IDF, ì„ë² ë”©)
- **lectLanguageModel2/**: ê³ ê¸‰ NLP (BERT, Attention, ê°ì •ë¶„ì„)
- **lectLanguageModel3/**: ë‹¤ì–‘í•œ ì–¸ì–´ëª¨ë¸ (GPT-2, íŒŒì¸íŠœë‹, ì±—ë´‡)
- **lectOpenAI/**: OpenAIì˜ API 

### ğŸ” LLM
- **RAG_Study/**: RAG(ê²€ìƒ‰ì¦ê°•ìƒì„±) ì—°êµ¬
- **Fine Tuning/**: Fine Tuning ì—°êµ¬



## ğŸ“ í•™ìŠµ ì¼ì§€
ìµœê·¼ í•™ìŠµìˆœìœ¼ë¡œ ì •ë¦¬

### Fine-Tuning 

- [Fine_Tuning_Demo_Unsloth](fine_tuning/Fine_Tuning_Demo_Unsloth/Fine_Tuning.ipynb) - Fine-Tuning LLMs with Unsloth   
    LoRA adapters ë°©ì‹ì˜ fine tuing : unsloth ë¥¼ ì´ìš©í•˜ì—¬ ë¹ ë¥¸ Fine-Turing  
    gguf ëª¨ë¸ ì €ì¥ : base ëª¨ë¸ê³¼ í•©ì³ì§„ ëª¨ë¸ë¡œ ì €ì¥

- [LLM_Fine Tuning Workflow](fine_tuning/qwen_lora_finetuning/LLM_FineTuningWorkflow.ipynb)  - PEFT LoRA  
    PEFT(Parameter-Efficient Fine-Tuning):  ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ íŒŒì¸íŠœë‹í•  ë•Œ, ëª¨ë¸ ì „ì²´ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•˜ì§€ ì•Šê³  ì¼ë¶€ë§Œ í•™ìŠµí•´ì„œ íš¨ìœ¨ì„ ë†’ì´ëŠ” ê¸°ë²•.  

    Qwen/Qwen2.5-3B-Instruct ëª¨ë¸ì„ LoRA ê¸°ë°˜ fine-tuning  
    fine_tuned model : Adapter layer ì €ì¥. 


### OpenAI ChatGPT í™œìš© 


- [OpenAI Function](lectOpenAI/06_í•¨ìˆ˜.ipynb) - Tool Calling(Function Calling)    
    OpenAIì˜ Tool Calling : ì™¸ë¶€ í•¨ìˆ˜ë‚˜ API, DB ë“±ì„ ìë™ìœ¼ë¡œ í˜¸ì¶œí•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ëŠ” ê¸°ëŠ¥   
    ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì‘ë‹µ â†’ ì‹¤í–‰ ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ë¡œ ë°”ê¿”ì£¼ëŠ” ê¸°ëŠ¥    
    í™œìš© ì˜ˆ:  
    - ì‹¤ì‹œê°„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°  
    - ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ
    - ê³„ì‚°/ì‹œë®¬ë ˆì´ì…˜ ì²˜ë¦¬
    - ë¬¸ì„œ/íŒŒì¼ ì²˜ë¦¬
    - ì™¸ë¶€ ì„œë¹„ìŠ¤ ì œì–´
    - Multi step workflow


- [ë‹¤êµ­ì–´ ë²ˆì—­ê¸°](lectOpenAI/í”„ë¡¬í”„íŒ…_cot.ipynb) - í”„ë¡¬í”„íŒ… ê¸°ë²•    
    Few-shot : ë‹¨ ëª‡ ê°œì˜ ì˜ˆì‹œ(examples)ë¥¼ í”„ë¡¬í”„íŠ¸(ì§€ì‹œë¬¸)ì— í¬í•¨í•˜ì—¬ ì œê³µí•˜ëŠ” ë°©ë²•(In-context Learning)

    | ê°œë…              | ì„¤ëª…                                                                 | ì˜ˆì‹œ (ê°ì • ë¶„ì„) |
    |-------------------|----------------------------------------------------------------------|------------------|
    | **Zero-shot (ì œë¡œìƒ·)** | ì˜ˆì‹œ ì—†ì´, ì˜¤ì§ ì§€ì‹œë§Œìœ¼ë¡œ ì‘ì—…ì„ ìš”ì²­. ëª¨ë¸ì˜ ê¸°ì¡´ ì§€ì‹ì— ì „ì ìœ¼ë¡œ ì˜ì¡´. | "ì´ ë¬¸ì¥ì€ ê¸ì •ì´ì•¼ ë¶€ì •ì´ì•¼?" |
    | **One-shot (ì›ìƒ·)**    | ë‹¨ í•˜ë‚˜ì˜ ì˜ˆì‹œë¥¼ ì œê³µ. ì›í•˜ëŠ” ì¶œë ¥ í˜•ì‹ì„ ëª…í™•íˆ í•  ë•Œ ìœ ìš©.            | ì…ë ¥: "ë°°ì†¡ ë¹¨ë¼ìš”." -> ê¸ì •<br>ì…ë ¥: "ë³„ë¡œë„¤ìš”." -> ? |
    | **Few-shot (í“¨ìƒ·)**    | ëª‡ ê°œ(2~5ê°œ)ì˜ ì˜ˆì‹œë¥¼ ì œê³µ. ë” ë³µì¡í•œ íŒ¨í„´ì´ë‚˜ ë‰˜ì•™ìŠ¤ë¥¼ í•™ìŠµì‹œí‚¤ê¸°ì— íš¨ê³¼ì . | (ìœ„ì˜ êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ ë™ì¼) |
    | **Fine-tuning (íŒŒì¸íŠœë‹)** | ìˆ˜ë°±~ìˆ˜ì²œ ê°œ ì´ìƒì˜ ë§ì€ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜(weights)ë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸. ëª¨ë¸ì„ íŠ¹ì • ì‘ì—…ì— ê¹Šê²Œ ì „ë¬¸í™”ì‹œí‚¤ëŠ” ê³¼ì •. | ê°ì •ë¶„ì„ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ì„ ì¬í›ˆë ¨ |


### Language Model 8ì›” 11ì¼ ~ 8ì›”22ì¼

- [ë‹¤êµ­ì–´ ë²ˆì—­ê¸°](lectLanguageModel3/ë‹¤êµ­ì–´ë²ˆì—­ê¸°.ipynb) - ë‹¤êµ­ì–´ ë²ˆì—­ê¸°.  
ë²ˆì—­ ëª¨ë¸ : facebook/m2m100_418M


- [ì§€ì†ì ì¸ ìˆ˜ì§‘ ë° í•™ìŠµ](lectLanguageModel3/ë°ì´í„°ìˆ˜ì§‘_ì²­í‚¹_ì¦ë¶„.ipynb) - simple í•™ìŠµ daemon   
AutoUpdateScheduler : simple daemon ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ë° í•™ìŠµ 

- [chunking & ìš”ì•½ì •ë¦¬](lectLanguageModel3/Chunks.ipynb)
tokenizerë¡œ token ë³€í™˜í›„ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì²­í‚¹.  
ìš”ì•½ : Map Reduce êµ¬ì¡°

- [chunking & embedding](lectLanguageModel3/chunking_gradio.ipynb) - ì²­í‚¹, Gradio UI  
í† í¬ë‚˜ì´ì € ëª¨ë¸ : "klue/bert-base"  
simple chunking

- [embedding & ê²€ìƒ‰](lectLanguageModel3í† ë§_ì˜ˆì œ.ipynb) - í˜„ìƒ -> ì›ì¸, ë¶€í’ˆ, í•´ê²° ë°©ë²• ê²€ìƒ‰

    ì„ë² ë”© ëª¨ë¸ : "jhgan/ko-sroberta-multitask"  # í•œêµ­ì–´ SBERT, CPUì—ì„œë„ ë¹ ë¦„
    ~~~
    sims = (self.alias_emb @ q).tolist() 
    ~~~
    1. í–‰ë ¬ ê³±ì…ˆ ì—°ì‚° (@)
    - @ ì—°ì‚°ìëŠ” Python 3.5+ì—ì„œ ë„ì…ëœ í–‰ë ¬ ê³±ì…ˆ ì—°ì‚°ì
    - NumPy ë°°ì—´ ê°„ì˜ ë‚´ì (dot product) ê³„ì‚°
    - self.alias_emb @ qëŠ” np.dot(self.alias_emb, q)ì™€ ë™ì¼


- [PDF embedding & ê²€ìƒ‰](lectLanguageModel3/ì‚¬ìš©ì„¤ëª…ì„œ_pdf_ì±—ë´‡.ipynb) - pdf embedding, 

    simple chunking.  
    ì„ë² ë”©(ë¬¸ì¥ ë²¡í„°) & FAISS ì¸ë±ìŠ¤ êµ¬ì¶•(cosine ìœ ì‚¬ë„)  
    Gradio ê°„ë‹¨ ì±—ë´‡ UI

- [ì†Œì„¤ ì‘ì„±](lectLanguageModel3/ì†Œì„¤_ê°œì„ .ipynb) - prompt ê¸°ë°˜ ì†Œì„¤ ì‘ì„±. 

    Qwen/Qwen2.5-3B-Instruct ëª¨ë¸ ì‚¬ìš©. ì—¬ëŸ¬ ëª¨ë¸ë“¤ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ í•˜ê¸° ìš©ì´.  
    outline - ì´ˆì•ˆ - ê²€í†  ë‹¨ê³„ë¥¼ ë‹¨ê³„ë³„ ê²°ê³¼ë¥¼ ì´ìš©í•˜ì—¬ ë°œì „ì‹œí‚¤ëŠ” ë°©ì‹. ê²°ê³¼ í›„ì²˜ë¦¬ ìƒ˜í”Œ.


- [í‚¤ì›Œë“œ ê²€ìƒ‰í•œ ë‰´ìŠ¤ìš”ì•½](lectLanguageModel3/í‚¤ì›Œë“œ_ë‰´ìŠ¤ìš”ì•½.ipynb) -í‚¤ì›Œë“œ ê¸°ë°˜ ë‰´ìŠ¤ ìš”ì•½ ì‹¤ìŠµ.  
gogamza/kobart-summarization ëª¨ë¸ ì‚¬ìš©.  
Chatbotì— í‚¤ì›Œë“œ ê²€ìƒ‰í•˜ë©´ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ crawling í•˜ì—¬ íƒìƒ‰í›„ ë‰´ìŠ¤ ìš”ì•½
    ~~~ python
    # ìš”ì•½ ëª¨ë¸
    pipeline( "summarization",model="gogamza/kobart-summarization",
                    tokenizer="gogamza/kobart-summarization",
                    device=0 if torch.cuda.is_available() else -1
                )
    ~~~

- [ë‰´ìŠ¤ ê°ì„± ë¶„ì„](lectLanguageModel3/ë‰´ìŠ¤_ê¸ì •_ë¶€ì •_GPT2.ipynb)  
kogpt2 ëª¨ë¸ì„ Classification ëª¨ë¸ë¡œë„ í™œìš©í•  ìˆ˜ ìˆë‹¤  
    ~~~
    num_labels = 3
    model = AutoModelForSequenceClassification.from_pretrained("skt/kogpt2-base-v2", num_labels=num_labels)
    model.cuda()
    ~~~

### Language Model 8ì›” 4ì¼ ~ 8ì›”15ì¼
- [ê°ì„± ë¶„ì„](lectLanguageModel2/Attension_ê°ì •ë¶„ì„_ì–´í…ì…˜.ipynb) - Graph Attention Layer  
GraphEmotionNetwork í´ë˜ìŠ¤ëŠ” BERTì™€ ê·¸ë˜í”„ ì‹ ê²½ë§(GNN)ì„ ê²°í•©í•´ ê°ì • ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ 

- [ê¸°ê³„ ë²ˆì—­](lectLanguageModel2/GRU_ê¸°ê³„ë²ˆì—­.ipynb) - GRU (Gated Recurrent Unit)  
pyTorch,  encoder & decoder (GRU model base)

- [GRUë¡œ í•œê¸€ ì˜ë„ ë¶„ë¥˜ (Intent Classification)](lectLanguageModel2/GRU_ì˜ë„ë¶„ë¥˜.ipynb) - GRU(Gated Recurrent Unit)  
tensorflow,  Bidirectional GRU for better context understanding

- [í’ˆì‚¬ íƒœê¹… ì˜ˆì œ](lectLanguageModel2/NER_í’ˆì‚¬_íƒœê¹….ipynb) - LSTM & bidirectional
pyTorch, ì‹ ê²½ë§ëª¨ë¸ êµ¬ì¶•


### Language Model 8ì›” 4ì¼ ~ 8ì›”15ì¼

- [N-gram ëª¨ë¸](lectLanguageModel1/2_gram.ipynb) - ë°”ì´ê·¸ë¨ í™•ë¥ 

- [BPE ëª¨ë¸](lectLanguageModel1/BPE_Unigram.ipynb)    
sentencepiece ì‚¬ìš©

- [TF-IDF](lectLanguageModel1/TF_IDF_ì˜í™”ì¶”ì²œ(ë¶ˆìš©ì œê±°).ipynb) -  
Okt, mecab í˜•íƒœì†Œ ë¶„ì„


### Neural Network Model 


### Machine Learning




## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ
- **ì–¸ì–´**: Python
- **ML/DL**: PyTorch, Transformers, Scikit-learn
- **ë°ì´í„°**: NumPy, Pandas, Matplotlib, Seaborn
- **ì›¹**: Streamlit, BeautifulSoup, Selenium
- **NLP**: Hugging Face, KoNLPy, WordCloud