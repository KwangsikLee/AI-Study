# AI-Study ğŸ“š

AI/ML í•™ìŠµì„ ìœ„í•œ ê°œì¸ ìŠ¤í„°ë”” ì €ì¥ì†Œì…ë‹ˆë‹¤. ê¸°ì´ˆ Pythonë¶€í„° ìµœì‹  ë”¥ëŸ¬ë‹, ìì—°ì–´ì²˜ë¦¬ê¹Œì§€ ì²´ê³„ì ìœ¼ë¡œ í•™ìŠµí•œ ë‚´ìš©ì„ ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.

## ğŸ“‹ í”„ë¡œì íŠ¸ êµ¬ì¡°

### ğŸ ê¸°ì´ˆ í•™ìŠµ ëª¨ë“ˆ
- **lectBasic/**: Python ê¸°ì´ˆ, ë°ì´í„°ë² ì´ìŠ¤, Streamlit ê¸°ë³¸ê¸°
- **lectCrawling/**: ì›¹ í¬ë¡¤ë§ (BeautifulSoup, Selenium)
- **lectAPI/**: API ì—°ë™ ì‹¤ìŠµ (World Bank, Hacker News ë“±)

### ğŸ“Š ë°ì´í„° ë¶„ì„ & ML
- **lectAnalysis/**: NumPy, Pandas ë°ì´í„° ë¶„ì„ ê¸°ì´ˆ
- **lectMLDL/**: ì „í†µì  ë¨¸ì‹ ëŸ¬ë‹ (KNN, ê²°ì •íŠ¸ë¦¬, ì„ í˜•íšŒê·€)
- **lectUnsupervised/**: ë¹„ì§€ë„í•™ìŠµ (í´ëŸ¬ìŠ¤í„°ë§, PCA, t-SNE)

### ğŸ§  ë”¥ëŸ¬ë‹ & AI
- **lectDL_NN/**: PyTorch ê¸°ë°˜ ë”¥ëŸ¬ë‹ (MLP, CNN, RNN/LSTM)
- **lectPytorch/**: PyTorch ì‹¬í™” í•™ìŠµ
- **lectVision/**: ì»´í“¨í„° ë¹„ì „ (YOLO ê°ì²´íƒì§€)

### ğŸ¤– ìì—°ì–´ì²˜ë¦¬ & ì–¸ì–´ëª¨ë¸
- **lectLanguageModel1/**: NLP ê¸°ì´ˆ (í† í¬ë‚˜ì´ì§•, TF-IDF, ì„ë² ë”©)
- **lectLanguageModel2/**: ê³ ê¸‰ NLP (BERT, Attention, ê°ì •ë¶„ì„)
- **lectLanguageModel3/**: ìµœì‹  ì–¸ì–´ëª¨ë¸ (GPT-2, íŒŒì¸íŠœë‹, ì±—ë´‡)

### ğŸ” ê³ ê¸‰ ì£¼ì œ
- **RAG_Study/**: RAG(ê²€ìƒ‰ì¦ê°•ìƒì„±) ì—°êµ¬



## ğŸ“ í•™ìŠµ ì¼ì§€

### Language Model 8ì›” 11ì¼ ~ 8ì›”22ì¼


- [ë‹¤êµ­ì–´ ë²ˆì—­ê¸°](lectLanguageModel3/ë‹¤êµ­ì–´ë²ˆì—­ê¸°.ipynb) - ë‹¤êµ­ì–´ ë²ˆì—­ê¸°.  
ë²ˆì—­ ëª¨ë¸ : facebook/m2m100_418M



- [ì§€ì†ì ì¸ ìˆ˜ì§‘ ë° í•™ìŠµ](lectLanguageModel3/ë°ì´í„°ìˆ˜ì§‘_ì²­í‚¹_ì¦ë¶„.ipynb) - simple í•™ìŠµ daemon   
AutoUpdateScheduler : simple daemon ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ë° í•™ìŠµ 

- [chunking & ìš”ì•½ì •ë¦¬](lectLanguageModel3/Chunks.ipynb)
tokenizerë¡œ token ë³€í™˜í›„ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì²­í‚¹.  
ìš”ì•½ : Map Reduce êµ¬ì¡°

- [chunking & embedding](lectLanguageModel3/chunking_gradio.ipynb) - ì²­í‚¹, Gradio UI  
í† í¬ë‚˜ì´ì € ëª¨ë¸ : "klue/bert-base"  
simple chunking

- [embedding & ê²€ìƒ‰](lectLanguageModel3í† ë§_ì˜ˆì œ.ipynb) - í˜„ìƒ -> ì›ì¸, ë¶€í’ˆ, í•´ê²° ë°©ë²• ê²€ìƒ‰

    ì„ë² ë”© ëª¨ë¸ : "jhgan/ko-sroberta-multitask"  # í•œêµ­ì–´ SBERT, CPUì—ì„œë„ ë¹ ë¦„
    ~~~
    sims = (self.alias_emb @ q).tolist() 
    ~~~
    1. í–‰ë ¬ ê³±ì…ˆ ì—°ì‚° (@)
    - @ ì—°ì‚°ìëŠ” Python 3.5+ì—ì„œ ë„ì…ëœ í–‰ë ¬ ê³±ì…ˆ ì—°ì‚°ì
    - NumPy ë°°ì—´ ê°„ì˜ ë‚´ì (dot product) ê³„ì‚°
    - self.alias_emb @ qëŠ” np.dot(self.alias_emb, q)ì™€ ë™ì¼


- [PDF embedding & ê²€ìƒ‰](lectLanguageModel3/ì‚¬ìš©ì„¤ëª…ì„œ_pdf_ì±—ë´‡.ipynb) - pdf embedding, 

    simple chunking.  
    ì„ë² ë”©(ë¬¸ì¥ ë²¡í„°) & FAISS ì¸ë±ìŠ¤ êµ¬ì¶•(cosine ìœ ì‚¬ë„)  
    Gradio ê°„ë‹¨ ì±—ë´‡ UI

- [ì†Œì„¤ ì‘ì„±](lectLanguageModel3/ì†Œì„¤_ê°œì„ .ipynb) - prompt ê¸°ë°˜ ì†Œì„¤ ì‘ì„±. 

    Qwen/Qwen2.5-3B-Instruct ëª¨ë¸ ì‚¬ìš©. ì—¬ëŸ¬ ëª¨ë¸ë“¤ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ í•˜ê¸° ìš©ì´.  
    outline - ì´ˆì•ˆ - ê²€í†  ë‹¨ê³„ë¥¼ ë‹¨ê³„ë³„ ê²°ê³¼ë¥¼ ì´ìš©í•˜ì—¬ ë°œì „ì‹œí‚¤ëŠ” ë°©ì‹. ê²°ê³¼ í›„ì²˜ë¦¬ ìƒ˜í”Œ.


- [í‚¤ì›Œë“œ ê²€ìƒ‰í•œ ë‰´ìŠ¤ìš”ì•½](lectLanguageModel3/í‚¤ì›Œë“œ_ë‰´ìŠ¤ìš”ì•½.ipynb) -í‚¤ì›Œë“œ ê¸°ë°˜ ë‰´ìŠ¤ ìš”ì•½ ì‹¤ìŠµ.  
gogamza/kobart-summarization ëª¨ë¸ ì‚¬ìš©.  
Chatbotì— í‚¤ì›Œë“œ ê²€ìƒ‰í•˜ë©´ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ crawling í•˜ì—¬ íƒìƒ‰í›„ ë‰´ìŠ¤ ìš”ì•½
    ~~~ python
    # ìš”ì•½ ëª¨ë¸
    pipeline( "summarization",model="gogamza/kobart-summarization",
                    tokenizer="gogamza/kobart-summarization",
                    device=0 if torch.cuda.is_available() else -1
                )
    ~~~

- [ë‰´ìŠ¤ ê°ì„± ë¶„ì„](lectLanguageModel3/ë‰´ìŠ¤_ê¸ì •_ë¶€ì •_GPT2.ipynb)  
kogpt2 ëª¨ë¸ì„ Classification ëª¨ë¸ë¡œë„ í™œìš©í•  ìˆ˜ ìˆë‹¤  
    ~~~
    num_labels = 3
    model = AutoModelForSequenceClassification.from_pretrained("skt/kogpt2-base-v2", num_labels=num_labels)
    model.cuda()
    ~~~

### Language Model 8ì›” 4ì¼ ~ 8ì›”15ì¼
- [ê°ì„± ë¶„ì„](lectLanguageModel2/Attension_ê°ì •ë¶„ì„_ì–´í…ì…˜.ipynb) - Graph Attention Layer  
GraphEmotionNetwork í´ë˜ìŠ¤ëŠ” BERTì™€ ê·¸ë˜í”„ ì‹ ê²½ë§(GNN)ì„ ê²°í•©í•´ ê°ì • ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ 

- [ê¸°ê³„ ë²ˆì—­](lectLanguageModel2/GRU_ê¸°ê³„ë²ˆì—­.ipynb) - GRU (Gated Recurrent Unit)  
pyTorch,  encoder & decoder (GRU model base)

- [GRUë¡œ í•œê¸€ ì˜ë„ ë¶„ë¥˜ (Intent Classification)](lectLanguageModel2/GRU_ì˜ë„ë¶„ë¥˜.ipynb) - GRU(Gated Recurrent Unit)  
tensorflow,  Bidirectional GRU for better context understanding

- [í’ˆì‚¬ íƒœê¹… ì˜ˆì œ](lectLanguageModel2/NER_í’ˆì‚¬_íƒœê¹….ipynb) - LSTM & bidirectional
pyTorch, ì‹ ê²½ë§ëª¨ë¸ êµ¬ì¶•


### Language Model 8ì›” 4ì¼ ~ 8ì›”15ì¼

- [N-gram ëª¨ë¸](lectLanguageModel1/2_gram.ipynb) - ë°”ì´ê·¸ë¨ í™•ë¥ 

- [BPE ëª¨ë¸](lectLanguageModel1/BPE_Unigram.ipynb)    
sentencepiece ì‚¬ìš©

- [TF-IDF](lectLanguageModel1/TF_IDF_ì˜í™”ì¶”ì²œ(ë¶ˆìš©ì œê±°).ipynb) -  
Okt, mecab í˜•íƒœì†Œ ë¶„ì„


### Neural Network Model 


### Machine Learning




## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ
- **ì–¸ì–´**: Python
- **ML/DL**: PyTorch, Transformers, Scikit-learn
- **ë°ì´í„°**: NumPy, Pandas, Matplotlib, Seaborn
- **ì›¹**: Streamlit, BeautifulSoup, Selenium
- **NLP**: Hugging Face, KoNLPy, WordCloud