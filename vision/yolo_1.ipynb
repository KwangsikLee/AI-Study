{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3IqFUATWoRkf",
        "outputId": "7fec866e-76a9-44c8-c37f-6475d761bb97"
      },
      "outputs": [],
      "source": [
        "# 1) YOLOv8 설치\n",
        "!pip install ultralytics --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-07-18 15:10:15--  https://ultralytics.com/images/bus.jpg\n",
            "Resolving ultralytics.com (ultralytics.com)... 75.2.70.75, 99.83.190.102\n",
            "Connecting to ultralytics.com (ultralytics.com)|75.2.70.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.ultralytics.com/images/bus.jpg [following]\n",
            "--2025-07-18 15:10:15--  https://www.ultralytics.com/images/bus.jpg\n",
            "Resolving www.ultralytics.com (www.ultralytics.com)... 104.18.40.102, 172.64.147.154\n",
            "Connecting to www.ultralytics.com (www.ultralytics.com)|104.18.40.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/ultralytics/assets/releases/download/v0.0.0/bus.jpg [following]\n",
            "--2025-07-18 15:10:15--  https://github.com/ultralytics/assets/releases/download/v0.0.0/bus.jpg\n",
            "Resolving github.com (github.com)... 20.200.245.247\n",
            "Connecting to github.com (github.com)|20.200.245.247|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/521807533/8d96d999-9a94-4cfc-ba1d-7c35cc3e45cc?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-18T07%3A04%3A29Z&rscd=attachment%3B+filename%3Dbus.jpg&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-18T06%3A03%3A55Z&ske=2025-07-18T07%3A04%3A29Z&sks=b&skv=2018-11-09&sig=izo4bopsr2rvWwFvpXvWd3uHSwzNgAWcFUucRp%2FXTus%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1MjgxOTIzNCwibmJmIjoxNzUyODE4OTM0LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.wEYh5Ek_Wg-usMB_Ij7pFC8gbO8r0_Fa33SEnxDMxCE&response-content-disposition=attachment%3B%20filename%3Dbus.jpg&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-07-18 15:10:15--  https://release-assets.githubusercontent.com/github-production-release-asset/521807533/8d96d999-9a94-4cfc-ba1d-7c35cc3e45cc?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-18T07%3A04%3A29Z&rscd=attachment%3B+filename%3Dbus.jpg&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-18T06%3A03%3A55Z&ske=2025-07-18T07%3A04%3A29Z&sks=b&skv=2018-11-09&sig=izo4bopsr2rvWwFvpXvWd3uHSwzNgAWcFUucRp%2FXTus%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1MjgxOTIzNCwibmJmIjoxNzUyODE4OTM0LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.wEYh5Ek_Wg-usMB_Ij7pFC8gbO8r0_Fa33SEnxDMxCE&response-content-disposition=attachment%3B%20filename%3Dbus.jpg&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 137419 (134K) [application/octet-stream]\n",
            "Saving to: ‘sample.jpg’\n",
            "\n",
            "sample.jpg          100%[===================>] 134.20K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-07-18 15:10:16 (7.92 MB/s) - ‘sample.jpg’ saved [137419/137419]\n",
            "\n",
            "\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Numpy is not available",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 4) 모델 불러오기 및 예측\u001b[39;00m\n\u001b[32m     10\u001b[39m model = YOLO(\u001b[33m'\u001b[39m\u001b[33myolov8n.pt\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# 가장 가벼운 버전\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msample.jpg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 5) 결과 시각화\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/work/AICamp/mywork/.venv/lib/python3.11/site-packages/ultralytics/engine/model.py:185\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, source, stream, **kwargs)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    157\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    158\u001b[39m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image.Image, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np.ndarray, torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    159\u001b[39m     stream: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    160\u001b[39m     **kwargs: Any,\n\u001b[32m    161\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[32m    164\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    183\u001b[39m \u001b[33;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/work/AICamp/mywork/.venv/lib/python3.11/site-packages/ultralytics/engine/model.py:555\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mset_prompts\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[32m    554\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.set_prompts(prompts)\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor.predict_cli(source=source) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/work/AICamp/mywork/.venv/lib/python3.11/site-packages/ultralytics/engine/predictor.py:227\u001b[39m, in \u001b[36mBasePredictor.__call__\u001b[39m\u001b[34m(self, source, model, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/work/AICamp/mywork/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:35\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         response = \u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     39\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/work/AICamp/mywork/.venv/lib/python3.11/site-packages/ultralytics/engine/predictor.py:326\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;66;03m# Preprocess\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[32m0\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     im = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[32m1\u001b[39m]:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/work/AICamp/mywork/.venv/lib/python3.11/site-packages/ultralytics/engine/predictor.py:167\u001b[39m, in \u001b[36mBasePredictor.preprocess\u001b[39m\u001b[34m(self, im)\u001b[39m\n\u001b[32m    165\u001b[39m     im = im.transpose((\u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m))  \u001b[38;5;66;03m# BHWC to BCHW, (n, 3, h, w)\u001b[39;00m\n\u001b[32m    166\u001b[39m     im = np.ascontiguousarray(im)  \u001b[38;5;66;03m# contiguous\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     im = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m im = im.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    170\u001b[39m im = im.half() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.fp16 \u001b[38;5;28;01melse\u001b[39;00m im.float()  \u001b[38;5;66;03m# uint8 to fp16/32\u001b[39;00m\n",
            "\u001b[31mRuntimeError\u001b[39m: Numpy is not available"
          ]
        }
      ],
      "source": [
        "\n",
        "# 2) 라이브러리 및 모델 불러오기\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 3) 이미지 로드 (예: 인터넷에서 샘플 이미지를 다운로드)\n",
        "!wget -O sample.jpg https://ultralytics.com/images/bus.jpg\n",
        "\n",
        "# 4) 모델 불러오기 및 예측\n",
        "model = YOLO('yolov8n.pt')  # 가장 가벼운 버전\n",
        "results = model('sample.jpg')\n",
        "\n",
        "# 5) 결과 시각화\n",
        "for r in results:\n",
        "    r.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kWiN8cERounf",
        "outputId": "5bd73af4-8f08-458f-ce47-c7ab05147823"
      },
      "outputs": [],
      "source": [
        "# 여러 이미지 일괄 탐지 예제\n",
        "import glob\n",
        "import os\n",
        "import requests # Import the requests library\n",
        "\n",
        "# 1) 샘플 이미지 여러 개 다운로드\n",
        "img_urls = [\n",
        "    \"https://ultralytics.com/images/bus.jpg\",\n",
        "    \"https://ultralytics.com/images/zidane.jpg\",\n",
        "    \"https://ultralytics.com/images/bike.jpg\"\n",
        "]\n",
        "downloaded_images = []\n",
        "for i, url in enumerate(img_urls):\n",
        "    img_filename = f\"img{i}.jpg\"\n",
        "    # Use requests to download the image\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(img_filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        downloaded_images.append(img_filename)\n",
        "    else:\n",
        "        print(f\"Failed to download {url}\")\n",
        "\n",
        "# 2) 일괄 예측\n",
        "if downloaded_images:\n",
        "    results = model(downloaded_images)\n",
        "    # 3) 시각화\n",
        "    for r in results:\n",
        "        r.show()\n",
        "else:\n",
        "    print(\"No images were successfully downloaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4fPMMp7Ioxix",
        "outputId": "0023333e-47b3-429f-a417-6bc5fb397d93"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# 1) 이미지 업로드\n",
        "uploaded = files.upload()\n",
        "img_path = list(uploaded.keys())[0]\n",
        "\n",
        "# 2) YOLO 예측\n",
        "results = model(img_path)\n",
        "# 3) 결과 시각화\n",
        "for r in results:\n",
        "  r.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-HIK-Glp12b",
        "outputId": "9d4b30e2-1c25-49af-c123-a66a50ccef25"
      },
      "outputs": [],
      "source": [
        "# 1) 이미지 예측\n",
        "results = model('sample.jpg')\n",
        "\n",
        "# 2) 결과를 pandas DataFrame으로 추출\n",
        "import pandas as pd\n",
        "\n",
        "for r in results:\n",
        "    boxes = r.boxes.xyxy.cpu().numpy()\n",
        "    confs = r.boxes.conf.cpu().numpy()\n",
        "    labels = r.boxes.cls.cpu().numpy()\n",
        "    classes = [model.names[int(l)] for l in labels]\n",
        "    df = pd.DataFrame(boxes, columns=['x1','y1','x2','y2'])\n",
        "    df['class'] = classes\n",
        "    df['confidence'] = confs\n",
        "    print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "HXcdon9Ypfbn",
        "outputId": "ae2ff13c-41a8-4116-a1ce-2a54b1c493c1"
      },
      "outputs": [],
      "source": [
        "# 예: 'person'(사람) 클래스만 시각화 (클래스명은 model.names 참고)\n",
        "results = model('sample.jpg')\n",
        "for r in results:\n",
        "    img = r.orig_img.copy()\n",
        "    for box, label, conf in zip(r.boxes.xyxy.cpu().numpy(), r.boxes.cls.cpu().numpy(), r.boxes.conf.cpu().numpy()):\n",
        "        if model.names[int(label)] == 'person':\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), (255,0,0), 2)\n",
        "            cv2.putText(img, f'person {conf:.2f}', (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2)\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.title('Only Person Detection')\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": ".venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
