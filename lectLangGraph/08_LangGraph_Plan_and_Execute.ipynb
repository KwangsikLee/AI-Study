{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5c95e9bf",
      "metadata": {
        "id": "5c95e9bf"
      },
      "source": [
        "# Plan-and-Execute\n",
        "\n",
        "\n",
        "![](./assets/langgraph-plan-and-execute.png)\n",
        "\n",
        "---\n",
        "\n",
        "## Plan-and-Execute?\n",
        "\n",
        "\n",
        "\n",
        "- **장기 계획 수립**:\n",
        "복잡한 작업을 수행하기 전에 큰 그림을 그리는 장기 계획을 수립\n",
        "- **단계별 실행 및 재계획**:\n",
        "세운 계획을 단계별로 실행하고, 각 단계가 완료될 때마다 계획이 여전히 유효한지 검토한 뒤 수정할 수 있음\n",
        "  \n",
        "\n",
        "---\n",
        "\n",
        "**주요 내용**\n",
        "\n",
        "- **도구 정의**: 사용할 도구 정의\n",
        "- **실행 에이전트 정의**: 실제 작업을 실행하는 에이전트 생성\n",
        "- **상태 정의**: 에이전트의 상태 정의\n",
        "- **계획 단계**: 장기 계획을 세우는 단계 생성\n",
        "- **재계획 단계**: 작업 진행 상황에 따라 계획을 재수정하는 단계 생성\n",
        "- **그래프 생성 및 실행**: 이러한 단계들을 연결하는 그래프 생성 및 실행\n",
        "\n",
        "---\n"
      ]
    },
    {
      "attachments": {
        "image.png": {
          "image/png": "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"
        }
      },
      "cell_type": "markdown",
      "id": "34592d22",
      "metadata": {},
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e99fefd6",
      "metadata": {
        "id": "e99fefd6"
      },
      "source": [
        "## 환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YUHMnMWghK9L",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUHMnMWghK9L",
        "outputId": "1610ed70-f9f3-49d9-8e3d-87e978fc5e61"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain langchain-openai langchain-community duckduckgo-search gradio langgraph langchain-teddynote"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9oX0LdRMhTBr",
      "metadata": {
        "id": "9oX0LdRMhTBr"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated, List, Dict\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langchain_teddynote.graphs import visualize_graph\n",
        "from langchain_teddynote.tools import GoogleNews\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04b4bac9",
      "metadata": {
        "id": "04b4bac9"
      },
      "outputs": [],
      "source": [
        "# OpenAI API 키 설정 (사용자가 입력해야 함)\n",
        "# from google.colab import userdata\n",
        "\n",
        "# api_key=userdata.get('api_key')\n",
        "# api_key2=userdata.get('api_key2')\n",
        "# api_key3=userdata.get('api_key3')\n",
        "# os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = api_key2\n",
        "# os.environ[\"TAVILY_API_KEY\"] = api_key3\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "# OpenAI API 클라이언트 생성\n",
        "OPENAPI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "LangSmith_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
        "\n",
        "# 2) LangSmith 연동 필수 환경변수\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"      # 트레이싱 활성화\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"]   = \"https://api.smith.langchain.com\"  # 기본값\n",
        "os.environ[\"LANGSMITH_PROJECT\"]    = \"LangGraph_plan_ex\"\n",
        "\n",
        "\n",
        "from langchain_teddynote.tools.tavily import TavilySearch\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6afbb11",
      "metadata": {
        "id": "c6afbb11"
      },
      "source": [
        "실습에 활용할 모델명 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "afe4377f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afe4377f",
        "outputId": "43bb581c-64c1-4569-d649-2e9a18ae9a9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gpt-4o\n"
          ]
        }
      ],
      "source": [
        "from langchain_teddynote.models import get_model_name, LLMs\n",
        "\n",
        "# 모델명 정의\n",
        "MODEL_NAME = get_model_name(LLMs.GPT4o)\n",
        "print(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0856a092",
      "metadata": {
        "id": "0856a092"
      },
      "source": [
        "## 도구 정의\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4dc76788",
      "metadata": {
        "id": "4dc76788"
      },
      "outputs": [],
      "source": [
        "from langchain_teddynote.tools import TavilySearch\n",
        "\n",
        "# Tavily 검색 도구 초기화\n",
        "tools = [TavilySearch(max_results=3)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d230b5c",
      "metadata": {
        "id": "9d230b5c"
      },
      "source": [
        "## 작업 실행 에이전트 정의\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f70cf9fa",
      "metadata": {
        "id": "f70cf9fa"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# 프롬프트 정의\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer in Korean.\",\n",
        "        ),\n",
        "        (\"human\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# LLM 정의\n",
        "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
        "\n",
        "# ReAct 에이전트 생성\n",
        "agent_executor = create_react_agent(llm, tools, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ab1307f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab1307f9",
        "outputId": "e26741a8-4058-4433-c172-b6bda1cce97f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='랭체인 한국어 튜토리얼에 대해서 설명해줘', additional_kwargs={}, response_metadata={}, id='d8b59e43-f7a3-4c2f-82e9-51efe4e11b72'),\n",
              "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_BQiNIHqQi6t6QdBH3e07qFfE', 'function': {'arguments': '{\"query\":\"랭체인 한국어 튜토리얼\"}', 'name': 'tavily_web_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 159, 'total_tokens': 186, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_cbf1785567', 'id': 'chatcmpl-CC6yEkivIzVaZ3YJEDhgd0IDfn3mI', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--65e9d693-a1c3-4f59-ae77-17c14506e4c2-0', tool_calls=[{'name': 'tavily_web_search', 'args': {'query': '랭체인 한국어 튜토리얼'}, 'id': 'call_BQiNIHqQi6t6QdBH3e07qFfE', 'type': 'tool_call'}], usage_metadata={'input_tokens': 159, 'output_tokens': 27, 'total_tokens': 186, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  ToolMessage(content='[{\"url\": \"https://www.gpters.org/dev/post/langchain-notes-langchain-korean-UbTO5u3bFgmL0SQ\", \"title\": \"<랭체인LangChain 노트> - LangChain 한국어 튜토리얼 - 지피터스\", \"content\": \"### (D-Day ⏰) 가장 빠르게 AI를 배우는 곳 | 지피터스 AI스터디 오늘 모집 마감 🚀 # <랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷 **지피터스** 안녕하세요? 위키독스에 **무료 전자책**을 게재하였습니다🙌   > 본 전자책은 랭체인 한국어 튜토리얼(랭체인 공식 GitHub 튜토리얼을 한국어 예제와 문구로 번역)을 기반으로 작성하였습니다. 1. 본 전자책은 **무료**이고, 앞으로도 **무료**입니다! 2. LangChain 공식 Dev 팀에 한국어 튜토리얼 등록 문의와 공식 홈페이지의 **튜토리얼 번역에 대한 허가**도 받게 되어 드디어 글을 쓰게 되었습니다! 단, 수량이 어느 정도 되어야 하기 때문에 추후에 수요가 있다면 진행해 보겠습니다. 4. 2024년 3월 10일 LangChain 최신 라이브러리 기준 코드를 업데이트 했습니다. 깃헙에 등록된 소스코드와 순서를 맞춰서 작업하고 있습니다. GitHub 리포에서 바로 코드를 돌려보실 수 있습니다! 아직 부족한 점이 많고 업데이트 해야할 내용이 많습니다! 자주 들러주시고 업데이트 내용 확인해 주세요! 지피터스 뉴스레터\", \"score\": 0.8351749, \"raw_content\": \"[### (D-Day ⏰) 가장 빠르게 AI를 배우는 곳 | 지피터스 AI스터디 오늘 모집 마감 🚀](/ai-study-list)\\\\n\\\\n[바이브 코딩](/dev)\\\\n\\\\n[테디](/member/7XCNZeHXcK \\\\\"\\\\b테디\\\\\")\\\\n\\\\n·[바이브 코딩에 게시됨](/dev \\\\\"바이브 코딩\\\\\")\\\\n\\\\n# <랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷\\\\n\\\\n**지피터스** 안녕하세요?   \\\\n위키독스에 **무료 전자책**을 게재하였습니다🙌  \\\\n> 본 전자책은 랭체인 한국어 튜토리얼(랭체인 공식 GitHub 튜토리얼을 한국어 예제와 문구로 번역)을 기반으로 작성하였습니다.\\\\n\\\\n📘**<랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷**  \\\\n<https://wikidocs.net/book/14314>\\\\n\\\\n**추천과 공유는 저에게 정말 큰 힘이 됩니다 🫶**\\\\n\\\\n**⭐️ 희소식 몇 가지 ⭐️**\\\\n\\\\n1. 본 전자책은 **무료**이고, 앞으로도 **무료**입니다!\\\\n\\\\n2. LangChain 공식 Dev 팀에 한국어 튜토리얼 등록 문의와 공식 홈페이지의 **튜토리얼 번역에 대한 허가**도 받게 되어 드디어 글을 쓰게 되었습니다!\\\\n\\\\n3. 혹시나 종이 제본에 대한 수요가 있다면, 어느 정도 튜토리얼이 완성되는 시점에 공동구매 형식으로 제본을 만들어 보고 싶습니다! 단, 수량이 어느 정도 되어야 하기 때문에 추후에 수요가 있다면 진행해 보겠습니다.  \\\\n4. 2024년 3월 10일 LangChain 최신 라이브러리 기준 코드를 업데이트 했습니다.  \\\\n5. <https://github.com/teddylee777/langchain-kr>   \\\\n깃헙에 등록된 소스코드와 순서를 맞춰서 작업하고 있습니다. GitHub 리포에서 바로 코드를 돌려보실 수 있습니다! 추후에는 Google Colab 배지 도입을 검토 중입니다.  \\\\n  \\\\n아직 부족한 점이 많고 업데이트 해야할 내용이 많습니다!  \\\\n자주 들러주시고 업데이트 내용 확인해 주세요!  \\\\n  \\\\n그럼, 즐거운 금요일 되시기 바랍니다! 감사합니다!!\\\\n\\\\n[서비스 개발](/search?query=%EC%84%9C%EB%B9%84%EC%8A%A4%20%EA%B0%9C%EB%B0%9C&type=post)\\\\n\\\\n14\\\\n\\\\n6개의 답글\\\\n\\\\n### 👉 이 게시글도 읽어보세요\\\\n\\\\n지피터스 뉴스레터\\\\n\\\\n실제로 쓰이는 AI 활용 사례를 프롬프트와 함께 매주 세 번 보내드립니다.\\\\n\\\\n실제로 쓰이는 AI 활용 사례를  \\\\n프롬프트와 함께 매주 세 번 보내드립니다.\"}, {\"url\": \"https://m.blog.naver.com/htk1019/223388363309\", \"title\": \"LangChain 공부하기 진짜진짜 좋은 자료 - 블로그\", \"content\": \"*My Menu 닫기* 본문 바로가기 * **본문 폰트 크기 조정** **<랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷** \\\\\\\\*\\\\\\\\*추천\\\\\\\\*\\\\\\\\*은 공유할 수 있는 무료 전자책을 집필하는데 정말 큰 힘이 됩니다. 만들어 주신분께 감사드립니다. ### 이 블로그**ML** 카테고리 글 * 현재 하고 있는 일들.. * 투자 이론 교육과 비법은 다릅니다. 투자 이론 교육이 낚시하는 법을 알려주는 것이라면 투자 비법은 낚시 하는 족족 대어를 낚는 방법을 알려주는 것입니다. 그러나 비법을 찾는 것은 다릅니다. 거기에 \\'쉽다\\' 라는 워딩까지 첨가되었다면 사기에 가깝다고 보면 됩니다. 문서를 작은 조각으로 나누는 이유는 LLM 모델이 긴 문서를 잘 처리하지 못해서이다.... 개발자들이 챗봇, 질의응답 시스템, 자동 요약 등 다양한 LLM 애플리케이션을 쉽게 개발할 수 있도록 지원하는 도구모음(프레임워크) LangChain 도구의 구성 1. 이미 부품들을 조합한 예시들을 만들어 놨기 때문에 사용자는 잘 쓰면... {\\\\\"title\\\\\":\\\\\"LangChain 공부하기 진짜진짜 좋은 자료\\\\\",\\\\\"source\\\\\":\\\\\"https://blog.naver.com/htk1019/223388363309\\\\\",\\\\\"blogName\\\\\":\\\\\"엉드루 블..\\\\\",\\\\\"domainIdOrBlogId\\\\\":\\\\\"htk1019\\\\\",\\\\\"nicknameOrBlogId\\\\\":\\\\\"엉드루\\\\\",\\\\\"logNo\\\\\":223388363309,\\\\\"smartEditorVersion\\\\\":4,\\\\\"lineDisplay\\\\\":true,\\\\\"outsideDisplay\\\\\":false,\\\\\"meDisplay\\\\\":true,\\\\\"cafeDisplay\\\\\":true,\\\\\"blogDisplay\\\\\":true} 닫기\", \"score\": 0.80844593, \"raw_content\": \"[로그인이 필요합니다.](#)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n*My Menu 닫기*\\\\n\\\\n[본문 바로가기](#ct)\\\\n\\\\n[검색](/PostSearchList.naver?blogId=htk1019)\\\\n\\\\n[ML](/PostList.naver?blogId=htk1019&categoryNo=25&logCode=0&categoryName=ML#postlist_block)\\\\n\\\\nLangChain 공부하기 진짜진짜 좋은 자료\\\\n\\\\n[**엉드루**](/PostList.naver?blogId=htk1019)\\\\n\\\\n2024. 3. 19. 15:37\\\\n\\\\n[이웃추가](#)\\\\n\\\\n\\\\n* **본문 폰트 크기 조정**\\\\n  가\\\\n* [*공감*](#)\\\\n* [공유하기](#)\\\\n* [URL복사](#)\\\\n* [신고하기](#)\\\\n\\\\n\\\\n\\\\n\\\\n<https://wikidocs.net/book/14314>\\\\n\\\\n[**<랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷**\\\\n\\\\n\\\\\\\\*\\\\\\\\*추천\\\\\\\\*\\\\\\\\*은 공유할 수 있는 무료 전자책을 집필하는데 정말 큰 힘이 됩니다. \\\\\\\\*\\\\\\\\*추천\\\\\\\\*\\\\\\\\* 한 번씩만 부탁 드리겠습니다🙏🙏 ✅ \\\\\\\\*\\\\\\\\*랭체인 한국어 튜토리얼 코드저장소(Gi…\\\\n\\\\nwikidocs.net](https://wikidocs.net/book/14314)\\\\n\\\\n만들어 주신분께 감사드립니다.\\\\n\\\\n\\u200b\\\\n\\\\n실적 시즌이라 개인 포트폴리오 열심히 보는 중이지만 마스터 해야겠습니다.\\\\n\\\\n\\u200b\\\\n\\\\n\\u200b\\\\n\\\\n[*12*](#)\\\\n\\\\n[공감한 사람 보러가기](/SympathyHistoryList.naver?blogId=htk1019&logNo=223388363309&categoryId=POST)\\\\n\\\\n[댓글 *0*](/CommentList.naver?blogId=htk1019&logNo=223388363309)\\\\n공유하기\\\\n\\\\n[**엉드루**\\\\n\\\\n일상·생각이웃7,897명](/htk1019)\\\\n\\\\n[### 이 블로그**ML** 카테고리 글](https://m.blog.naver.com/PostList.naver?blogId=htk1019&categoryNo=25&logCode=0&tab=1#postlist_block)\\\\n\\\\n* [LLM을 쓰려면 역시 LangChain 2편\\\\n\\\\n  1년 전공감17댓글3](https://m.blog.naver.com/htk1019/223413412145?recommendTrackingCode=2)\\\\n* [LLM을 쓰려면 역시 LangChain 1편\\\\n\\\\n  1년 전공감24댓글1](https://m.blog.naver.com/htk1019/223413194642?recommendTrackingCode=2)\\\\n* LangChain 공부하기 진짜진짜 좋은 자료\\\\n\\\\n  1년 전공감12댓글0\\\\n* [데이터 없이 만드는 주식분석 GPTS\\\\n\\\\n  1년 전공감30댓글5](https://m.blog.naver.com/htk1019/223313391721?recommendTrackingCode=2)\\\\n* [NeuroIPS 2023 최우수 논문상 수상 논문 4편 요약\\\\n\\\\n  1년 전공감17댓글1](https://m.blog.naver.com/htk1019/223310002666?recommendTrackingCode=2)\\\\n\\\\n[### 이 블로그 인기글 ✨](https://m.blog.naver.com/htk1019?anchorBlockType=popularpostlist)\\\\n\\\\n* [현재 하고 있는 일들..\\\\n\\\\n  1주 전공감50댓글8](https://m.blog.naver.com/htk1019/223979149493?recommendCode=2&recommendTrackingCode=2)\\\\n* [투자 이론 교육과 비법은 다릅니다. 투자 이론 교육이 낚시하는 법을 알려주는 것이라면 투자 비법은 낚시 하는 족족 대어를 낚는 방법을 알려주는 것입니다. 투자를 배우고 책을 읽는 행위는 올바른 투자자의 방식입니다. 그러나 비법을 찾는 것은 다릅니다. 투자에 \\'비법\\' 이라는 단어를 결합한 것이 있다면 그 어떤 강의나 책이든 모두 무가치 합니다. 거기에 \\'쉽다\\' 라는 워딩까지 첨가되었다면 사기에 가깝다고 보면 됩니다. ...\\\\n\\\\n  잡담...\\\\n\\\\n  3주 전공감57댓글6](https://m.blog.naver.com/htk1019/223957194867?recommendCode=2&recommendTrackingCode=2)\\\\n* [팔란티어 Foundary 관련 내용\\\\n\\\\n  2개월 전공감42댓글2](https://m.blog.naver.com/htk1019/223890433039?recommendCode=2&recommendTrackingCode=2)\\\\n* [지난 시간에는 문서들을 모듈을 통해 가져오는 과정을 진행했다. 이번에는 불러온 긴 문서를 작은 단위인 청크로 나누는 도구 청크로 나눠진 텍스트 데이터를 숫자로 변환(벡터변환) 이 두가지를 알아본다. 텍스트 분리하기 (Text Splitter) 텍스트를 분리하는 작업을 청킹(chunking) 이라고 한다. 나눠진 작은 조각 단위는 청크라고 한다. 문서를 작은 조각으로 나누는 이유는 LLM 모델이 긴 문서를 잘 처리하지 못해서이다....\\\\n\\\\n  LLM을 쓰려면 역시 LangChain 6편 - 임베딩\\\\n\\\\n  1년 전공감14댓글0](https://m.blog.naver.com/htk1019/223445162918?recommendCode=2&recommendTrackingCode=2)\\\\n* [LLM을 쓰려면 역시 LangChain 5편 - RAG\\\\n\\\\n  1년 전공감15댓글0](https://m.blog.naver.com/htk1019/223442628204?recommendCode=2&recommendTrackingCode=2)\\\\n* [궁금하지 않은 내용은 쏙 빼고 핵심만 정리한 LangChain 포스트 LangChain 이란? 개발자들이 챗봇, 질의응답 시스템, 자동 요약 등 다양한 LLM 애플리케이션을 쉽게 개발할 수 있도록 지원하는 도구모음(프레임워크) LangChain 도구의 구성 1. 라이브러리 : 파이썬과 자바스크립트 라이브러리 있음. 2. 템플릿 : 작업을 도와주는 \\'조립식 건축세트\\' . 이미 부품들을 조합한 예시들을 만들어 놨기 때문에 사용자는 잘 쓰면...\\\\n\\\\n  LLM을 쓰려면 역시 LangChain 1편\\\\n\\\\n  1년 전공감24댓글1](https://m.blog.naver.com/htk1019/223413194642?recommendCode=2&recommendTrackingCode=2)\\\\n\\\\n{\\\\\"title\\\\\":\\\\\"LangChain 공부하기 진짜진짜 좋은 자료\\\\\",\\\\\"source\\\\\":\\\\\"https://blog.naver.com/htk1019/223388363309\\\\\",\\\\\"blogName\\\\\":\\\\\"엉드루 블..\\\\\",\\\\\"domainIdOrBlogId\\\\\":\\\\\"htk1019\\\\\",\\\\\"nicknameOrBlogId\\\\\":\\\\\"엉드루\\\\\",\\\\\"logNo\\\\\":223388363309,\\\\\"smartEditorVersion\\\\\":4,\\\\\"lineDisplay\\\\\":true,\\\\\"outsideDisplay\\\\\":false,\\\\\"meDisplay\\\\\":true,\\\\\"cafeDisplay\\\\\":true,\\\\\"blogDisplay\\\\\":true}\\\\n\\\\n[닫기](#)\\\\n\\\\n[이 블로그 홈](/htk1019)\\\\n\\\\n[*12*](#)\\\\n\\\\n[공감한 사람 보러가기](/SympathyHistoryList.naver?blogId=htk1019&logNo=223388363309&categoryId=POST)\\\\n\\\\n[댓글 *0*](/CommentList.naver?blogId=htk1019&logNo=223388363309)\\\\n[공유하기](#)\\\\n\\\\n**엉드루(htk1019)**\\\\n님을 이웃추가하고 새글을 받아보세요\\\\n\\\\n[취소](#)\\\\n[이웃추가](#)\\\\n\\\\n[**0** / 0](javascript:void(0);)\"}, {\"url\": \"https://kr.linkedin.com/posts/teddy-lee_langgraph-%EB%9E%AD%EC%B2%B4%EC%9D%B8%ED%95%9C%EA%B5%AD%EC%96%B4%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-langgraph-activity-7276667949925507072-MGeK\", \"title\": \"LangGraph 튜토리얼 3차 업데이트 완료 및 #랭체인한국어 ... - LinkedIn\", \"content\": \"#LangGraph 튜토리얼 3차 업데이트 완료 및 #랭체인한국어튜토리얼 1차 완료 소식을 공유 드립니다! 가입이나 로그인을 하기 위해 계속 버튼을 클릭하면 LinkedIn 사용자약관, 개인정보 처리방침, 쿠키정책에 동의하게 됩니다. Image 1: Teddy Lee #LangGraph 튜토리얼 3차 업데이트 완료 및 #랭체인한국어튜토리얼 1차 완료 소식을 공유 드립니다! 드디어, 2023년 12월부터 시작한 \\\\\"랭체인 한국어 튜토리얼\\\\\" 이 \\\\\"183\\\\\" 개의 튜토리얼을 모두 업데이트 하였으며 1차로 완료 되었습니다. 올해 마지막 업데이트 내용은 #LangGraph 와 #멀티에이전트 에 집중하였습니다. 특히, Use-Case 섹션에는 멀티에이전트 협업, 슈퍼바이저, SQL 에이전트, STORM 컨셉을 적용한 멀티에인전트 팀 연구 등 재밌고 유용한 튜토리얼을 업데이트 하였습니다. Image 11: JAE-HONG E.님, 그래픽 2024년 연말 최고의 AI RAG 컨텐츠!! Image 13: Soonji Jung, PMP님, 그래픽 Image 15: chioh song님, 그래픽 Image 16: Byeonghwi Jeong님, 그래픽 Image 18 Image 19: Teddy Lee Image 20\", \"score\": 0.7738498, \"raw_content\": \"#LangGraph 튜토리얼 3차 업데이트 완료 및 #랭체인한국어튜토리얼 1차 완료 소식을 공유 드립니다! | Teddy Lee\\\\n\\\\n===============\\\\n\\\\nLinkedIn 동의 후 가입\\\\n\\\\n가입이나 로그인을 하기 위해 계속 버튼을 클릭하면 LinkedIn [사용자약관](https://kr.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [개인정보 처리방침](https://kr.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), [쿠키정책](https://kr.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy)에 동의하게 됩니다.\\\\n\\\\n[본문 내용으로 가기](https://kr.linkedin.com/posts/teddy-lee_langgraph-%EB%9E%AD%EC%B2%B4%EC%9D%B8%ED%95%9C%EA%B5%AD%EC%96%B4%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-langgraph-activity-7276667949925507072-MGeK#main-content)[LinkedIn](https://kr.linkedin.com/?trk=public_post_nav-header-logo)\\\\n*   [글](https://www.linkedin.com/pulse/topics/home/?trk=public_post_guest_nav_menu_articles)\\\\n*   [사람](https://www.linkedin.com/pub/dir/+/+?trk=public_post_guest_nav_menu_people)\\\\n*   [온라인클래스](https://www.linkedin.com/learning/search?trk=public_post_guest_nav_menu_learning)\\\\n*   [채용공고](https://www.linkedin.com/jobs/search?trk=public_post_guest_nav_menu_jobs)\\\\n*   [게임](https://www.linkedin.com/games?trk=public_post_guest_nav_menu_games)\\\\n\\\\n[회원 가입](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_nav-header-join)[로그인](https://www.linkedin.com/login?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&fromSignIn=true&trk=public_post_nav-header-signin)\\\\n\\\\nTeddy Lee님의 업데이트\\\\n================\\\\n\\\\n[![Image 1: Teddy Lee](https://static.licdn.com/aero-v1/sc/h/9c8pery4andzj6ohjkjp54ma2)](https://kr.linkedin.com/in/teddy-lee?trk=public_post_feed-actor-image)\\\\n\\\\n[Teddy Lee](https://kr.linkedin.com/in/teddy-lee?trk=public_post_feed-actor-name)\\\\n\\\\n테디노트 YouTuber, LangChain Ambassador, Data & AI Lover\\\\n\\\\n 6개월 \\\\n\\\\n*   [글 신고](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fkr.linkedin.com%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\\\\n\\\\n[#LangGraph](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Flanggraph&trk=public_post-text) 튜토리얼 3차 업데이트 완료 및 [#랭체인한국어튜토리얼](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2FRunTtOSUaUkuQvPSLqTWSTSaREeSLy&trk=public_post-text) 1차 완료 소식을 공유 드립니다! 드디어, 2023년 12월부터 시작한 \\\\\"랭체인 한국어 튜토리얼\\\\\" 이 \\\\\"183\\\\\" 개의 튜토리얼을 모두 업데이트 하였으며 1차로 완료 되었습니다. 딱 1년 걸렸네요! 지난 1년간 2329건의 추천을 받았는데, 정말 감사드립니다! 올해 마지막 업데이트 내용은 [#LangGraph](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Flanggraph&trk=public_post-text) 와 [#멀티에이전트](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2FRGWTYySLSSTWSWKTXu&trk=public_post-text) 에 집중하였습니다. 특히, Use-Case 섹션에는 멀티에이전트 협업, 슈퍼바이저, SQL 에이전트, STORM 컨셉을 적용한 멀티에인전트 팀 연구 등 재밌고 유용한 튜토리얼을 업데이트 하였습니다. 파일 하나하나가 분량이 상당하여 교정 및 검수에 오랜기간 걸렸습니다. 많은 분들께 도움이 되셨으면 좋겠습니다. 링크: - 위키독스: [https://lnkd.in/d_SsWJ_v](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fd_SsWJ_v&urlhash=Ut0l&trk=public_post-text) - 소스코드: [https://lnkd.in/g-CTTC-3](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fg-CTTC-3&urlhash=Wqqx&trk=public_post-text) 주요내용: 1. Agentic RAG 2. Adaptive RAG 3. 에이전트 대화 시뮬레이션 4. 사용자 요구사항 기반 메타 프롬프트 생성 5. CRAG 6. Self-RAG 7. Plan and Execute 8. 멀티에이전트 협업 네트워크 9. 멀티에이전트 감독자 10. 계층적 멀티에이전트 팀 협업 11. SQL 데이터베이스 에이전트 12. STORM 개념을 도입한 연구를 위한 멀티 에이전트 ---- 여기서, 끝은 아닙니다. 매일 같이 새로운 내용이 나오고, 신규 기능이 업데이트 되면서 앞으로는 기존의 자료를 업데이트 하고, 레거시 코드를 보완해 나가도록 하겠습니다. 지난 5월부터 촬영한 패스트캠퍼스 \\\\\"RAG 비법노트\\\\\" 강의도 약 70~80시간 분량의 촬영을 모두 마치고, 7개월만에 강의 촬영을 마쳤습니다. 입문하시는 분들께 좋은 참고자료가 될 수 있도록 많은 프로젝트도 준비하였으니, 혹시 관심있으시다면 패스트캠퍼스에서 수강하실 수 있습니다. ---- 올 한해 \\\\\"랭체인 한국어 튜토리얼\\\\\" 을 많이 방문해 주시고 공유해 주셔서 다시 한 번 깊은 감사를 드립니다. 1년 전의 약속인 \\\\\"올해 안에 튜토리얼 완료\\\\\" 를 지킬 수 있음에 저 스스로 매우 기쁘게 생각합니다. 얼마 남지 않은 2024년 한 해 잘 마무리 하시고요. 2025년에도 이루고자 하시는 일 모두 이루셨으면 좋겠습니다. 감사합니다.\\\\n\\\\n*   ![Image 2: 이 이미지의 대체 텍스트 설명이 없음](https://kr.linkedin.com/posts/teddy-lee_langgraph-%EB%9E%AD%EC%B2%B4%EC%9D%B8%ED%95%9C%EA%B5%AD%EC%96%B4%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-langgraph-activity-7276667949925507072-MGeK)\\\\n*   ![Image 3: 이 이미지의 대체 텍스트 설명이 없음](https://kr.linkedin.com/posts/teddy-lee_langgraph-%EB%9E%AD%EC%B2%B4%EC%9D%B8%ED%95%9C%EA%B5%AD%EC%96%B4%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-langgraph-activity-7276667949925507072-MGeK)\\\\n*   ![Image 4: 이 이미지의 대체 텍스트 설명이 없음](https://kr.linkedin.com/posts/teddy-lee_langgraph-%EB%9E%AD%EC%B2%B4%EC%9D%B8%ED%95%9C%EA%B5%AD%EC%96%B4%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-langgraph-activity-7276667949925507072-MGeK)\\\\n*   ![Image 5: 이 이미지의 대체 텍스트 설명이 없음](https://kr.linkedin.com/posts/teddy-lee_langgraph-%EB%9E%AD%EC%B2%B4%EC%9D%B8%ED%95%9C%EA%B5%AD%EC%96%B4%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-langgraph-activity-7276667949925507072-MGeK)\\\\n*   ![Image 6: 이 이미지의 대체 텍스트 설명이 없음](https://kr.linkedin.com/posts/teddy-lee_langgraph-%EB%9E%AD%EC%B2%B4%EC%9D%B8%ED%95%9C%EA%B5%AD%EC%96%B4%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-langgraph-activity-7276667949925507072-MGeK) +3  \\\\n\\\\n[![Image 7](https://kr.linkedin.com/posts/teddy-lee_langgraph-%EB%9E%AD%EC%B2%B4%EC%9D%B8%ED%95%9C%EA%B5%AD%EC%96%B4%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-langgraph-activity-7276667949925507072-MGeK)![Image 8](https://kr.linkedin.com/posts/teddy-lee_langgraph-%EB%9E%AD%EC%B2%B4%EC%9D%B8%ED%95%9C%EA%B5%AD%EC%96%B4%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-langgraph-activity-7276667949925507072-MGeK)![Image 9](https://kr.linkedin.com/posts/teddy-lee_langgraph-%EB%9E%AD%EC%B2%B4%EC%9D%B8%ED%95%9C%EA%B5%AD%EC%96%B4%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-langgraph-activity-7276667949925507072-MGeK) 238](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_social-actions-reactions)[댓글 15](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_social-actions-comments)\\\\n\\\\n[추천](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_like-cta)[댓글](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment-cta)\\\\n\\\\n 공유 \\\\n*   복사\\\\n*   LinkedIn\\\\n*   Facebook\\\\n*   Twitter\\\\n\\\\n[![Image 10: Kwang-Yong Jung님, 그래픽](https://kr.linkedin.com/posts/teddy-lee_langgraph-%EB%9E%AD%EC%B2%B4%EC%9D%B8%ED%95%9C%EA%B5%AD%EC%96%B4%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-langgraph-activity-7276667949925507072-MGeK)](https://kr.linkedin.com/in/kwang-yong-jung?trk=public_post_comment_actor-image)\\\\n\\\\n[Kwang-Yong Jung](https://kr.linkedin.com/in/kwang-yong-jung?trk=public_post_comment_actor-name)\\\\nAI Product Manager | AI, ML, Innovative Solutions in Domain\\\\n\\\\n 6개월 \\\\n\\\\n*   [댓글 신고](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fkr.linkedin.com%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\\\\n\\\\n[Teddy Lee](https://kr.linkedin.com/in/teddy-lee?trk=public_post_comment-text) 님 저희 프로젝트에서 딱 필요한 부분을 이렇게 릴리즈 해주셔서 감사합니다. 같이 열심히 공부하면서 적용해 보겠습니다.\\\\n\\\\n[추천](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_like)[댓글](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_reply) 반응 1 \\\\n\\\\n[![Image 11: JAE-HONG E.님, 그래픽](https://kr.linkedin.com/posts/teddy-lee_langgraph-%EB%9E%AD%EC%B2%B4%EC%9D%B8%ED%95%9C%EA%B5%AD%EC%96%B4%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-langgraph-activity-7276667949925507072-MGeK)](https://kr.linkedin.com/in/jae-hong-e-93368248?trk=public_post_comment_actor-image)\\\\n\\\\n[JAE-HONG E.](https://kr.linkedin.com/in/jae-hong-e-93368248?trk=public_post_comment_actor-name)\\\\nVP, Head of AI/DX | Ph.D. & 25+ Years in AI 🤖\\\\n\\\\n 6개월 \\\\n\\\\n*   [댓글 신고](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fkr.linkedin.com%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\\\\n\\\\n2024년 연말 최고의 AI RAG 컨텐츠!! 👍 🎁🎄🎅\\\\n\\\\n[추천](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_like)[댓글](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_reply)[반응 3](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_reactions) 반응 4 \\\\n\\\\n[![Image 12: Cheonsol Lee님, 그래픽](https://kr.linkedin.com/posts/teddy-lee_langgraph-%EB%9E%AD%EC%B2%B4%EC%9D%B8%ED%95%9C%EA%B5%AD%EC%96%B4%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-langgraph-activity-7276667949925507072-MGeK)](https://kr.linkedin.com/in/cheonsol-lee/ko?trk=public_post_comment_actor-image)\\\\n\\\\n[Cheonsol Lee](https://kr.linkedin.com/in/cheonsol-lee/ko?trk=public_post_comment_actor-name)\\\\nAI Research Engineer at Hana Institute of Technology\\\\n\\\\n 6개월 \\\\n\\\\n*   [댓글 신고](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fkr.linkedin.com%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\\\\n\\\\n감사합니다. 처음부터 차근차근 따라가보겠습니다 ㅎㅎ\\\\n\\\\n[추천](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_like)[댓글](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_reply) 반응 1 \\\\n\\\\n[![Image 13: Soonji Jung, PMP님, 그래픽](https://kr.linkedin.com/posts/teddy-lee_langgraph-%EB%9E%AD%EC%B2%B4%EC%9D%B8%ED%95%9C%EA%B5%AD%EC%96%B4%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-langgraph-activity-7276667949925507072-MGeK)](https://kr.linkedin.com/in/soonji-jung?trk=public_post_comment_actor-image)\\\\n\\\\n[Soonji Jung, PMP](https://kr.linkedin.com/in/soonji-jung?trk=public_post_comment_actor-name)\\\\nPM 과 개발자 중간 어디쯤 | 책임연구원 | 영상SW개발그룹 | 영상솔루션팀 | R&D센터 | 에스원\\\\n\\\\n 6개월 \\\\n\\\\n*   [댓글 신고](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fkr.linkedin.com%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\\\\n\\\\n감사합니다~! 잘 읽고 공부하겠습니다 :)\\\\n\\\\n[추천](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_like)[댓글](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_reply)[반응 1](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_reactions) 반응 2 \\\\n\\\\n[![Image 14: 강동민님, 그래픽](https://kr.linkedin.com/posts/teddy-lee_langgraph-%EB%9E%AD%EC%B2%B4%EC%9D%B8%ED%95%9C%EA%B5%AD%EC%96%B4%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-langgraph-activity-7276667949925507072-MGeK)](https://kr.linkedin.com/in/%EB%8F%99%EB%AF%BC-%EA%B0%95-5a1b50303?trk=public_post_comment_actor-image)\\\\n\\\\n[강동민](https://kr.linkedin.com/in/%EB%8F%99%EB%AF%BC-%EA%B0%95-5a1b50303?trk=public_post_comment_actor-name)\\\\n문제를 해결중입니다 :)\\\\n\\\\n 6개월 \\\\n\\\\n*   [댓글 신고](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fkr.linkedin.com%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\\\\n\\\\n와.....! 정말 감사합니다 테디님\\\\n\\\\n[추천](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_like)[댓글](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_reply)[반응 1](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_reactions) 반응 2 \\\\n\\\\n[![Image 15: chioh song님, 그래픽](https://kr.linkedin.com/posts/teddy-lee_langgraph-%EB%9E%AD%EC%B2%B4%EC%9D%B8%ED%95%9C%EA%B5%AD%EC%96%B4%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-langgraph-activity-7276667949925507072-MGeK)](https://kr.linkedin.com/in/chioh?trk=public_post_comment_actor-image)\\\\n\\\\n[chioh song](https://kr.linkedin.com/in/chioh?trk=public_post_comment_actor-name)\\\\n기업, 기관 대상 DX, AX 솔루션, 컨설팅 및 임직원 교육 제공, 강사 에이전시 운영\\\\n\\\\n 6개월 \\\\n\\\\n*   [댓글 신고](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fkr.linkedin.com%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\\\\n\\\\n크리스마스 선물인가요🎄🎁👍👍👍\\\\n\\\\n[추천](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_like)[댓글](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_reply)[반응 1](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_reactions) 반응 2 \\\\n\\\\n[![Image 16: Byeonghwi Jeong님, 그래픽](https://kr.linkedin.com/posts/teddy-lee_langgraph-%EB%9E%AD%EC%B2%B4%EC%9D%B8%ED%95%9C%EA%B5%AD%EC%96%B4%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-langgraph-activity-7276667949925507072-MGeK)](https://kr.linkedin.com/in/byeonghwi-jeong-00243025b?trk=public_post_comment_actor-image)\\\\n\\\\n[Byeonghwi Jeong](https://kr.linkedin.com/in/byeonghwi-jeong-00243025b?trk=public_post_comment_actor-name)\\\\n벡엔드 개발자\\\\n\\\\n 6개월 \\\\n\\\\n*   [댓글 신고](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fkr.linkedin.com%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\\\\n\\\\n감사합니다!!! 강의 열심히 듣겠습니다:)\\\\n\\\\n[추천](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_like)[댓글](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_reply)[반응 1](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_reactions) 반응 2 \\\\n\\\\n[![Image 17: 박상희님, 그래픽](https://kr.linkedin.com/posts/teddy-lee_langgraph-%EB%9E%AD%EC%B2%B4%EC%9D%B8%ED%95%9C%EA%B5%AD%EC%96%B4%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-langgraph-activity-7276667949925507072-MGeK)](https://kr.linkedin.com/in/%EC%83%81%ED%9D%AC-%EB%B0%95-705152250?trk=public_post_comment_actor-image)\\\\n\\\\n[박상희](https://kr.linkedin.com/in/%EC%83%81%ED%9D%AC-%EB%B0%95-705152250?trk=public_post_comment_actor-name)\\\\nAI콘텐츠PD / Education Lover\\\\n\\\\n 6개월 \\\\n\\\\n*   [댓글 신고](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fkr.linkedin.com%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\\\\n\\\\n약 61시간의 영상을 만드시느라 정말 고생 많으셨습니다. 그 시간보다 훨씬 더 많은 노력과 정성을 쏟으셨을 텐데, 25년에도 더욱 좋은 결과 있으시길 진심으로 응원하겠습니다!🥰\\\\n\\\\n[추천](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_like)[댓글](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_comment_reply) 반응 1 \\\\n\\\\n[더 보기](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_see-more-comments)\\\\n댓글을 보거나 남기려면 [로그인](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_feed-cta-banner-cta)\\\\n\\\\n![Image 18](https://static.licdn.com/aero-v1/sc/h/5q92mjc5c51bjlwaj3rs9aa82)\\\\n\\\\n![Image 19: Teddy Lee](https://static.licdn.com/aero-v1/sc/h/9c8pery4andzj6ohjkjp54ma2)\\\\n팔로워 6,673명\\\\n\\\\n*   [업데이트 279개](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fin%2Fteddy-lee%2Frecent-activity%2F&trk=public_post_follow-posts)\\\\n\\\\n[프로필 보기](https://kr.linkedin.com/in/teddy-lee?trk=public_post_follow-view-profile)[팔로우](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Ffeed%2Fupdate%2Furn%3Ali%3Aactivity%3A7276667949925507072&trk=public_post_follow)\\\\n\\\\n*   LinkedIn© 2025\\\\n*   [정보](https://about.linkedin.com/?trk=d_public_post_footer-about)\\\\n*   [웹접근성](https://www.linkedin.com/accessibility?trk=d_public_post_footer-accessibility)\\\\n*   [사용자약관](https://www.linkedin.com/legal/user-agreement?trk=d_public_post_footer-user-agreement)\\\\n*   [개인정보 처리방침](https://www.linkedin.com/legal/privacy-policy?trk=d_public_post_footer-privacy-policy)\\\\n*   [Your California Privacy Choices](https://www.linkedin.com/legal/california-privacy-disclosure?trk=d_public_post_footer-california-privacy-rights-act)\\\\n*   [쿠키정책](https://www.linkedin.com/legal/cookie-policy?trk=d_public_post_footer-cookie-policy)\\\\n*   [저작권정책](https://www.linkedin.com/legal/copyright-policy?trk=d_public_post_footer-copyright-policy)\\\\n*   [브랜드정책](https://brand.linkedin.com/policies?trk=d_public_post_footer-brand-policy)\\\\n*   [비회원 설정](https://www.linkedin.com/psettings/guest-controls?trk=d_public_post_footer-guest-controls)\\\\n*   [커뮤니티정책](https://www.linkedin.com/legal/professional-community-policies?trk=d_public_post_footer-community-guide)\\\\n*   \\\\n    *    아랍어(العربية) \\\\n    *    벵골어(বাংলা) \\\\n    *    체코어(Čeština) \\\\n    *    덴마크어(Dansk) \\\\n    *    독일어(Deutsch) \\\\n    *    그리스어(Ελληνικά) \\\\n    *    영어(English) \\\\n    *    스페인어(Español) \\\\n    *    페르시아어(فارسی) \\\\n    *    핀란드어(Suomi) \\\\n    *    프랑스어(Français) \\\\n    *    힌디어(हिंदी) \\\\n    *    헝가리어(Magyar) \\\\n    *    인도네시아어(Bahasa Indonesia) \\\\n    *    이탈리아어(Italiano) \\\\n    *    히브리어(עברית) \\\\n    *    일본어(日本語) \\\\n    *   **한국어(Korean)**\\\\n    *    마라티어(मराठी) \\\\n    *    말레이시아어(Bahasa Malaysia) \\\\n    *    네덜란드어(Nederlands) \\\\n    *    노르웨이어(Norsk) \\\\n    *    펀자브어(ਪੰਜਾਬੀ) \\\\n    *    폴란드어(Polski) \\\\n    *    포르투갈어(Português) \\\\n    *    루마니아어(Română) \\\\n    *    러시아어(Русский) \\\\n    *    스웨덴어(Svenska) \\\\n    *    텔루구어(తెలుగు) \\\\n    *    태국어(ภาษาไทย) \\\\n    *    필리핀어(Tagalog) \\\\n    *    터키어(Türkçe) \\\\n    *    우크라이나어(Українська) \\\\n    *    베트남어(Tiếng Việt) \\\\n    *    중국어 간체(简体中文) \\\\n    *    중국어 번체(正體中文) \\\\n\\\\n 언어 \\\\n\\\\n![Image 20](https://static.licdn.com/aero-v1/sc/h/5k9cgtx8rhoyqkcxfoncu1svl)\\\\n로그인해서 콘텐츠 더보기\\\\n-------------\\\\n\\\\n무료 계정을 만들거나 로그인하여 검색을 계속하세요.\\\\n\\\\nContinue with Google Continue with Google\\\\n\\\\n 로그인 \\\\n\\\\n어서오세요!\\\\n------\\\\n\\\\n 이메일 또는 전화  \\\\n\\\\n 비밀번호  \\\\n\\\\n표시\\\\n\\\\n[비밀번호를 잊으셨나요?](https://www.linkedin.com/uas/request-password-reset?trk=public_post_contextual-sign-in-modal_sign-in-modal_forgot_password) 로그인 \\\\n\\\\n또는\\\\n\\\\n가입이나 로그인을 하기 위해 계속 버튼을 클릭하면 LinkedIn [사용자약관](https://kr.linkedin.com/legal/user-agreement?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_user-agreement), [개인정보 처리방침](https://kr.linkedin.com/legal/privacy-policy?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_privacy-policy), [쿠키정책](https://kr.linkedin.com/legal/cookie-policy?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_cookie-policy)에 동의하게 됩니다.\\\\n\\\\nContinue with Google Continue with Google\\\\n\\\\nLinkedIn이 처음이세요? [회원 가입](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_contextual-sign-in-modal_sign-in-modal_join-link)\\\\n\\\\n또는\\\\n\\\\nLinkedIn이 처음이세요? [회원 가입](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fkr%2Elinkedin%2Ecom%2Fposts%2Fteddy-lee_langgraph-%25EB%259E%25AD%25EC%25B2%25B4%25EC%259D%25B8%25ED%2595%259C%25EA%25B5%25AD%25EC%2596%25B4%25ED%258A%259C%25ED%2586%25A0%25EB%25A6%25AC%25EC%2596%25BC-langgraph-activity-7276667949925507072-MGeK&trk=public_post_contextual-sign-in-modal_join-link)\\\\n\\\\n가입이나 로그인을 하기 위해 계속 버튼을 클릭하면 LinkedIn [사용자약관](https://kr.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [개인정보 처리방침](https://kr.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), [쿠키정책](https://kr.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy)에 동의하게 됩니다.\\\\n\"}]', name='tavily_web_search', id='9d3fb7de-8c07-47d0-aead-6d702b46acdb', tool_call_id='call_BQiNIHqQi6t6QdBH3e07qFfE'),\n",
              "  AIMessage(content='랭체인(LangChain) 한국어 튜토리얼에 대한 정보를 찾았습니다. 지피터스라는 사이트에서 무료 전자책 형태로 제공되는 랭체인 한국어 튜토리얼이 있습니다. 이 전자책은 랭체인 공식 GitHub 튜토리얼을 한국어로 번역한 것으로, 무료로 제공되며 앞으로도 무료로 유지될 예정입니다. 이 튜토리얼은 2024년 3월 10일 기준으로 최신 라이브러리에 맞춰 업데이트되었습니다. GitHub 리포지토리에서 소스코드를 확인하고 실행해볼 수 있습니다.\\n\\n자세한 내용은 [지피터스의 LangChain 한국어 튜토리얼](https://www.gpters.org/dev/post/langchain-notes-langchain-korean-UbTO5u3bFgmL0SQ)에서 확인할 수 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 197, 'prompt_tokens': 17547, 'total_tokens': 17744, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_cbf1785567', 'id': 'chatcmpl-CC6yGGg8RPR5ujx5JyMttOJYXzxBM', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--6d566637-f56d-4834-a1c0-b2fcb5744f29-0', usage_metadata={'input_tokens': 17547, 'output_tokens': 197, 'total_tokens': 17744, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 에이전트 실행\n",
        "agent_executor.invoke(\n",
        "    {\"messages\": [(\"user\", \"랭체인 한국어 튜토리얼에 대해서 설명해줘\")]}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05f6efaa",
      "metadata": {
        "id": "05f6efaa"
      },
      "source": [
        "## 상태 정의\n",
        "\n",
        "- `input`: 사용자의 입력\n",
        "- `plan`: 현재 계획\n",
        "- `past_steps`: 이전에 실행한 계획과 실행 결과\n",
        "- `response`: 최종 응답"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "27c44393",
      "metadata": {
        "id": "27c44393"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "from typing import Annotated, List, Tuple\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "# 상태 정의\n",
        "class PlanExecute(TypedDict):\n",
        "    input: Annotated[str, \"User's input\"]\n",
        "    plan: Annotated[List[str], \"Current plan\"]\n",
        "    past_steps: Annotated[List[Tuple], operator.add]\n",
        "    response: Annotated[str, \"Final response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8c5d026",
      "metadata": {
        "id": "b8c5d026"
      },
      "source": [
        "## 계획(Plan) 단계\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ad37fbe6",
      "metadata": {
        "id": "ad37fbe6"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "\n",
        "# Plan 모델 정의\n",
        "class Plan(BaseModel):\n",
        "    \"\"\"Sorted steps to execute the plan\"\"\"\n",
        "\n",
        "    steps: Annotated[List[str], \"Different steps to follow, should be in sorted order\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "726052a2",
      "metadata": {
        "id": "726052a2"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# 계획 수립을 위한 프롬프트 템플릿 생성\n",
        "planner_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
        "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
        "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
        "Answer in Korean.\"\"\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "planner = planner_prompt | ChatOpenAI(\n",
        "    model=MODEL_NAME, temperature=0\n",
        ").with_structured_output(Plan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c782515c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c782515c",
        "outputId": "be509ba6-089b-4ce3-9b9f-208c60384fb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Plan(steps=['LangGraph의 정의와 목적을 이해한다.', 'LangGraph의 주요 기능과 특징을 조사한다.', 'LangGraph의 장점을 나열한다.', 'LangGraph의 단점을 나열한다.', 'LangGraph를 사용하는 이유를 정리한다.'])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Planner 실행\n",
        "planner.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            (\n",
        "                \"user\",\n",
        "                \"LangGraph 의 핵심 장단점과 LangGraph 를 사용하는 이유는 무엇인가?\",\n",
        "            )\n",
        "        ]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef28ddf5",
      "metadata": {
        "id": "ef28ddf5"
      },
      "source": [
        "## 재계획(Re-Plan) 단계\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c80e74e3",
      "metadata": {
        "id": "c80e74e3"
      },
      "outputs": [],
      "source": [
        "from typing import Union\n",
        "\n",
        "\n",
        "class Response(BaseModel):\n",
        "    \"\"\"Response to user.\"\"\"\n",
        "\n",
        "    # 사용자 응답\n",
        "    response: str\n",
        "\n",
        "\n",
        "class Act(BaseModel):\n",
        "    \"\"\"Action to perform.\"\"\"\n",
        "\n",
        "    # 수행할 작업: \"Response\", \"Plan\". 사용자에게 응답할 경우 Response 사용, 추가 도구 사용이 필요할 경우 Plan 사용\n",
        "    action: Union[Response, Plan] = Field(\n",
        "        description=\"Action to perform. If you want to respond to user, use Response. \"\n",
        "        \"If you need to further use tools to get the answer, use Plan.\"\n",
        "    )\n",
        "\n",
        "\n",
        "# 계획을 재수립하기 위한 프롬프트 정의\n",
        "replanner_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
        "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
        "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
        "\n",
        "Your objective was this:\n",
        "{input}\n",
        "\n",
        "Your original plan was this:\n",
        "{plan}\n",
        "\n",
        "You have currently done the follow steps:\n",
        "{past_steps}\n",
        "\n",
        "Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\n",
        "\n",
        "Answer in Korean.\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "# Replanner 생성\n",
        "replanner = replanner_prompt | ChatOpenAI(\n",
        "    model=MODEL_NAME, temperature=0\n",
        ").with_structured_output(Act)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b88a7250",
      "metadata": {
        "id": "b88a7250"
      },
      "source": [
        "## 그래프 생성\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "df12d945",
      "metadata": {
        "id": "df12d945"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "# 사용자 입력을 기반으로 계획을 생성하고 반환\n",
        "def plan_step(state: PlanExecute):\n",
        "    plan = planner.invoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
        "    # 생성된 계획의 단계 리스트 반환\n",
        "    return {\"plan\": plan.steps}\n",
        "\n",
        "\n",
        "# 에이전트 실행기를 사용하여 주어진 작업을 수행하고 결과를 반환\n",
        "def execute_step(state: PlanExecute):\n",
        "    plan = state[\"plan\"]\n",
        "    # 계획을 문자열로 변환하여 각 단계에 번호를 매김\n",
        "    plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n",
        "    task = plan[0]\n",
        "    # 현재 실행할 작업을 포맷팅하여 에이전트에 전달\n",
        "    task_formatted = f\"\"\"For the following plan:\n",
        "{plan_str}\\n\\nYou are tasked with executing [step 1. {task}].\"\"\"\n",
        "    # 에이전트 실행기를 통해 작업 수행 및 결과 수신\n",
        "    agent_response = agent_executor.invoke({\"messages\": [(\"user\", task_formatted)]})\n",
        "    # 이전 단계와 그 결과를 포함하는 딕셔너리 반환\n",
        "    return {\n",
        "        \"past_steps\": [(task, agent_response[\"messages\"][-1].content)],\n",
        "    }\n",
        "\n",
        "\n",
        "# 이전 단계의 결과를 바탕으로 계획을 업데이트하거나 최종 응답을 반환\n",
        "def replan_step(state: PlanExecute):\n",
        "    output = replanner.invoke(state)\n",
        "    # 응답이 사용자에게 반환될 경우\n",
        "    if isinstance(output.action, Response):\n",
        "        return {\"response\": output.action.response}\n",
        "    # 추가 단계가 필요할 경우 계획의 단계 리스트 반환\n",
        "    else:\n",
        "        next_plan = output.action.steps\n",
        "        if len(next_plan) == 0:\n",
        "            return {\"response\": \"No more steps needed.\"}\n",
        "        else:\n",
        "            return {\"plan\": next_plan}\n",
        "\n",
        "\n",
        "# 에이전트의 실행 종료 여부를 결정하는 함수\n",
        "def should_end(state: PlanExecute):\n",
        "    if \"response\" in state and state[\"response\"]:\n",
        "        return \"final_report\"\n",
        "    else:\n",
        "        return \"execute\"\n",
        "\n",
        "\n",
        "final_report_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are given the objective and the previously done steps. Your task is to generate a final report in markdown format.\n",
        "Final report should be written in professional tone.\n",
        "\n",
        "Your objective was this:\n",
        "\n",
        "{input}\n",
        "\n",
        "Your previously done steps(question and answer pairs):\n",
        "\n",
        "{past_steps}\n",
        "\n",
        "Generate a final report in markdown format. Write your response in Korean.\"\"\"\n",
        ")\n",
        "\n",
        "final_report = (\n",
        "    final_report_prompt\n",
        "    | ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "def generate_final_report(state: PlanExecute):\n",
        "    past_steps = \"\\n\\n\".join(\n",
        "        [\n",
        "            f\"Question: {past_step[0]}\\n\\nAnswer: {past_step[1]}\\n\\n####\"\n",
        "            for past_step in state[\"past_steps\"]\n",
        "        ]\n",
        "    )\n",
        "    response = final_report.invoke({\"input\": state[\"input\"], \"past_steps\": past_steps})\n",
        "    return {\"response\": response}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff7ebe60",
      "metadata": {
        "id": "ff7ebe60"
      },
      "source": [
        "## 그래프 생성\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "16abdcf5",
      "metadata": {
        "id": "16abdcf5"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "\n",
        "# 그래프 생성\n",
        "workflow = StateGraph(PlanExecute)\n",
        "\n",
        "# 노드 정의\n",
        "workflow.add_node(\"planner\", plan_step)\n",
        "workflow.add_node(\"execute\", execute_step)\n",
        "workflow.add_node(\"replan\", replan_step)\n",
        "workflow.add_node(\"final_report\", generate_final_report)\n",
        "\n",
        "# 엣지 정의\n",
        "workflow.add_edge(START, \"planner\")\n",
        "workflow.add_edge(\"planner\", \"execute\")\n",
        "workflow.add_edge(\"execute\", \"replan\")\n",
        "workflow.add_edge(\"final_report\", END)\n",
        "\n",
        "# 조건부 엣지: replan 후 종료 여부를 결정하는 함수 사용\n",
        "workflow.add_conditional_edges(\n",
        "    \"replan\",\n",
        "    should_end,\n",
        "    {\"execute\": \"execute\", \"final_report\": \"final_report\"},\n",
        ")\n",
        "\n",
        "# 그래프 컴파일\n",
        "app = workflow.compile(checkpointer=MemorySaver())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd850b8c",
      "metadata": {
        "id": "bd850b8c"
      },
      "source": [
        "그래프를 시각화 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "0ee7d379",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "0ee7d379",
        "outputId": "be79a6a9-cecb-4d50-fd1f-b45f2d01d99c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain_teddynote.graphs import visualize_graph\n",
        "\n",
        "visualize_graph(app, xray=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6d266cb",
      "metadata": {
        "id": "b6d266cb"
      },
      "source": [
        "## 그래프 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6323963b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6323963b",
        "outputId": "46403b06-c1ed-4466-cd04-11440500dd9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mplanner\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "Naive RAG와 Modular RAG의 기본 개념을 이해한다.\n",
            "Naive RAG의 구조와 작동 방식을 조사한다.\n",
            "Modular RAG의 구조와 작동 방식을 조사한다.\n",
            "Naive RAG와 Modular RAG의 구조적 차이점을 비교한다.\n",
            "Naive RAG와 Modular RAG의 작동 방식 차이점을 비교한다.\n",
            "Modular RAG가 production level에서 가지는 이점을 조사한다.\n",
            "Naive RAG와 비교하여 Modular RAG의 이점을 정리한다.\n",
            "Modular RAG의 이점을 production level에서의 사용 사례와 연결하여 설명한다.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Naive RAG와 Modular RAG의 기본 개념을 이해하기 위해 각각의 정의와 특징을 살펴보겠습니다.\n",
            "\n",
            "1. **Naive RAG (Retrieval-Augmented Generation)**:\n",
            "   - Naive RAG는 정보 검색과 생성 모델을 결합한 접근 방식입니다.\n",
            "   - 주로 대량의 비구조화된 데이터에서 정보를 검색하고, 이를 기반으로 자연어 생성 모델이 응답을 생성하는 방식입니다.\n",
            "   - Naive RAG는 검색 단계와 생성 단계를 단순히 결합한 형태로, 구조가 비교적 단순합니다.\n",
            "\n",
            "2. **Modular RAG**:\n",
            "   - Modular RAG는 Naive RAG의 구조를 개선하여 모듈화된 형태로 발전시킨 모델입니다.\n",
            "   - 각 모듈이 독립적으로 최적화될 수 있도록 설계되어 있으며, 검색 모듈과 생성 모듈이 명확히 분리되어 있습니다.\n",
            "   - 이러한 모듈화는 시스템의 유연성을 높이고, 특정 모듈을 교체하거나 업그레이드하기 쉽게 만듭니다.\n",
            "\n",
            "이 두 가지 개념을 이해하면, Naive RAG와 Modular RAG의 구조적 및 작동 방식의 차이점을 더 깊이 있게 분석할 수 있습니다. Naive RAG는 단순한 통합 방식인 반면, Modular RAG는 각 기능을 모듈화하여 더 나은 성능과 유연성을 제공합니다.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mexecute\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "('Naive RAG와 Modular RAG의 기본 개념을 이해한다.', 'Naive RAG와 Modular RAG의 기본 개념을 이해하기 위해 각각의 정의와 특징을 살펴보겠습니다.\\n\\n1. **Naive RAG (Retrieval-Augmented Generation)**:\\n   - Naive RAG는 정보 검색과 생성 모델을 결합한 접근 방식입니다.\\n   - 주로 대량의 비구조화된 데이터에서 정보를 검색하고, 이를 기반으로 자연어 생성 모델이 응답을 생성하는 방식입니다.\\n   - Naive RAG는 검색 단계와 생성 단계를 단순히 결합한 형태로, 구조가 비교적 단순합니다.\\n\\n2. **Modular RAG**:\\n   - Modular RAG는 Naive RAG의 구조를 개선하여 모듈화된 형태로 발전시킨 모델입니다.\\n   - 각 모듈이 독립적으로 최적화될 수 있도록 설계되어 있으며, 검색 모듈과 생성 모듈이 명확히 분리되어 있습니다.\\n   - 이러한 모듈화는 시스템의 유연성을 높이고, 특정 모듈을 교체하거나 업그레이드하기 쉽게 만듭니다.\\n\\n이 두 가지 개념을 이해하면, Naive RAG와 Modular RAG의 구조적 및 작동 방식의 차이점을 더 깊이 있게 분석할 수 있습니다. Naive RAG는 단순한 통합 방식인 반면, Modular RAG는 각 기능을 모듈화하여 더 나은 성능과 유연성을 제공합니다.')\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mreplan\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "Naive RAG의 구조와 작동 방식을 조사한다.\n",
            "Modular RAG의 구조와 작동 방식을 조사한다.\n",
            "Naive RAG와 Modular RAG의 구조적 차이점을 비교한다.\n",
            "Naive RAG와 Modular RAG의 작동 방식 차이점을 비교한다.\n",
            "Modular RAG가 production level에서 가지는 이점을 조사한다.\n",
            "Naive RAG와 비교하여 Modular RAG의 이점을 정리한다.\n",
            "Modular RAG의 이점을 production level에서의 사용 사례와 연결하여 설명한다.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_web_search (call_dFj2N9Jj1Rmee2JJ9UAw5jqF)\n",
            " Call ID: call_dFj2N9Jj1Rmee2JJ9UAw5jqF\n",
            "  Args:\n",
            "    query: Naive RAG 구조와 작동 방식\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mtools\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_web_search\n",
            "\n",
            "[{\"url\": \"https://blog.naver.com/simula/223921547177?fromRss=true&trackingCode=rss\", \"title\": \"그래프 RAG : 네이버 블로그\", \"content\": \"가장 기본적인 형태인 'Naive RAG'의 작동 방식은 '검색 후 읽기(Retrieve-Read)' 또는 '검색 후 생성(Retrieve-and-Generate)' 프레임워크로 요약될 수\", \"score\": 0.8795718, \"raw_content\": null}, {\"url\": \"https://velog.io/@yooni1231__/ChatGPT%EB%B3%B4%EB%8B%A4-%EB%98%91%EB%98%91%ED%95%9C-RAG-%EB%A7%8C%EB%93%A4%EA%B8%B0-Advanced-RAG-%EA%B0%9C%EB%85%90%EB%B6%80%ED%84%B0-%EA%B8%B0%EB%B2%95%EA%B9%8C%EC%A7%80-%EC%99%84%EC%A0%84-%EC%A0%95%EB%A6%AC\", \"title\": \"ChatGPT보다 똑똑한 RAG 만들기?! Advanced RAG 개념부터 기법 ...\", \"content\": \"Naive RAG는 이렇게 작동해요. 기본적인 RAG는 다음과 같은 구조로 되어 있어요: 문서를 청크(chunk) 단위로 잘라서 벡터로 변환. 사용자의\", \"score\": 0.8682137, \"raw_content\": \"# 🤖 ChatGPT보다 똑똑한 RAG 만들기?! Advanced RAG 개념부터 기법까지 정리\\n\\n## [CapstoneProject](/@yooni1231__/series/CapstoneProject)\\n\\n📌 \\\"검색만 잘해도 반은 먹고 들어간다\\\" — RAG에도 완벽히 적용되는 말인듯 싶습니다.\\n\\n## 🧑‍💻 Intro: RAG가 뭐예요?\\n\\n**RAG (Retrieval-Augmented Generation)**는  \\nLLM(예: GPT-4, Claude, Mistral 등)에게 검색을 붙여주는 방식이에요.\\n\\n처음에 RAG를 접했을 땐  \\n“그냥 문서 검색해서 LLM에 넣는 거 아닌가?” 싶었는데…  \\n막상 공부해보니까 진짜 달라요.\\n\\n고급 RAG는 단순 검색 + 생성이 아니에요.  \\n파이프라인 전반을 최적화해서  \\n검색 정확도, 문맥 이해도, 응답의 신뢰도까지 다 높여주는 구조거든요.\\n\\n이번 글에서는 제가 공부하면서 정리한  \\n\\\"Advanced RAG가 뭔지\\\", \\\"왜 필요한지\\\", \\\"어떤 기술들이 쓰이는지\\\"  \\n하나하나 정리해서 공유해볼게요!\\n\\n## 📚 RAG가 뭔데요?\\n\\nRAG는 “Retrieval-Augmented Generation”의 줄임말로,  \\n쉽게 말하면 LLM에게 ‘검색 기능’을 붙여주는 구조입니다.\\n\\n예를 들어 GPT-3.5 같은 LLM은  \\n2023년 이전까지의 지식만 학습돼 있어서  \\n그 이후 생긴 정보는 모르고 있어요.\\n\\n예를 들어,\\n\\n\\\"2024년 총선에서 어떤 정책이 새로 나왔어요?\\\"  \\n\\\"GPT-4.5에서 추가된 기능이 뭐예요?\\\"\\n\\n이런 질문은 기본 GPT로는 답을 못 해요.\\n\\n이럴 때 필요한 게 바로 RAG예요.  \\n외부 문서, PDF, 웹 검색 결과 등을 LLM에 함께 넣어서  \\n지금 이 순간의 정보까지 반영한 응답을 생성할 수 있게 해주는 기술입니다.\\n\\n## 📦 Naive RAG는 이렇게 작동해요\\n\\n기본적인 RAG는 다음과 같은 구조로 되어 있어요:\\n\\n📄 문서를 청크(chunk) 단위로 잘라서 벡터로 변환\\n\\n🔍 사용자의 질문도 벡터로 만들어서, 문서 벡터들과 비교해 Top-K 검색\\n\\n🧠 검색된 청크를 LLM에게 함께 보내서 응답 생성\\n\\n이게 기본 구조인데, 현실에서는 여러 문제가 발생합니다...\\n\\n## ⚠️ Naive RAG의 한계\\n\\n항목 문제점  \\n인덱싱 PDF나 보고서 같은 문서를 제대로 파싱 못 해서 정보가 유실됨  \\n검색 중복된 내용만 나오거나, 중요한 내용을 못 찾는 경우 많음  \\n생성 LLM이 잘못된 문맥을 받아서 오답을 내거나 편향된 정보 생성  \\n특히 검색이 부정확하면,  \\n아무리 좋은 LLM을 써도 헛다리 짚은 답변이 나오게 되더라고요 😭\\n\\n## 🌟 그래서 등장한 Advanced RAG\\n\\nAdvanced RAG는 Naive RAG의 단점을 보완한 구조예요.  \\n전체 과정을 다음 4단계로 나누고, 각 단계마다 최적화 기법을 적용합니다.\\n\\nAdvanced RAG의 4단계 구조  \\nPre-Retrieval: 검색 전 준비 (문서 파싱, 쿼리 정리 등)\\n\\nRetrieval: 검색 최적화 (하이브리드, 임베딩 튜닝 등)\\n\\nPost-Retrieval: 결과 압축, 재정렬, 필터링\\n\\nGeneration: 답변 생성 최적화 (다중 생성, 요약 등)\\n\\n## 🔍 1단계: Pre-Retrieval\\n\\n🧾 PDF 파싱  \\nPDF는 단순한 텍스트가 아니라  \\n**\\\"어디에 어떻게 출력할지를 지시하는 명령어 덩어리\\\"**에 가깝습니다.\\n\\n그래서 그냥 텍스트로 추출하면  \\n문단, 표, 그림이 다 깨지거나 줄바꿈이 엉망이 되죠.\\n\\n## 💡 해결 방법:\\n\\npypdf: 룰 기반 파서, 간단하지만 구조 인식 어려움\\n\\nUnstructured, LayoutParser: 딥러닝 기반, 표/문단 인식 가능\\n\\nPP-StructureV2: 문서 안의 핵심 정보까지 추출\\n\\n## ✍️ 쿼리 재작성\\n\\n사용자 질문이 애매하거나 여러 주제를 섞어서 묻는 경우,  \\n그대로 검색하면 정확한 결과를 못 찾는 경우가 많아요.\\n\\n## 💡 해결 방법:\\n\\nStep-Back Prompting: 질문을 더 일반화해서 검색\\n\\nHyDE: 질문으로 가상 문서를 생성 → 임베딩 후 검색\\n\\nQuery2Doc: LLM이 문서처럼 재작성해서 검색 효율 ↑\\n\\nITER-RETGEN: 생성과 검색을 반복해서 더 정확한 정보 획득\\n\\n🔗 문맥 확장  \\n하나의 문장만 검색되면 이해하기 어렵죠.  \\n그래서 앞뒤 문장을 함께 가져오거나, 상위 문서 단위로 묶어서 전달해요.\\n\\nSentence Window Retrieval: 앞뒤 k개 문장 함께 전송\\n\\nParent Chunking: 청크들을 상위 의미 단위로 묶기\\n\\n## 🔎 2단계: Retrieval (검색 최적화)\\n\\n🧬 하이브리드 검색  \\n키워드 검색 (BM25 등) → 정확한 단어 일치\\n\\n의미 기반 검색 (벡터 임베딩) → 유사한 문맥 이해\\n\\n💡 두 가지를 **RRF (Reciprocal Rank Fusion)**으로 조합해서  \\n정확도와 커버리지를 동시에 높입니다.\\n\\n## ✂️ 3단계: Post-Retrieval (결과 정리)\\n\\n🔄 리랭킹 (Re-Ranking)  \\n검색된 Top-K 청크 중에서도  \\n진짜 중요한 걸 위로 올려주는 과정입니다.\\n\\nbge-reranker, Cohere API\\n\\nRankGPT: LLM 기반으로 슬라이딩 윈도우 방식 리랭킹\\n\\n## 📉 프롬프트 압축 (Prompt Compression)\\n\\nLLM에게 넣을 수 있는 토큰 수는 한계가 있어요.  \\n(예: GPT-3.5는 약 4,000 tokens)\\n\\n## 💡 해결 방법:\\n\\nSelective Context: 정보량 많은 부분만 남김\\n\\nLLMLingua, LongLLMLingua: 의미 단위로 압축\\n\\nAutoCompressor: soft prompt로 요약 정보 전달\\n\\nRECOMP: 문장 단위로 압축 or 요약 생성\\n\\n🧼 필터링  \\n필요 없는 문서, 중복된 내용, 신뢰도 낮은 정보는 제거합니다.\\n\\nFiD-TF, Self-RAG, CRAG 등은  \\n토큰 수준에서 중요성 판단 후 필터링까지 수행해요!\\n\\n## 🧠 4단계: 생성 (Generation)\\n\\n다양한 생성 기술들  \\nDSP: 여러 쿼리 → 여러 문서 검색 → 종합해서 응답 생성\\n\\nPRCA: 보상 기반 학습으로 더 나은 응답 유도\\n\\nREPLUG: 검색 결과를 그대로 LLM 입력에 추가\\n\\nRECITE: 다양한 답변 생성 후 다수결로 최종 응답 선택\\n\\n## 🧑‍💻 기타 고급 구성\\n\\n🗨️ Chat Engine  \\n대화형에서는 이전 대화 내용을 자동 요약해서 쿼리에 반영해야 해요.  \\n예: ContextChatEngine, CondensePlusContextMode\\n\\n## 🤖 Agent 구조\\n\\n문서마다 요약/질의응답 전담 Agent를 만들고,  \\nTop Agent가 전체 통제 → 질문을 각 Agent에 전달하고 응답을 종합!\\n\\n## 🔧 모델 튜닝\\n\\nGPT-4로 질문-응답 데이터 만들고 → GPT-3.5에 파인튜닝\\n\\nRA-DIT: Retriever + Generator를 동시에 학습시켜 성능 ↑\\n\\n## ✅ 평가도 중요해요!\\n\\n잘 작동하는 RAG인지 어떻게 알 수 있을까요?\\n\\nRagas: 정답성, 검색 정확도, 응답 적절성 측정\\n\\nLangSmith: 체인 내 행동 추적\\n\\nOpenAI Cookbook: 평가 기준별 테스트 스크립트 제공\\n\\n참고자료: [ https://medium.com/@vipra\\\\_singh/building-llm-applications-advanced-rag-part-10-ec0fe735aeb1 ]\\n\\n![profile](https://velog.velcdn.com/images/yooni1231__/profile/e5e569d8-0f8a-49b3-8cfa-d9b4ef9e892f/social_profile.png)\\n\\n### <Ink&Codes>Bringing Museums to Life: Starting Our Journey with LLM, RAG, and Smart Glasses\\n\\n### Search Smarter, Generate Better: The Power of Advanced RAG\\n\\n#### 0개의 댓글\"}, {\"url\": \"https://mezzaninex.tistory.com/6673\", \"title\": \"Naive RAG, Advanced RAG, Modular RAG 250821 - MezzanineX\", \"content\": \"... Naive RAG는 가장 기본적인 RAG 모델로, 사용자의 질의에 대해 단일 검색기와 단일 생성기를 활용합니다.작동 방식 * 검색 (Retrieval): 사용자의\", \"score\": 0.84918517, \"raw_content\": \"# [MezzanineX](https://mezzaninex.tistory.com/)\\n\\n![N](https://tistory1.daumcdn.net/tistory_admin/blogs/image/category/new_ico_5.gif)\\n\\n## [Naive RAG, Advanced RAG, Modular RAG 250821](/6673)\\n\\nNaive RAG (**Retrieval-Augmented Generation**)  \\n**Naive RAG는 가장 기본적인 RAG 모델**로, **사용자의 질의에 대해 단일 검색기와 단일 생성기를 활용**합니다.  \\n작동 방식  \\n \\\\* **검색 (Retrieval): 사용자의 질의가 주어지면, 검색기는 미리 구축된 지식 데이터베이스**에서 \\\\*\\\\***가장 관련성이 높은 문서 조각(chunk)\\\\*\\\\*을 찾아냅**니다. **이때 주로 임베딩(embedding) 기술을 사용하여 질의와 문서 조각 간의 유사도를 측정**합니다.  \\n \\\\* **생성 (Generation)**: **검색된 문서 조각과 사용자의 원래 질의가 함께 LLM(Large Language Model)의 입력으로 들어갑**니다. **LLM은 이 정보를 바탕으로 답변을 생성**합니다.  \\n특징  \\n \\\\* **단순성: 구현이 간단하고 직관적**입니다.  \\n \\\\* **한계: 검색된 문서가 충분히 관련성이 높지 않거나, 여러 문서에 걸쳐 복잡한 추론이 필요한 경우 답변의 품질이 낮아질 수** 있습니다.  \\n  \\n**Advanced RAG**  \\nAdvanced RAG는 Naive RAG의 한계를 극복하기 위해 **다양한 최적화 기술을 도입한 모델**입니다. **검색 및 생성 단계의 품질을 향상시키는 데 중점**을 둡니다.  \\n주요 기술  \\n \\\\* **전처리 (Pre-processing):**  \\n   \\\\* **청크 최적화: 문서를 단순히 고정된 크기로 자르는 대신, 문맥에 맞게 의미 있는 단위로 분할**합니다.  \\n   \\\\* **메타데이터 추가: 문서 조각에 제목, 날짜, 출처 등 부가 정보를 추가하여 검색 정확도를 높입**니다.  \\n \\\\* **검색(Retrieval) 향상:**  \\n   \\\\* **하이브리드 검색: 키워드 검색(BM25)과 벡터 검색을 동시에 사용하여 상호 보완**합니다.  \\n   \\\\* **재순위화 (Re-ranking): 1차 검색으로 나온 결과물들을 더 정교한 모델을 사용해 다시 순위를 매깁**니다. 이를 통해 질의와 가장 관련성 높은 문서를 최종적으로 선택합니다.  \\n   \\\\* **쿼리 변환**: **사용자의 질의를 여러 개의 하위 질문으로 나누거나, 질의를 더 명확하게 바꾸어 검색 효율을 높입**니다.  \\n \\\\* **생성(Generation) 향상:**  \\n   \\\\* **응답 압축: 검색된 문서 중 핵심적인 정보만 추출하여 LLM에 전달**합니다.  \\n   \\\\* **응답 정제: LLM이 생성한 답변을 다시 검토하고 보강하는 과정을 추가할 수** 있습니다.  \\n특징  \\n \\\\* **고도화: Naive RAG보다 복잡하지만, 답변의 정확도와 품질이 훨씬 뛰어납**니다.  \\n \\\\* **유연성: 다양한 기술을 조합하여 특정 애플리케이션에 맞게 최적화할 수 있습**니다.  \\n  \\n**Modular RAG**  \\n**Modular RAG는 RAG의 각 구성 요소를 독립적인 모듈로 분리**하고, **이를 동적으로 조합하여 복잡한 질의에 대응하는 모델**입니다. 마치 **레고 블록처럼 필요한 모듈을 그때그때 연결해 사용하는 방식**입니다.  \\n작동 방식  \\n \\\\* **모듈화: 검색, 재순위화, 요약, 추론 등 다양한 기능을 수행하는 모듈들을 미리 준비**합니다.  \\n \\\\* **동적 파이프라인**: **사용자의 질의를 분석하는 라우터 모듈이 가장 먼저 작동**합니다. **이 라우터는 질의의 성격에 따라 어떤 모듈들을 어떤 순서로 실행할지 결정**합니다.  \\n   \\\\* **단순 사실 질문: \\\"가장 높은 산은?\\\" → 검색 → 생성 모듈만 사용.**  \\n   \\\\* **복합 추론 질문: \\\"A 제품과 B 제품 중 어느 것이 더 효율적인가?\\\" → 검색(A) → 검색(B) → 비교/추론 → 생성 모듈 사용.**  \\n   \\\\* **요약 질문: \\\"이 문서의 핵심 내용은?\\\" → 요약 모듈 사용.**  \\n특징  \\n \\\\* **적응성: 질문의 유형에 따라 최적의 워크플로우를 구성하여 효율성을 극대화**합니다.  \\n \\\\* **유연성: 새로운 모듈을 쉽게 추가하거나 교체할 수 있어, 시스템 확장이 용이**합니다.  \\n \\\\* **복잡성: 구조가 복잡해지므로, 각 모듈 간의 상호작용을 정교하게 설계해야** 합니다.\\n\\n#### [+ Recent posts](/category)\\n\\n![post image]()\\n![post image]()\\n![post image]()\\n![post image]()\\n\\n## 티스토리툴바\"}]\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Naive RAG의 구조와 작동 방식에 대해 조사한 결과는 다음과 같습니다:\n",
            "\n",
            "1. **구조**:\n",
            "   - Naive RAG는 가장 기본적인 RAG 모델로, 사용자의 질의에 대해 단일 검색기와 단일 생성기를 활용합니다.\n",
            "   - 문서를 청크(chunk) 단위로 잘라서 벡터로 변환하고, 사용자의 질문도 벡터로 만들어 문서 벡터들과 비교하여 Top-K 검색을 수행합니다.\n",
            "   - 검색된 청크를 LLM(Large Language Model)에게 함께 보내서 응답을 생성합니다.\n",
            "\n",
            "2. **작동 방식**:\n",
            "   - **검색 (Retrieval)**: 사용자의 질의가 주어지면, 검색기는 미리 구축된 지식 데이터베이스에서 가장 관련성이 높은 문서 조각(chunk)을 찾아냅니다. 이때 주로 임베딩(embedding) 기술을 사용하여 질의와 문서 조각 간의 유사도를 측정합니다.\n",
            "   - **생성 (Generation)**: 검색된 문서 조각과 사용자의 원래 질의가 함께 LLM의 입력으로 들어가며, LLM은 이 정보를 바탕으로 답변을 생성합니다.\n",
            "\n",
            "3. **특징**:\n",
            "   - 단순성: 구현이 간단하고 직관적입니다.\n",
            "   - 한계: 검색된 문서가 충분히 관련성이 높지 않거나, 여러 문서에 걸쳐 복잡한 추론이 필요한 경우 답변의 품질이 낮아질 수 있습니다.\n",
            "\n",
            "이러한 Naive RAG의 구조와 작동 방식은 기본적인 RAG의 형태로, 복잡한 문제 해결에는 한계가 있을 수 있습니다.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mexecute\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "('Naive RAG의 구조와 작동 방식을 조사한다.', 'Naive RAG의 구조와 작동 방식에 대해 조사한 결과는 다음과 같습니다:\\n\\n1. **구조**:\\n   - Naive RAG는 가장 기본적인 RAG 모델로, 사용자의 질의에 대해 단일 검색기와 단일 생성기를 활용합니다.\\n   - 문서를 청크(chunk) 단위로 잘라서 벡터로 변환하고, 사용자의 질문도 벡터로 만들어 문서 벡터들과 비교하여 Top-K 검색을 수행합니다.\\n   - 검색된 청크를 LLM(Large Language Model)에게 함께 보내서 응답을 생성합니다.\\n\\n2. **작동 방식**:\\n   - **검색 (Retrieval)**: 사용자의 질의가 주어지면, 검색기는 미리 구축된 지식 데이터베이스에서 가장 관련성이 높은 문서 조각(chunk)을 찾아냅니다. 이때 주로 임베딩(embedding) 기술을 사용하여 질의와 문서 조각 간의 유사도를 측정합니다.\\n   - **생성 (Generation)**: 검색된 문서 조각과 사용자의 원래 질의가 함께 LLM의 입력으로 들어가며, LLM은 이 정보를 바탕으로 답변을 생성합니다.\\n\\n3. **특징**:\\n   - 단순성: 구현이 간단하고 직관적입니다.\\n   - 한계: 검색된 문서가 충분히 관련성이 높지 않거나, 여러 문서에 걸쳐 복잡한 추론이 필요한 경우 답변의 품질이 낮아질 수 있습니다.\\n\\n이러한 Naive RAG의 구조와 작동 방식은 기본적인 RAG의 형태로, 복잡한 문제 해결에는 한계가 있을 수 있습니다.')\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mreplan\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "Modular RAG의 구조와 작동 방식을 조사한다.\n",
            "Naive RAG와 Modular RAG의 구조적 차이점을 비교한다.\n",
            "Naive RAG와 Modular RAG의 작동 방식 차이점을 비교한다.\n",
            "Modular RAG가 production level에서 가지는 이점을 조사한다.\n",
            "Naive RAG와 비교하여 Modular RAG의 이점을 정리한다.\n",
            "Modular RAG의 이점을 production level에서의 사용 사례와 연결하여 설명한다.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_web_search (call_4u4yHw1GXH7mZogEPEWeQeeb)\n",
            " Call ID: call_4u4yHw1GXH7mZogEPEWeQeeb\n",
            "  Args:\n",
            "    query: Modular RAG 구조와 작동 방식\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mtools\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_web_search\n",
            "\n",
            "[{\"url\": \"https://brunch.co.kr/@@aPda/338\", \"title\": \"12화 복잡한 RAG 분해하기-Modular RAG란?\", \"content\": \"모듈러 RAG는 복잡한 RAG 시스템을 마치 레고 블록처럼 여러 개의 독립적인 모듈로 나누어, 필요에 따라 각 모듈을 교체하거나 조합할 수 있게 만들어 줍니다. **모듈러 RAG는 각 모듈을 독립적으로 설계하고 이를 필요에 따라 교체하거나 결합할 수 있도록 합니다.** 예를 들어, 새로운 임베딩 모델이 등장하거나 기존의 벡터 DB가 성능을 더 높일 수 있다고 판단되면, 해당 모듈만 교체하는 방식으로 시스템을 최적화할 수 있습니다. **모듈러 RAG는 또한 동적이고 적응 가능한 시스템을 가능하게 만듭니다.** 예를 들어, 특정 작업이나 데이터 소스에 맞는 모듈을 선택하고, 이를 조합함으로써 효율적인 RAG 시스템을 구성할 수 있습니다. **또한, 모듈러 RAG는 시스템의 복잡도를 낮추고, 성능을 더욱 향상할 수 있는 가능성을 제공합니다.** 예를 들어, 특정 데이터 소스에 더 적합한 벡터 DB를 찾거나, 최신의 임베딩 모델을 도입하는 등의 작업을 통해 성능을 지속적으로 개선할 수 있습니다.\", \"score\": 0.79022324, \"raw_content\": \"         \\n\\n[brunch](/)\\n\\n연재 중 [알바트로스의 생성형 AI 연구소 12화](/brunchbook/genainote)\\n\\n# 복잡한 RAG 분해하기-Modular RAG란?\\n\\nRAG의 성능을 개선하는 비법, 모듈화\\n\\nby [알바트로스](/@harryban0917)  Nov 30. 2024\\n\\n지금까지 검색증강생성(RAG, Retrieval Augmented Generation)을 구성하는 핵심적인 요소들에 대해 배워보았습니다. RAG 파이프라인을 구성하는 요소인 **임베딩 모델과 벡터 DB 그리고 최종적인 LLM 답변의 전체적인 톤 앤 매너와 제약사항 등을 통해 답변을 제어하는 시스템 프롬프트(System Prompt)**까지 검색증강생성 파이프라인에는 정말 다양한 요소들이 있었습니다.\\n\\n여태껏 배운 것들을 정리해 봅시다. [생성형 AI도 구글링을 한다?](https://brunch.co.kr/@harryban0917/325) 에서 살펴보았던 것처럼 결국 RAG는 LLM이 양질의 정보를 '검색'하기 위한 일종의 파이프라인입니다. LLM에게 검색하게 하고자 하는 텍스트 데이터를 질서 정연하게 인덱싱(Indexing) 및 청킹(Chunking) 해준 뒤에 '임베딩 모델'에 제공해 주면 Token ID와 함께 숫자로 된 벡터들을 얻을 수 있습니다. 그 벡터들을 chromaDB나 Elasic Search와 같은 여러 벡터 DB에 저장해 두었다가 LLM이 프롬프트와 함께 관련된 질문을 받으면 꺼내다 쓸 수 있도록 해주는 것이지요.\\n\\n출처 : Freepiks\\n\\n이때 RAG 파이프라인의 각 요소들은 하나의 시스템으로 유기적으로 작용하는 것처럼 보이지만 각각의 요소를 떼어놓고 살펴보면 마치 레고 블록처럼 분리하고 갈아 끼울 수도 있습니다. 이번에 새로 나온 어느 '임베딩 모델'이 좋다고 하면 해당 부분만 갈아 끼울 수도 있고, 우리 회사의 데이터에 더 적합한 '벡터 DB'를 발견하면 마찬가지로 그 부분만 교체할 수도 있지요.\\n\\n이러한 블록 작업들을 용이하게 하고, 보다 뛰어난 성능을 구현해 내는 방법으로 제안된 모듈러 RAG라는 개념이 있습니다. 이번 시간에는 **Modular RAG: A Reconfigurable Framework for Retrieval-Augmented Generation**라는 논문을 리뷰하며 모듈러 RAG의 개념에 대해 소개호도록 하겠습니다.\\n\\n### **1. 복잡한 RAG 시스템 분해하기 : 모듈러 레그(Modular RAG)란?**\\n\\n지난 몇 주에 걸쳐 RAG 파이프라인을 구성하는 임베딩 모델, 벡터 DB, 그리고 LLM의 최종 답변을 제어하는 시스템 프롬프트까지, 이 모든 요소들이 어떻게 유기적으로 작동하는지에 대해 살펴봤습니다. 그런데 이러한 요소들이 각각 독립적으로 존재하면서도 쉽게 교체될 수 있다는 사실을 알고 계셨나요?\\n\\n이번에 소개할 '모듈러 RAG'는 바로 이러한 교체 가능성을 극대화하고, 성능을 더욱 향상할 수 있는 새로운 접근법입니다. 모듈러 RAG는 복잡한 RAG 시스템을 마치 레고 블록처럼 여러 개의 독립적인 모듈로 나누어, 필요에 따라 각 모듈을 교체하거나 조합할 수 있게 만들어 줍니다. 이로 인해 시스템의 유연성은 물론, 확장성까지 한층 강화됩니다.\\n\\n### **2. 모듈러 RAG의 개념과 구성 요소**\\n\\n### \\n\\n모듈러 RAG는 RAG 시스템을 여러 개의 독립적인 모듈로 나누어 구성하는 방식을 제안합니다. 이를 통해 각 모듈은 다른 모듈과 독립적으로 동작하면서도, 서로 필요한 정보를 교환하며 전체 시스템을 구성할 수 있습니다. 여기서 핵심은 '모듈화'입니다. 각 모듈은 특정한 역할을 담당하며, 해당 모듈을 다른 모듈로 교체하거나 조합함으로써 다양한 작업을 처리할 수 있습니다.\\n\\n기존 RAG(Naive RAG) vs Advanced RAG vs 모듈러 RAG (출처 : Modular RAG: A Reconfigurable Framework...)\\n\\n모듈러 RAG는 크게 세 가지 주요 수준으로 나누어집니다. **첫 번째는 모듈로,** RAG 시스템의 핵심 프로세스를 담당합니다. 예를 들어, 데이터 인덱싱, 검색 전후 처리, 생성 과정 등이 모듈에 해당합니다. **두 번째는 하위 모듈로,** 각 모듈 내의 세부 기능을 세분화하고 최적화하는 역할을 합니다. 예를 들어, 쿼리 확장이나 검색기 선택 등이 하위 모듈에 해당합니다. **세 번째는 연산자**로, 하위 모듈 내에서 실제 기능을 수행하는 기본 단위입니다. 예를 들어, 슬라이딩 윈도나 메타데이터 첨부 같은 기능이 연산자에 해당합니다.\\n\\n### **3. 모듈러 RAG의 작동 원리와 장점**\\n\\n### \\n\\n**모듈러 RAG는 각 모듈을 독립적으로 설계하고 이를 필요에 따라 교체하거나 결합할 수 있도록 합니다.** 예를 들어, 새로운 임베딩 모델이 등장하거나 기존의 벡터 DB가 성능을 더 높일 수 있다고 판단되면, 해당 모듈만 교체하는 방식으로 시스템을 최적화할 수 있습니다. 이렇게 모듈화 된 시스템은 단순히 기존의 시스템을 확장하거나 개선하는 것에 그치지 않고, 다양한 시나리오에 맞게 빠르게 적응할 수 있습니다.\\n\\n**모듈러 RAG는 또한 동적이고 적응 가능한 시스템을 가능하게 만듭니다.** 예를 들어, 특정 작업이나 데이터 소스에 맞는 모듈을 선택하고, 이를 조합함으로써 효율적인 RAG 시스템을 구성할 수 있습니다. 또한, 모듈러 RAG는 각 모듈이 독립적으로 유지보수되고 개선될 수 있기 때문에, 시스템의 유지 관리가 용이하고, 새로운 기술이나 방법론을 도입하는 데에도 유리하지요.\\n\\n**모듈러 RAG의 가장 큰 장점은 바로 유연성과 확장성입니다.** 특정 작업에 맞는 모듈을 자유롭게 선택하고 교체함으로써 시스템을 최적화할 수 있고, 새로운 기술이나 방법론을 빠르게 적용할 수 있습니다. 또한, 각 모듈은 독립적으로 유지 관리되기 때문에, 시스템 전체의 안정성을 높이는 데에도 유리합니다.\\n\\n**또한, 모듈러 RAG는 시스템의 복잡도를 낮추고, 성능을 더욱 향상할 수 있는 가능성을 제공합니다.** 예를 들어, 특정 데이터 소스에 더 적합한 벡터 DB를 찾거나, 최신의 임베딩 모델을 도입하는 등의 작업을 통해 성능을 지속적으로 개선할 수 있습니다. 이는 빠르게 변화하는 기술 환경에 적응하는 데 큰 이점이 됩니다.\\n\\n모듈러 RAG는 기존의 RAG 시스템을 보다 유연하고 확장 가능한 형태로 발전시킨 혁신적인 접근법입니다. 이를 통해 각 요소를 독립적으로 조정하고 최적화할 수 있어, 다양한 작업 시나리오에 맞는 맞춤형 시스템을 구축할 수 있습니다. 모듈러 RAG는 계속해서 발전하는 AI 환경에서 더 나은 성능과 효율성을 제공합니다.\\n\\n### **4. 모듈러 RAG를 통해 바라본 생성형 AI 발전 트렌드**\\n\\n이처럼 생성형 AI 기술의 최신 트렌드는 수직적인 시스템 구조에서 탈피해 극도의 '유연성'을 추구하는 방향으로 나아가고 있습니다. 생성형 AI는 수개월 주기로 워낙 기술이 빠르게 바뀌고 있는 분야이기 때문에, 각 요소를 개별적으로 조정하고 교체할 수 있는 유연한 시스템은 필수적이기 때문이죠.\\n\\n예를 들어 RAG의 각 구성 요소를 활용할 수 있는 대표적인 라이브러리인 랭체인(Langchain)과 랭그래프(Langgraph)를 보아도 알 수 있습니다. 다음 시간에는 랭체인과 랭그레프의 기초 개념과 왜 랭그래프가 최근 랭체인보다 더 각광받고 있는지 그 이유에 대해 생각해 볼 수 있는 시간을 가져보도록 하겠습니다.\\n\\n###\\n\\n**keyword**\\n\\n* [AI](/keyword/AI)\\n* [인공지능](/keyword/인공지능)\\n* [챗GPT](/keyword/챗GPT)\\n\\n**Brunch Book**   *토요일*\\n연재\\n\\n[연재 알바트로스의 생성형 AI 연구소](/brunchbook/genainote)\\n\\n* [10   벡터 DB는 기존의 데이터베이스와 어떻게 다를까?](/@@aPda/336)\\n* [11   대표적인 벡터 DB에는 어떤 것들이 있을까?](/@@aPda/337)\\n* [12   복잡한 RAG 분해하기-Modular RAG란?](/@@aPda/338)\\n* [13   LangChain과 LangGgraph 어떻게 다를까?](/@@aPda/339)\\n* [14   AI 에이전트에 날개를 달아준 LLM](/@@aPda/340)\\n\\n[전체 목차 보기](/brunchbook/genainote)\\n\\n  \\n\\n**댓글0**\\n\\n**작성된 댓글이 없습니다.** 작가에게 첫 번째 댓글을 남겨주세요!\\n\\n \\n\\n**[알바트로스](/@harryban0917)**  IT 분야 크리에이터  [*직업* 에세이스트](/@harryban0917)\\n\\n[문과 출신으로 생성형 AI 엔지니어로 일하고 있습니다. 챗GPT와 같은 AI가 보다 더 사람의 말을 잘 알아듣고 생성할 수 있도록 연구하는 NLP(자연어처리) 분야에서 일합니다.](/@harryban0917)\\n\\n[구독자 547](/@harryban0917)\\n\\n[**오늘만 무료**\\n\\n남은 시간\\n:   06:33:45\\n\\nbrunch membership\\n\\n당신이 향수를 고르지 못하는 이유\\n\\nby 권도연](/@@5NQs/365)\\n\\n[이전 11화**대표적인 벡터 DB에는 어떤 것들이 있을까?**](/@harryban0917/337)[**LangChain과 LangGgraph 어떻게 다를까?**다음 13화](/@harryban0917/339)\\n\\n              \\n\\nYou can make anything  \\nby writing\\n\\n   \"}, {\"url\": \"https://g3lu.tistory.com/42\", \"title\": \"RAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG)\", \"content\": \"## RAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG) 본문 Retrieval-Augmented Geneartion(RAG)는 외부 지식 소스로부터 추가적인 정보를 통합하여 대형 언어 모델(LLM)을 개선하는 과정이다. 이는 중요한 정보를 놓칠 수 있게 된다. * **잘못된 정보 제공** : 유용한 정보를 제공하지 않고 검색된 내용을 단순히 반복하는 결과를 초래할 수 있으며, 일관성 없는 답변을 뱉는 경우가 발생한다. Advanced RAG는 Naive RAG 방식에서 직면하고 있는 문제를 해결하기 위해 고안되었다. 위의 세 가지 고찰을 보완하기 위해 **Pre-Retreival 및 Post-Retrieval**를 기존 RAG 아키텍처에 추가한 것이 Advanced RAG이다. 하지만 검색된 청크들이 간혹 중복이 되거나 의미 없는 정보를 담는 경우 발생하게 되는데, 이는 LLM이 주어진 컨텍스트를 처리하는 방식에 영향을 미칠 수 있다. * **Prompt Compression** : 검색된 정보에 Noisy가 많을 수 있으므로, LLM에 태우기전에 관련 없는 정보를 압축하고 길이를 줄이는 것도 중요하다.\", \"score\": 0.6651719, \"raw_content\": \"[본문 바로가기](#dkBody)\\n\\n# Attention, Please!!!\\n\\n[Attention, Please!!!](/)\\n\\n\\nG3LU\\n\\n---\\n\\n## Attention, Please!!!\\n\\n## RAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG) 본문\\n\\n**[LLM/RAG](/category/LLM/RAG)**\\n\\n### RAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG)\\n\\nG3LU\\n2024. 8. 3. 18:54\\n\\n오픈AI의 GPT 시리즈, Meta의 LLama 시리즈, Google의 Gemini와 같은 대형 언어 모델(LLM)은 생성 AI분야에서 큰 성과를 이루게 되었다. 하지만 위와 같은 모델들의 문제점은 종종 부정확하거나 관련 없는 정보를 생성하며, 오래된 정보에 의존하고, decision-making 과정이 투명하지 않아 블랙박스 추론을 초래하게 된다. \\n\\nRetrieval-Augmented Geneartion(RAG)는 외부 지식 소스로부터 추가적인 정보를 통합하여 대형 언어 모델(LLM)을 개선하는 과정이다. 이를 통해 LLM은 더 정확하고 문맥을 고려할 수 있는 답변을 생성하며, 환각(hallucination)을 방지할 수 있게 되었다. 이러한 장점을 가진 RAG는 2023년 이후 LLM 기반 시스템에서 범용적으로 사용되는 아키텍처로 자리 잡았다. 이에 RAG가 어떻게 발전해 왔는지 알아보도록 하겠다. \\n\\n## **💡 RAG Framework**\\n\\n최근 몇 년 동안 RAG 분야에서는 많은 연구가 이루어졌으며, 크게 RAG는 세 가지의 범주로 나눌 수 있다.\\n\\n* [Naive RAG](https://g3lu.tistory.com/41)\\n* Advanced RAG\\n* Modular RAG\\n\\n그럼 이 세 가지의 범주에 대해서 아래의 그림을 통해 자세히 알아보도록 하겠다. \\n\\n## **💡 Naive RAG**\\n\\n1. **Indexing** : Indexing은 RAG에서 수행되는 초기 단계이다. 이 단계는 Raw Data를 추출하는 것에서 시작하며, PDF, HTML, Word와 같은 다양한 파일 형식을 표준화된 plain text로 변환하게 된다. 대규모 언에 모델(LLM)은 한 번에 처리할 수 있는 텍스트의 최대 길이에 제한이 있다. 대규모 문서나 데이터 셋을 더 효율적으로 관리하기 위해 더 작고 관리 가능한 청크로 나누게 되는데, 이를 Chunking이라고 한다. 그런 다음 Embedding Model를 통해 Chunking된 text들을 벡터로 표현이 된다. 마지막으로 벡터화된 청크는 Vector DB에 키-값 쌍으로 저장된 게 된다. 이러한 Vector DB는 이후의 Retreival 단계에서 efficient 하고 scalable search capabilities 기능을 제공한다.\\n2. **Retrieval :**사용자 쿼리는 외부 지식 소스(Vector DB)으로 부터 관련 문맥을 검색하는데 활용이 된다. 이를 수행하기 위해 사용자 쿼리는 인코딩 모델에 의해 처리되어 의미적으로 관련된 임베딩을 생성하게 된다. 그런 다음 벡터화된 쿼리는 Vector DB에서 유사성 검색을 기반으로 상위 k개 검색을 수행하여 가장 비슷한 데이터를 찾게 된다.\\n3. **Generation :**사용자의 Query와 검색된 추가적인 정보는 Prompt에 입력되고 LLM를 거쳐 답변을 생성하게 된다.\\n\\n## **💡 Naive RAG의 문제점**\\n\\n#### 1. Indexing\\n\\n* **정보 추출의 불안정성 :** PDF와 같은 비정형 파일 내 이미지와 표에 있는 유용한 정보를 효과적으로 처리하지 못한다.\\n* **청킹 방법** : 청킹 과정에서 파일 유형의 특성을 고려하지 않고 \\\"one-size-fits-all\\\" 을 사용하는 것이 대반사이다. 이는 각 청크에 일관성과 불필요한 의미 정보가 포함될 가능성이 크며, 기존 텍스트의 문단 구분과 중요한 세부 사항을 놓치게 된다\\n* **비최적화 인덱싱 구조** : 인덱싱 구조가 최적화되지 않아 비효율적인 검색 기능을 초래하게 되며, 이는 검색 속도를 현저하게 느리게 만들고 검색 결과의 정확성을 떨어지게 만든다.\\n* **임베딩 모델의 의미 표현 능력** : 임베딩 모델이 텍스트의 의미를 제대로 파악하지 못해, 검색된 정보의 관련성이 낮아진다. 이는 중요한 정보를 놓칠 수 있게 된다.\\n\\n#### 2. Retrieval\\n\\n* **제한된 검색 알고리즘**: 키워드, 의미, 벡터 검색을 결합하지 않은 등 다양한 검색이나 알고리즘의 통합이 제한적이며, 이는 검색 결과의 다양성과 정확성을 저하시킨다.\\n* **쿼리 및 임베딩 모델의 한계** : 쿼리가 부족하거나 임베딩 모델의 의미 표현 성능이 낮아 유용한 정보를 검색하지 못한다.\\n* **답변 정보 중복** : 여러 검색된 컨텍스트가 유사한 정보를 포함하여 생성된 답변에 반복적인 내용이 포함된다.\\n\\n#### 3. Generation\\n\\n* **잘못된 응답 생성** : LLM이 관련 없거나 편향된 응답을 생성할 가능성이 높다.\\n* **잘못된 정보 제공** : 유용한 정보를 제공하지 않고 검색된 내용을 단순히 반복하는 결과를 초래할 수 있으며, 일관성 없는 답변을 뱉는 경우가 발생한다.\\n\\n---\\n\\n## **💡 Advanced RAG**\\n\\nAdvanced RAG는 Naive RAG 방식에서 직면하고 있는 문제를 해결하기 위해 고안되었다. 하지만 여기에서 고려해야 할 것은 어떠한 방식으로 데이터 소스에서 관련 문서를 효율적으로 검색하는 것이며, 아래와 같은 사항들을 해결해야 한다. \\n\\n* 문서와 쿼리의 semantic representation의 정확성을 어떻게 극대화할 수 있을까?\\n* 쿼리와 문서(청크)의 semantic space를 어떻게 align 시킬수 있을까?\\n* Retrieval의 출력을 LLM의 선호도에 맞게 조정할 수 있는 방법이 무엇일까?\\n\\n위의 세 가지 고찰을 보완하기 위해 **Pre-Retreival 및 Post-Retrieval**를 기존 RAG 아키텍처에 추가한 것이 Advanced RAG이다. 이들의 역할에 대해 알아보자. \\n\\n#### **1. Pre-Retrieval**\\n\\n주요 목적은 색인 구조와 사용자 쿼리를 개선하는 것이다. \\n\\n* 데이터 품질 향상 : 엔터티와 용어의 모호성을 제거하고, 사실 정확성을 위해 문맥 유지 및 새로운 정보를 업데이트 한다.\\n* 인덱스 구조 최적화 : 청크 크기를 최적화하여 문맥을 일정화하고 엔터티 간의 관계를 포착하기 위해 그래프 구조의 정보를 추가한다.\\n* 메타데이터 추가 : dates, chapters, subsections, purposes 등과 같은 관련 정보를 청크에 메타데이터로 추가하여 데이터 필터링을 개선한다.\\n* 청크 최적화 : 외부 데이터 소스/문서를 사용하여 RAG 파이프라인을 구축할 때, 청크를 더 작은 조각으로 나누어 세부적인 특성을 추출한다. 그런 다음 청크를 임베딩하여 내포하고 있는 의미를 도출한다.\\n\\n#### **2. Retrieval**\\n\\n청크 크기가 결정된 후, 임베딩하게 된다. 해당 단계에서는 쿼리와 임베딩된 청크 간의 유사성을 계산하여 가장 관련성 높은 청크를 식별하게된다. 여기서 쿼리와 청크에 사용되는 임베딩 모델을 최적화할 수 있다. \\n\\n* **Domain Knowledge Fine-Tuning** : 임베딩 모델이 각 도메인별 정보를 정확하게 포착할 수 있도록,  도메인 특화 데이터셋을 사용하여 fine-tuning한다. 이를 위한 데이터셋에는 쿼리, 코퍼스 및 관련 문서가 포함되어야 한다.\\n* **Dynamic Embedding** : 단어가 등장하는 맥락에 맞춰 임베딩을 fine-tuning하는 방식이다. 이는 각 단어에 대해 하나의 벡터만을 사용하여 각 토큰 당 정해진 임베딩을 리턴하는 방식이 아니라 주변 단어에 따라 맥락을 고려할 수 있는 BERT를 사용하는 것과 같다.\\n\\n#### **3. Post-Retrieval**\\n\\n관련된 정보(청크)들을 Vector Database 내에서 검색한 후, 쿼리와 함께 LLM에 입력된다. 하지만 검색된 청크들이 간혹 중복이 되거나 의미 없는 정보를 담는 경우 발생하게 되는데, 이는 LLM이 주어진 컨텍스트를 처리하는 방식에 영향을 미칠 수 있다. 이러한 문제를 극복하기 위해 사용되는 방법에 대해 간단하게 알아보자.\\n\\n* **Reranking** : 검색된 정보를 재순위하여 가장 관련성 높은 답변을 우선시한다. LLM에 입력이 추가될 때, 성능이 저하되는 경우가 발생한다. 이에 검색된 청크를 재정렬하고 Top-K 가장 관련성 높은 청크를 식별하여 LLM에 사용할 컨텍스트로 제공한다.\\n* **Prompt Compression** : 검색된 정보에 Noisy가 많을 수 있으므로, LLM에 태우기전에 관련 없는 정보를 압축하고 길이를 줄이는 것도 중요하다.\\n\\n---\\n\\n## **💡****Modular RAG**\\n\\n앞서 알아보았던 Advanced RAG는 Naive RAG의 컴포턴트를 조금씩 보완한 형태라면, Modular RAG는 조금 더 나아가 다양하고 유연한 구조를 지향한다. 이러한 구조는 RAG의 전반적인 성능을 향상 시켰으며, 현재 어플리케이션을 구축할 때 표준 패러다임이 되었다. 그럼 몇 가지 모듈에 대해 알아보도록 하자. \\n\\n* **Search** **Module** : 임베딩 유사도 기반 검색 외에도 추가적인 검색 시나리오를 가능하게 한다. 즉 Search Module은 특정 시나리오에 맞춰 LLM이 생성한 코드나 SQL 등을 사용하여 검색을 수행하는 모듈이다. 이에 다양한 데이터 소스를 사용할 수 있다는 점이다.\\n* **Memory Module :** LLM이 벡터 데이터베이스에서 검색된 청크뿐만 아니라 시스템 메모리에 저장된 이전 쿼리와 결합하여 현재 입력과 가장 유사한 답변을 찾는 모듈이다.\\n* **Fusion Module :** 유저의 의도를 정확하게 반영하지 않을 수도 있다는 차관에서 비롯되었다. LLM을 통해 유저의 쿼리로 부터 여러 개의 가상 쿼리를 생성하여 검색하는 방식이다.\\n\\n위의 모듈뿐만 아니라 다양한 모듈이 존재하니, 이에 대해 궁금하시다면 [링크](https://discuss.pytorch.kr/t/rag-1-2/3135) 참고하시길 바랍니다.\\n\\n\\n\\n#### '[LLM](/category/LLM) > [RAG](/category/LLM/RAG)' 카테고리의 다른 글\\n\\n|  |  |\\n| --- | --- |\\n| [LLAMA3.1 임베딩 모델로 변환하기 via. LLM2VEC](/61)  (2) | 2025.07.13 |\\n| [사용자의 질문을 여러 개 만드는 기법 : Query Translation (Part 1)](/48)  (0) | 2025.01.28 |\\n| [딥러닝 모델을 통한 PDF Parsing 기법](/47)  (0) | 2025.01.22 |\\n| [RAG 성능을 좌지우지 하는 PARSING(파싱)의 한계점](/44)  (2) | 2024.11.16 |\\n| [Retrieval-Augmented Generation 이란?](/41)  (0) | 2024.07.21 |\\n\\n**'LLM/RAG' Related Articles**\\n\\n* [사용자의 질문을 여러 개 만드는 기법 : Query Translation (Part 1)\\n  2025.01.28](/48?category=727982)\\n* [딥러닝 모델을 통한 PDF Parsing 기법\\n  2025.01.22](/47?category=727982)\\n* [RAG 성능을 좌지우지 하는 PARSING(파싱)의 한계점\\n  2024.11.16](/44?category=727982)\\n* [Retrieval-Augmented Generation 이란?\\n  2024.07.21](/41?category=727982)\\n\\n[more](/category/LLM/RAG)\\n\\n[Attention, Please!!!](https://g3lu.tistory.com)[LLM의 주목을 끄는 공간](https://g3lu.tistory.com)\\n\\n댓글0\\n\\n---\\n\\nBlog is powered by\\n[kakao](http://www.kakaocorp.com) / Designed by\\n[Tistory](http://www.tistory.com)\\n\\n## 티스토리툴바\\n\\n## 개인정보\\n\\n* [티스토리 홈](https://www.tistory.com)\\n* [포럼](https://www.tistory.com/forum)\\n* [로그인](#)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n## 단축키\\n\\n### 내 블로그\\n\\n|  |  |\\n| --- | --- |\\n| 내 블로그 - 관리자 홈 전환 | `Q`  `Q` |\\n| 새 글 쓰기 | `W` |\\n\\n### 블로그 게시글\\n\\n|  |  |\\n| --- | --- |\\n| 글 수정 (권한 있는 경우) | `E` |\\n| 댓글 영역으로 이동 | `C` |\\n\\n### 모든 영역\\n\\n|  |  |\\n| --- | --- |\\n| 이 페이지의 URL 복사 | `S` |\\n| 맨 위로 이동 | `T` |\\n| 티스토리 홈 이동 | `H` |\\n| 단축키 안내 | `Shift` + `/`  `⇧` + `/` |\\n\\n\\\\* 단축키는 한글/영문 대소문자로 이용 가능하며, 티스토리 기본 도메인에서만 동작합니다.\"}, {\"url\": \"https://jinsan-rm.tistory.com/5\", \"title\": \"LLM(Large language Model) 대규모 언어 모델을 위한 검색\", \"content\": \"모듈식 RAG은 고급 RAG의 발전된 형태로, 기존의 RAG 프레임워크에서 한 단계 더 나아가 다양한 모듈과 기능을 통합하여 더 큰 다양성과 유연성을 제공\", \"score\": 0.62879556, \"raw_content\": \"# [산으로 가는 블로그](https://jinsan-rm.tistory.com/)\\n\\n![블로그 이미지](https://tistory1.daumcdn.net/tistory/6777597/attach/15f2d99811d14ceab233ff756669f630)\\n\\n# LLM(Large language Model) 대규모 언어 모델을 위한 검색-증강 생성(RAG) 기술\\n\\n<https://arxiv.org/abs/2312.10997v1?utm_source=pytorchkr>\\n\\nRetrieval-Augmented Generation for Large Language Models: A Survey\\n\\nLarge language models (LLMs) demonstrate powerful capabilities, but they still face challenges in practical applications, such as hallucinations, slow knowledge updates, and lack of transparency in answers. Retrieval-Augmented Generation (RAG) refers to th\\n\\narxiv.org\\n\\n## 검색-증강 생성(RAG, Retrieval-Augmented Generation) 기술\\n\\n![](https://blog.kakaocdn.net/dna/oIojx/btsCTzEhURt/AAAAAAAAAAAAAAAAAAAAACsuk1BjwhMxhRR1DInueVLyPnN5kxR_HD3VEtEXh8Bw/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=DV5%2B2T4YJalcA32oPA5whZF8NtI%3D)\\n\\nRAG 연구의 타임 라인. 주요 기술들의 형성 과정\\n\\n![](https://blog.kakaocdn.net/dna/oIojx/btsCTzEhURt/AAAAAAAAAAAAAAAAAAAAACsuk1BjwhMxhRR1DInueVLyPnN5kxR_HD3VEtEXh8Bw/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=DV5%2B2T4YJalcA32oPA5whZF8NtI%3D)\\n\\n   LLM (대규모 언어 모델)은 강력한 성능과 기능을 보여준다. 성능과 기능은 뛰어나지만 여전히 완벽하지 못한 어려움에 직면해 있다. 실제로 적용하기 위해서는 환각, 느린 속도의 지식 업데이트 및 투명성이 부족한 답변등의 해결해야 할 문제가 있다. RAG는 이러한 문제를 해결해 나가기 위해 제안된 기술이다.\\n\\n   RAG는 LLM (대규모 언어 모델)에서 Input값에 대한 답변이나 결과를 생성하기 전에 문서 집합에서 관련 정보를 검색하여 이를 이용한 응답 방식으로 동작한다. 이는 LLM의 기존 문제점인 지식의 시대에 뒤떨어짐, 특정 영역에 대한 지식 부족, 그리고 응답의 투명성 부족을 해결하는 데 중점을 두고 있다.\\n\\n   이러한 방식의 RAG는 응답에 대한 정확도를 크게 향상시키고, 지식 집약적인 작업에서 모델이 환각에 빠지는 현상을 줄이는데 도움이 된다. 실제 문서에서 정보를 검색하기 때문에 사용자가 출처를 인용하여 답변의 정확성을 검증할 수 있으며, 이는 모델의 결과값에 대한 신뢰도를 높인다. 또한, 지식 업데이트와 특정 분야의 지식 도입에 용이하다.\\n\\n![](https://blog.kakaocdn.net/dna/8CJMf/btsCOCvfckD/AAAAAAAAAAAAAAAAAAAAANE7dXcnsn5HEZgqO3AbuqgDYgYk1tlDfdI7g6HnOUTs/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=zPv%2BObL%2FU7VYy7TOqiKk%2FbhTfuU%3D)\\n\\n![](https://blog.kakaocdn.net/dna/8CJMf/btsCOCvfckD/AAAAAAAAAAAAAAAAAAAAANE7dXcnsn5HEZgqO3AbuqgDYgYk1tlDfdI7g6HnOUTs/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=zPv%2BObL%2FU7VYy7TOqiKk%2FbhTfuU%3D)\\n\\n   파인 튜닝(Fine tuning)을 사용하여 RAG 외의 대규모 언어 모델이 정해진, 특정한 / 지식, 정보에 집중하도록 할 수 있다. 하지만 이러한 파인 튜닝은 실시간으로 바뀌는 정보를 반영하는데 시간이 걸리며, 모델을 새로 학습하는데 추가적인 컴퓨팅 자원이 소모된다. 이에 반해, RAG는 LLM에 특정 질의에 대한 정보를 검색할 수 있는 '문서 집합'을 제공하는 것과 유사하다.\\n\\n### RAG의 3가지 주요 패러다임 - 개요\\n\\n![](https://blog.kakaocdn.net/dna/b8Hb3l/btsCTXkOtn9/AAAAAAAAAAAAAAAAAAAAAONlGQRYVfhagZtX7YkkX1jEX6tbr8P8_CWH_xXk4tNC/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=FDPj0VCUjfmljA9e2ZmimD414Ww%3D)\\n\\n![](https://blog.kakaocdn.net/dna/b8Hb3l/btsCTXkOtn9/AAAAAAAAAAAAAAAAAAAAAONlGQRYVfhagZtX7YkkX1jEX6tbr8P8_CWH_xXk4tNC/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=FDPj0VCUjfmljA9e2ZmimD414Ww%3D)\\n\\n   RAG 기술의 연구 패러다임은 시간에 따라 끊임없이 발전하고 있다. 주요 패러다임들은 기본 RAG, 고급 RAG, 모듈형 RAG의 3가지로, 초창기의 기본 RAG는 비용 효율적이고 LLM만 사용하는 것보다 성능이 뛰어났지만 많은 단점들이 있다. 고급 RAG와 모듈러 RAG의 등장은 기본 RAG의 특정 결함을 해결하기 위한 것이다.\\n\\n## RAG의 3가지 주요 패러다임 - 기본 RAG(Naive RAG)\\n\\n![](https://blog.kakaocdn.net/dna/yzID9/btsCR962e0x/AAAAAAAAAAAAAAAAAAAAANdY4aY9nXU-7-Gv4-qatvpK5SZpgGtplssdaA34K1LS/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=u5s%2Bo6r7PMOAifXr6SxNQImMPzE%3D)\\n\\n![](https://blog.kakaocdn.net/dna/yzID9/btsCR962e0x/AAAAAAAAAAAAAAAAAAAAANdY4aY9nXU-7-Gv4-qatvpK5SZpgGtplssdaA34K1LS/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=u5s%2Bo6r7PMOAifXr6SxNQImMPzE%3D)\\n\\n   기본 RAG는 RAG 연구의 초기 방법론을 지칭하며, 전통적인 인덱싱, 검색 및 생성 과정을 포함하고 있다. 기본 RAG는 주로 간단한 검색 및 생성 방식에 초점을 맞추고 있으며, RAG의 기본 개념과 원리를 설명하는데 중요하다.\\n\\n   하지만, 낮은 검색 정확도, 응답 생성의 낮은 품질, 증강 과정의 어려움 등에 따라 발생하는 불필요한 반복, 부정확한 정보, 잘못된 문맥의 통합 등이 발생할 수 있다. 따라서 복잡하거나 도전적인 시나리오에 기본 RAG를 사용하기는 부적절할 수 있다.\\n\\n기본 RAG의 주요 구성요소로는 인덱싱(Indexing), 검색(Retrieve), 생성(Generation)이 있다.\\n\\n### **인덱싱(Indexing)**\\n\\n   데이터 소스에서 데이터를 얻고 인덱스를 생성하는 과정으로 오프라인 상태에서 일반적으로 수행된다. 이 과정은 데이터 정제, 청크 분할, 벡터 인코딩 및 인덱스 생성을 포함한다.\\n\\n### **검색(Retrieve)**\\n\\n   사용자의 입력을 벡터로 변환하고, 해당 벡터와 문서 블록의 벡터 간 유사성을 계산하여 상위 K 문서 블록을 선택한다.\\n\\n### **생성(Generation)**\\n\\n   주어진 질문과 관련 문서를 결합하여 새로운 프롬프트를 생성하고, 대규모 언어 모델이 이 정보를 기반으로 질문에 답변한다.\\n\\n## RAG의 3가지 주요 패러다임 - 고급 RAG(Advanced RAG)\\n\\n![](https://blog.kakaocdn.net/dna/byhcRZ/btsCP4E7KlD/AAAAAAAAAAAAAAAAAAAAAEk0XvqVt4KZ5yql54GlEiUm13y-Xc0ruyZ6iidQJyom/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=J7WQ1%2Bl4t8owHBArTpAKMBy0roA%3D)\\n\\n![](https://blog.kakaocdn.net/dna/byhcRZ/btsCP4E7KlD/AAAAAAAAAAAAAAAAAAAAAEk0XvqVt4KZ5yql54GlEiUm13y-Xc0ruyZ6iidQJyom/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=J7WQ1%2Bl4t8owHBArTpAKMBy0roA%3D)\\n\\n   고급 RAG는 기본 RAG의 부족한 점을 개선하기 위해 개발된 패러다임이다. 이는 주로 검색 및 생성의 질을 향상시키기 위한 사전 및 사후 검색 방법을 포함한다. 고급 RAG는 크게 검색 전 절차(Pre-Retrieval Process), 검색 후 절차(Post-Retrieval Process), RAG 파이프라인 최적화(RAG Pipeline Optimization)의 3단계로 나누어볼 수 있다:\\n\\n### 검색 전 절차(Pre-Retrieval Process)\\n\\n   검색 전 절차는 RAG 시스템의 검색 효율성과 결과의 질을 향상시키기 위해 필수적이다. 이 절차는 데이터 인덱싱 최적화 및 임베딩을 통해서 검색 결과의 관련성과 정확성을 높이는 데 중점을 둔다​​.\\n\\n#### 1. **데이터 인덱싱 최적화(Optimizing Data Indexing)**\\n\\n   인덱싱된 콘텐츠 품질을 향상시키기 위해서는 데이터 인덱싱을 최적화하는 것이 필요하다. 데이터 인덱싱 최적화를 위해서는 다음의 5가지 전략들이 주로 사용된다.\\n\\n#### 2. **임베딩(Embedding)**\\n\\n   임베딩은 효과적인 검색을 위한 RAG의 핵심 구성요소이다. 임베딩은 검색된 내용과 질문 간의 관련성을 강화하고 최종 생성된 출력에 대한 영향을 최적화하는 데 중요하다. 특히, 질문과 문서 간의 의미론적 유사성을 측정하고 이를 개선하는 데 필수적입니다​​. 임베딩에는 크게 2가지 전략이 있다.\\n\\n### 검색 후 절차(Post-Retrieval Process)\\n\\n   검색 후 절차는 고급 RAG에서 매우 중요한 단계로, 데이터베이스에서 검색된 중요한 문맥을 질의와 결합하여 LLM에 입력하는 과정이다. 예를 들어, 검색 결과로 나온 모든 문서들을 한꺼번에 LLM에 제공하는 것은 비효율적이며, LLM의 컨텍스트 윈도우(context window) 크기를 초과할 수도 있다. 이러한 문제를 해결하기 위해서는 검색된 문서들의 추가적인 후처리가 필요합니다. 크게 2가지 방법들을 살펴보겠다.\\n\\n#### 1. **순위 다시 매기기(ReRank)**:\\n\\n   검색 결과들 중 관련성(relevant)이 가장 높은 정보를 프롬프트의 앞부분(edge)으로 재배치하는 직관적인 아이디어이다. 이미 LlamaIndex, LangChain, HayStack 등과 같은 프레임워크들에 이 개념이 구현되어 있다. 예를 들어 Diversity Ranker는 문서 다양성을 기반으로 재정렬을 우선시하며, LostInTheMiddleRanker는 최상의 문서를 콘텍스트 창의 시작과 끝에 번갈아 배치한다. cohereAI rerank, bge-rerank, LongLLMLingua와 같은 접근 방식은 관련 텍스트와 질의 간의 의미론적 유사성을 다시 계산하여 벡터 기반의 시뮬레이션 검색을 해석하는 문제를 해결하고자 한다​​.\\n\\n#### 2. **프롬프트 압축(Prompt Compression)**\\n\\n   관련 연구들에 따르면 검색된 문서에서 발생하는 잡음(noise)이 RAG 성능에 부정적인 영향을 끼친다고 한다. 따라서 후처리 단계에서는 검색 문서들 중 중요하지 않은 맥락을 압축하고, 중요한 문단을 강조하면서 전체 콘텍스트 길이를 줄이는 데 중점을 둔다. Selective Context, LLMLingua와 같은 접근 방식은 작은 언어 모델을 사용하여 프롬프트의 상호 정보 또는 복잡성을 계산하여 요소의 중요성을 추정한다. Recomp는 다양한 수준의 압축기를 훈련시키며, Long Context는 광범위한 맥락을 다루며, “Walking in the Memory Maze”같은 방법론은 LLM의 주요 정보 인식을 향상시키기 위해 계층적 요약 트리를 설계한다​​.\\n\\n### RAG 파이프라인 최적화 (RAG Pipeline Optimization)\\n\\n   RAG 파이프라인 최적화는 RAG 시스템의 효율성과 정보 품질을 향상시키는 것을 목표로 한다. 현재까지의 연구들은 다양한 검색 기술을 지능적으로 결합하고, 검색 단계를 최적화하며, 인지적 역추적(cognitive backtracking) 개념을 도입하고, 다양한 질의 전략을 유연하게 적용하며, 임베딩 유사도를 활용하는 데 초점을 맞추고 있다. RAG 파이프라인 최적화는 효율성과 RAG 검색에서의 맥락적 정보의 풍부함 사이의 균형을 달성하는 것이 목표이다. 주요한 RAG 파이프라인 최적화 방법론들은 다음과 같다:\\n\\n#### 1. **하이브리드 검색(Hybrid Search)**\\n\\n   키워드 기반 검색, 의미론적 검색, 벡터 검색과 같은 다양한 기술을 지능적으로 혼합하여 검색하는 방법으로, RAG 시스템이 다양한 질의 유형과 정보 요구에 적응할 수 있게 하여 가장 관련성 높고 맥락이 풍부한 정보를 일관되게 검색하는 것이 목표이다. 이러한 하이브리드 검색은 검색 전략을 강화하고 RAG 파이프라인의 전반적인 성능을 향상시킬 수 있다.\\n\\n#### 2. **재귀적 검색 및 쿼리 엔진(Recursive Retrieval and Query Engine)**\\n\\n   초기 검색 단계에서 더 작은 문서 블록을 획득하여 주요 의미를 포착한 후, 후반 단계에서 더 많은 맥락적 정보를 가진 더 큰 블록을 언어 모델에 제공하는 방법이다. 이는 효율성과 맥락적으로 풍부한 응답 사이의 균형을 이루는 데 도움이 되며, 이러한 두 단계의 검색 방법은 효율성과 맥락적 풍부함을 조화롭게 결합할 수 있다.\\n\\n#### 3. **역추적 프롬프트(StepBack-prompt)**\\n\\n   LLM이 특정 사례에서 벗어나 일반적인 개념이나 원리에 대해 추론하도록 장려하기 위한 방법이다. 이러한 역추적 프롬프트를 도입하면 다양한 도전적인 추론 집약적 작업에서 눈에 띄는 성능 향상을 보여준다. 이 방법은 역추적 프롬프트에 대한 답변 생성과 최종 질문-응답 과정 모두에 적용할 수 있다.\\n\\n#### 4. **서브쿼리(Subqueries)**\\n\\n   서브쿼리는 다양한 질의 전략을 다양한 시나리오에 적용할 수 있으며, LlamaIndex와 같은 프레임워크에서 제공하는 쿼리 엔진을 사용할 수 있다. 예를 들어, 트리 쿼리, 벡터 쿼리 또는 가장 기본적인 순차적 청크 쿼리 등에서 사용할 수 있어 다양한 질의 전략을 통해 효율적인 검색을 도모할 수 있다.\\n\\n#### 5. **HyDE(Hypothetical Document Embeddings)**\\n\\n   HyDE는 LLM이 생성한 문장이 사용자의 질의보다 임베딩 공간에서 더 가까울 수 있다는 가정에 기반한 방법론이다. 즉, LLM을 사용하여 가상의 문서(= LLM의 답변)를 생성하고, 이를 임베딩한 후, 이 임베딩을 사용하여 실제 문서를 검색하는 방법이다. HyDE는 질의 기반의 임베딩 유사성을 추구하는 것과 달리, 이 방법은 답변 간의 임베딩 유사성에 중점을 두고 있다.\\n\\n## RAG의 3가지 주요 패러다임 - 모듈형 RAG(Modular RAG)\\n\\n![](https://blog.kakaocdn.net/dna/rHGda/btsCZg4VC0Q/AAAAAAAAAAAAAAAAAAAAAA3O1GgNpCwJS8K5ueMoRmbCc1QxhDqqeTr-CHJZwoP8/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=TkJPIQ%2BUj9zaQVmSf6l66W1mUR8%3D)\\n\\n![](https://blog.kakaocdn.net/dna/rHGda/btsCZg4VC0Q/AAAAAAAAAAAAAAAAAAAAAA3O1GgNpCwJS8K5ueMoRmbCc1QxhDqqeTr-CHJZwoP8/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=TkJPIQ%2BUj9zaQVmSf6l66W1mUR8%3D)\\n\\n   모듈식 RAG은 고급 RAG의 발전된 형태로, 기존의 RAG 프레임워크에서 한 단계 더 나아가 다양한 모듈과 기능을 통합하여 더 큰 다양성과 유연성을 제공한다. 이 접근 방식은 RAG 시스템을 다양한 시나리오와 요구 사항에 맞게 조정할 수 있도록 하는 여러 새로운 모듈과 패턴을 포함하고 있다.\\n\\n### 새로운 모듈들(New Modules)\\n\\n#### 1. **검색 모듈(Search Module)**\\n\\n   기본 및 고급 RAG의 유사성 검색과 달리, 검색 모듈은 특정 시나리오에 맞춰 LLM이 생성한 코드나 SQL, 사용자 도구 등을 사용하여 직접 검색을 수행하는 모듈이다. 이러한 검색 모듈을 사용하는 경우, 외부 검색 엔진이나 텍스트 데이터, 테이블 데이터 또는 지식 그래프 등을 데이터 소스로 사용할 수 있다.\\n\\n#### 2. **메모리 모듈(Memory Module)**\\n\\n   LLM 자체의 메모리 기능을 사용하여 검색을 개선(guide)하며, 현재 입력과 가장 유사한 기억을 찾는 모듈이다. 예를 들어, Self-mem 방법론은 검색 강화 생성기(Retrieval-Enhanced Generator)를 반복적으로 사용하여 사용자의 질문(original question)과 이중 질문(dual question)을 결합한 무제한 메모리 풀(unbounded memory pool)을 생성한다. 이러한 과정을 통하여 추론 과정을 향상시킨다.\\n\\n#### 3. **추가 생성 모듈(Extra Generation Module)**\\n\\n   검색된 내용의 중복 및 잡음 문제에 대응하기 위한 모듈로, LLM을 활용하여 검색을 위한 문서를 별도로 생성한다. 사용자의 질문을 데이터 소스에서 직접 검색하는 것보다, 이렇게 LLM에 의해 생성된 콘텐를 사용하였을 때 관련 정보를 더 많이 포함할 가능성이 높다.\\n\\n#### 4. **태스크 적응 모듈(Task Adaptable Module)**\\n\\n   다양한 다운스트림(downstream) 작업에 적응할 수 있도록 조정하는 모듈이다. 각 세부 작업과 모델의 범용성을 강화하는 것을 목표로, 필요한 제로샷 프롬프트를 검색하여 사용(UPRISE 방법)하거나, 제로샷 쿼리를 생성하는 LLM을 별도로 사용(Promptagator 방법)하기도 한다.\\n\\n#### 5. **정렬 모듈(Alignment Module)**\\n\\n   질의와 텍스트 간의 정렬 문제를 해결하기 위해 검색기에 별도의 학습 가능한 어댑터를 추가하는 모듈이다. PRCA, AAR, RRR 등의 방법론들이 있으며, 이를 통해 검색된 정보를 최적화하고 RAG의 효과를 향상시키는 것이 목적이다.\\n\\n#### 6. **검증 모듈(Validation Module)**\\n\\n   검색된 정보의 신뢰성을 평가하기 위해 도입되었으며, 실제 문서와 질의 간의 관련성을 평가하는 모듈이다. 이 모듈은 RAG의 강건성(robustness)을 향상시키는 것이 목표이다.\\n\\n### 새로운 패턴들(New Patterns)\\n\\n#### **모듈 추가 또는 교체(Adding or Replacing Modules)**\\n\\n   기존의 검색-읽기(RR; Retrieval-Read) 구조를 유지하면서 특정 기능을 향상시키기 위해 추가 모듈을 도입한다. 예를 들어, 재작성-검색-읽기(RRR; Rewrite-Retrieve-Read) 프로세스를 통해 검색 쿼리를 조작하고, 읽기 모듈의 하위 작업 성능을 향상시킬 수 있다.\\n\\n#### **모듈 간 조직적 흐름 조정(Adjusting the Flow between Modules)**\\n\\n   언어 모델과 검색 모델 간의 상호작용을 강화하기 위해 모듈 간 조직적 흐름을 조정한다. 이는 특정 문제 맥락에 기반하여 모듈 내에서의 대체 또는 재구성을 가능하게 한다.\\n\\n#### '[AI](/category/AI)' 카테고리의 다른 글\\n\\n|  |  |\\n| --- | --- |\\n| [MagicVideo-V2, 고품질 비디오 생성 기법](/7)  (0) | 2024.01.24 |\\n| [LLM(Large language Model) 대규모 언어 모델을 위한 검색-증강 생성(RAG) 기술 2편](/6)  (2) | 2024.01.24 |\\n| [(ML, Machine Learnin) 머신러닝 기초](/3)  (0) | 2023.12.20 |\\n| [ML / DL에 대하여](/2)  (0) | 2023.12.13 |\\n| [AI History (인공지능의 역사)](/1)  (0) | 2023.12.12 |\\n\\n## 태그\\n\\n## 관련글\\n\\n![](//i1.daumcdn.net/thumb/C176x120.fwebp.q85/?fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FU0WcC%2FbtsDSo3hF7O%2FAAAAAAAAAAAAAAAAAAAAAIEl9onytguTZ5JuijlPW84l_xlHKXsYkni6qYqUHRiw%2Fimg.jpg%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3Dvewz3BuO8OZsvSbbj9Wghi%252Fl224%253D)\\n![](//i1.daumcdn.net/thumb/C176x120.fwebp.q85/?fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FU71zp%2FbtsDUh3qcdH%2FAAAAAAAAAAAAAAAAAAAAANKAHgCUHyswfzKcKNYCWhZiASL0IY6UbEjIkkpSwCQ-%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3DWaoJZCVTxVJSzwDH8J7fsP7kNVE%253D)\\n![](//i1.daumcdn.net/thumb/C176x120.fwebp.q85/?fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FbZNiWF%2FbtsCfYTqgvv%2FAAAAAAAAAAAAAAAAAAAAABmra2qlo9ss6pGshAzw0LB1P3_mFAJQXyrdC16dAR2w%2Fimg.jpg%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3DHhKFDIeIBatkhtQaSjA6BNhpJHM%253D)\\n![](//i1.daumcdn.net/thumb/C176x120.fwebp.q85/?fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2Fdf6Dau%2FbtsBKdJ595c%2FAAAAAAAAAAAAAAAAAAAAAOTtBBiGK0xQN-CZHkR4M8cbr9AJrPC-uHrsmnE9qVFV%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3DMK5VI8K%252FLUHfxFHyIl99fignFdo%253D)\\n\\n## 공지사항\\n\\n## 최근글\\n\\n![](//i1.daumcdn.net/thumb/C58x58.fwebp.q85/?fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FbaG30a%2FbtsPZoh6t4Q%2FAAAAAAAAAAAAAAAAAAAAAMAaqX87a98tCDZFAVLr4VsrEcKuSOGKPmbRNvObsobW%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3DTMY5eqDQTdEtTOnG0tBdCbPF%252BKY%253D)\\n서강대 AI·SW대학원 수강신청 방법 & 꿀팁\\n2025.08.19\\n![](//i1.daumcdn.net/thumb/C58x58.fwebp.q85/?fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FYo5FE%2FbtsPSPa83ay%2FAAAAAAAAAAAAAAAAAAAAAIlui_qFJSZIqVMtFU632VNr0tBrMFLF4_3embMB7kL8%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3D3IKzu5tcQw33U%252FJ%252FynbydLCyMyc%253D)\\nClaude Code를 이용해 MCP로 RAG 시⋯\\n2025.08.18\\n![](//i1.daumcdn.net/thumb/C58x58.fwebp.q85/?fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FbpjB93%2FbtsPUO254OX%2FAAAAAAAAAAAAAAAAAAAAAE18tmWoR8hl3ZCgY-m7T31KPQ6JWUdn0zl04ty6HSxR%2Fimg.jpg%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3DwAbnku96gPaTawk7%252FcTL%252B1SthbI%253D)\\n서강대 AI·SW대학원 2025년 2학기 수강신청⋯\\n2025.08.17\\n![](//i1.daumcdn.net/thumb/C58x58.fwebp.q85/?fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FdcLTKh%2FbtsPK2zBQT1%2FAAAAAAAAAAAAAAAAAAAAANz70uLkleEbZYgc_tLzc0Ud_1D38qEcar49gjCNVatk%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3DOFo5mhKnoZyuCzWYJ3cvV6pT2kk%253D)\\n서강대 AISW대학원 - 텐서플로우활용기초 \\\\_ 1\\n2025.08.07\\n\\n## 인기글\\n\\n![](//i1.daumcdn.net/thumb/C58x58.fwebp.q85/?fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2Fxf3xc%2FbtsNA1c3SWK%2FAAAAAAAAAAAAAAAAAAAAAH9KBm7du1SzA1i2nUbwIrLtl2LyIHyk7p0YeRS3f20l%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3DSmUtiIUKCBnlGrCu5oz6yz1vFpA%253D)\\n![](//i1.daumcdn.net/thumb/C58x58.fwebp.q85/?fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2F6ysyl%2FbtsOK7cTBuI%2FAAAAAAAAAAAAAAAAAAAAAGS4chiOcGRtzLr2npDJuupNMbjIXDJ4rRRWhJdEPu-g%2Fimg.jpg%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3D5nYwvtIvDT%252BN%252Fe1o%252FveWXCwFhy8%253D)\\n![](//i1.daumcdn.net/thumb/C58x58.fwebp.q85/?fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FZzAim%2FbtsNB7jeWQe%2FAAAAAAAAAAAAAAAAAAAAAP29VPuO-yI9NmPzgDkQ65_ClNp4ql23BIQGwZtwcT5M%2Fimg.jpg%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3DFGpeZ87I6ugkm5e4bat5IPE6aZE%253D)\\n![](//i1.daumcdn.net/thumb/C58x58.fwebp.q85/?fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FpGNTt%2FbtsNCBjR2pE%2FAAAAAAAAAAAAAAAAAAAAABKvnH9Eo9c6quheXWE_4Ot2tqPjk2IZ2W7GcGV4rTjQ%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3DS5odG71Fq5M5t1rOxfkQHan2Tkc%253D)\\n\\n## 최근댓글\\n\\n## 태그\\n\\n## 전체 방문자\\n\\n1,469\\n\\nToday : 0\\n\\nYesterday : 2\\n\\nDesigned by 티스토리\\n\\n© Kakao Corp.\\n\\n## 티스토리툴바\"}]\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Modular RAG는 복잡한 RAG 시스템을 여러 개의 독립적인 모듈로 나누어, 필요에 따라 각 모듈을 교체하거나 조합할 수 있게 만들어 주는 구조입니다. 이는 시스템의 유연성과 확장성을 높이며, 각 모듈을 독립적으로 설계하고 필요에 따라 교체하거나 결합할 수 있도록 합니다. 예를 들어, 새로운 임베딩 모델이 등장하거나 기존의 벡터 DB가 성능을 더 높일 수 있다고 판단되면, 해당 모듈만 교체하는 방식으로 시스템을 최적화할 수 있습니다.\n",
            "\n",
            "모듈러 RAG는 크게 세 가지 주요 수준으로 나누어집니다:\n",
            "1. **모듈**: RAG 시스템의 핵심 프로세스를 담당합니다. 예를 들어, 데이터 인덱싱, 검색 전후 처리, 생성 과정 등이 포함됩니다.\n",
            "2. **하위 모듈**: 각 모듈 내의 세부 기능을 세분화하고 최적화하는 역할을 합니다. 예를 들어, 쿼리 확장이나 검색기 선택 등이 포함됩니다.\n",
            "3. **연산자**: 하위 모듈 내에서 실제 기능을 수행하는 기본 단위입니다. 예를 들어, 슬라이딩 윈도나 메타데이터 첨부 같은 기능이 포함됩니다.\n",
            "\n",
            "이러한 구조는 시스템의 복잡도를 낮추고, 성능을 더욱 향상할 수 있는 가능성을 제공합니다. 모듈러 RAG는 다양한 시나리오에 맞게 빠르게 적응할 수 있으며, 시스템의 유지 관리가 용이하고, 새로운 기술이나 방법론을 도입하는 데에도 유리합니다.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mexecute\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "('Modular RAG의 구조와 작동 방식을 조사한다.', 'Modular RAG는 복잡한 RAG 시스템을 여러 개의 독립적인 모듈로 나누어, 필요에 따라 각 모듈을 교체하거나 조합할 수 있게 만들어 주는 구조입니다. 이는 시스템의 유연성과 확장성을 높이며, 각 모듈을 독립적으로 설계하고 필요에 따라 교체하거나 결합할 수 있도록 합니다. 예를 들어, 새로운 임베딩 모델이 등장하거나 기존의 벡터 DB가 성능을 더 높일 수 있다고 판단되면, 해당 모듈만 교체하는 방식으로 시스템을 최적화할 수 있습니다.\\n\\n모듈러 RAG는 크게 세 가지 주요 수준으로 나누어집니다:\\n1. **모듈**: RAG 시스템의 핵심 프로세스를 담당합니다. 예를 들어, 데이터 인덱싱, 검색 전후 처리, 생성 과정 등이 포함됩니다.\\n2. **하위 모듈**: 각 모듈 내의 세부 기능을 세분화하고 최적화하는 역할을 합니다. 예를 들어, 쿼리 확장이나 검색기 선택 등이 포함됩니다.\\n3. **연산자**: 하위 모듈 내에서 실제 기능을 수행하는 기본 단위입니다. 예를 들어, 슬라이딩 윈도나 메타데이터 첨부 같은 기능이 포함됩니다.\\n\\n이러한 구조는 시스템의 복잡도를 낮추고, 성능을 더욱 향상할 수 있는 가능성을 제공합니다. 모듈러 RAG는 다양한 시나리오에 맞게 빠르게 적응할 수 있으며, 시스템의 유지 관리가 용이하고, 새로운 기술이나 방법론을 도입하는 데에도 유리합니다.')\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mreplan\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "Naive RAG와 Modular RAG의 구조적 차이점을 비교한다.\n",
            "Naive RAG와 Modular RAG의 작동 방식 차이점을 비교한다.\n",
            "Modular RAG가 production level에서 가지는 이점을 조사한다.\n",
            "Naive RAG와 비교하여 Modular RAG의 이점을 정리한다.\n",
            "Modular RAG의 이점을 production level에서의 사용 사례와 연결하여 설명한다.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Naive RAG와 Modular RAG의 구조적 차이점을 비교하기 위해, 두 가지 접근 방식의 기본적인 구조를 이해해야 합니다.\n",
            "\n",
            "1. **Naive RAG (Retrieval-Augmented Generation)**:\n",
            "   - **구조**: Naive RAG는 일반적으로 단순한 구조를 가지고 있으며, 주로 검색 단계와 생성 단계를 포함합니다. 검색 단계에서는 주어진 질문에 대해 관련 문서를 검색하고, 생성 단계에서는 검색된 문서를 기반으로 답변을 생성합니다.\n",
            "   - **구성 요소**: Naive RAG는 검색기(retriever)와 생성기(generator)로 구성됩니다. 검색기는 주어진 질문에 대해 관련 문서를 찾고, 생성기는 이 문서를 바탕으로 답변을 생성합니다.\n",
            "\n",
            "2. **Modular RAG**:\n",
            "   - **구조**: Modular RAG는 보다 복잡하고 유연한 구조를 가지고 있습니다. 이 구조는 다양한 모듈을 포함하여 각 모듈이 특정 기능을 수행하도록 설계되어 있습니다. 이러한 모듈화된 접근 방식은 시스템의 확장성과 유지보수성을 높입니다.\n",
            "   - **구성 요소**: Modular RAG는 여러 모듈로 구성되며, 각 모듈은 특정한 역할을 수행합니다. 예를 들어, 데이터 전처리 모듈, 검색 모듈, 필터링 모듈, 생성 모듈 등이 포함될 수 있습니다. 이러한 모듈들은 독립적으로 개발 및 개선될 수 있습니다.\n",
            "\n",
            "**구조적 차이점 요약**:\n",
            "- Naive RAG는 단순한 검색 및 생성 구조로 이루어져 있는 반면, Modular RAG는 다양한 기능을 수행하는 여러 모듈로 구성되어 있어 더 복잡하고 유연한 구조를 가지고 있습니다.\n",
            "- Modular RAG는 모듈화된 설계를 통해 시스템의 확장성과 유지보수성을 높이며, 각 모듈이 독립적으로 작동할 수 있도록 설계되어 있습니다.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mexecute\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "('Naive RAG와 Modular RAG의 구조적 차이점을 비교한다.', 'Naive RAG와 Modular RAG의 구조적 차이점을 비교하기 위해, 두 가지 접근 방식의 기본적인 구조를 이해해야 합니다.\\n\\n1. **Naive RAG (Retrieval-Augmented Generation)**:\\n   - **구조**: Naive RAG는 일반적으로 단순한 구조를 가지고 있으며, 주로 검색 단계와 생성 단계를 포함합니다. 검색 단계에서는 주어진 질문에 대해 관련 문서를 검색하고, 생성 단계에서는 검색된 문서를 기반으로 답변을 생성합니다.\\n   - **구성 요소**: Naive RAG는 검색기(retriever)와 생성기(generator)로 구성됩니다. 검색기는 주어진 질문에 대해 관련 문서를 찾고, 생성기는 이 문서를 바탕으로 답변을 생성합니다.\\n\\n2. **Modular RAG**:\\n   - **구조**: Modular RAG는 보다 복잡하고 유연한 구조를 가지고 있습니다. 이 구조는 다양한 모듈을 포함하여 각 모듈이 특정 기능을 수행하도록 설계되어 있습니다. 이러한 모듈화된 접근 방식은 시스템의 확장성과 유지보수성을 높입니다.\\n   - **구성 요소**: Modular RAG는 여러 모듈로 구성되며, 각 모듈은 특정한 역할을 수행합니다. 예를 들어, 데이터 전처리 모듈, 검색 모듈, 필터링 모듈, 생성 모듈 등이 포함될 수 있습니다. 이러한 모듈들은 독립적으로 개발 및 개선될 수 있습니다.\\n\\n**구조적 차이점 요약**:\\n- Naive RAG는 단순한 검색 및 생성 구조로 이루어져 있는 반면, Modular RAG는 다양한 기능을 수행하는 여러 모듈로 구성되어 있어 더 복잡하고 유연한 구조를 가지고 있습니다.\\n- Modular RAG는 모듈화된 설계를 통해 시스템의 확장성과 유지보수성을 높이며, 각 모듈이 독립적으로 작동할 수 있도록 설계되어 있습니다.')\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mreplan\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "Modular RAG가 production level에서 가지는 이점을 조사한다.\n",
            "Naive RAG와 비교하여 Modular RAG의 이점을 정리한다.\n",
            "Modular RAG의 이점을 production level에서의 사용 사례와 연결하여 설명한다.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_web_search (call_rEKeB6VFCHF0nXgHjff10Iil)\n",
            " Call ID: call_rEKeB6VFCHF0nXgHjff10Iil\n",
            "  Args:\n",
            "    query: Modular RAG advantages in production level\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mtools\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_web_search\n",
            "\n",
            "[{\"url\": \"https://coralogix.com/ai-blog/rag-in-production-deployment-strategies-and-practical-considerations/\", \"title\": \"RAG in Production: Deployment Strategies & Practical Considerations\", \"content\": \"Sep 2, 2024·This modular approach transcends traditional linear designs by integrating routing, scheduling, and fusion mechanisms, thus facilitating more\", \"score\": 0.60861784, \"raw_content\": \"[Skip to content](#primary)\\n\\n\\n[Get a Demo](/demo/)\\n[Sign In](https://dashboard.app.eu2.coralogix.com/#/login/user)\\n\\n[Back](/ai-blog/)\\n\\n[Building AI systems](https://coralogix.com/ai-guides-category/building-ai-systems/)\\n\\n# RAG in Production: Deployment Strategies and Practical Considerations\\n\\nDeval Shah\\nSep 02, 2024\\n\\n23 mins read\\n\\nAs organizations rush to implement Retrieval-Augmented Generation (RAG) systems, many struggle at the production stage, their prototypes breaking under real-world scale.  Some key reasons are unexpected query patterns, overwhelming retrieval mechanisms, latency issues, and the demand for up-to-date information.\\n\\nThis article cuts through the noise, offering actionable strategies for scaling RAG systems from development to production. We provide a comprehensive blueprint for building robust, responsive RAG systems based on recent advances in vector database management, LLM deployment architectures, and API design.\\n\\nWe address the critical challenges, from optimizing content freshness to orchestrating complex deployment pipelines. Whether you’re fine-tuning retrieval algorithms or architecting system-wide scaling solutions, this guide equips you with actionable insights to transform your RAG implementation from a proof of concept into a production-ready deployment.\\n\\n## TL;DR:\\n\\n* Implement distributed vector databases with sharding for scalable, low-latency retrieval in RAG systems.\\n* Utilize GPU-accelerated models and caching strategies to optimize the retrieval pipeline’s performance.\\n* Track and manage latencies across the system for optimal user experience.\\n* Deploy LLMs using managed services or self-hosted solutions with load balancing, considering trade-offs between control and ease of management.\\n* Design RAG-specific APIs with streaming capabilities, context-aware endpoints, and robust error handling for production resilience.\\n* Adopt Kubernetes for orchestration, implementing custom auto-scaling based on RAG-specific metrics like query complexity and retrieval time.\\n\\n## What is RAG?\\n\\nRetrieval-Augmented Generation (RAG) is a framework that aims to provide external information to generative models using a retrieval component. The core idea is to combine the strengths of information retrieval and generation to handle complex, knowledge-intensive tasks more effectively.  \\n  \\nInformation retrieval began with electromechanical searching devices and evolved with the birth of computers, such as the [ENIAC](https://en.wikipedia.org/wiki/ENIAC#:~:text=ENIAC%20was%20completed%20in%201945,Giant%20Brain%22%20by%20the%20press.) in 1945, which marked the beginning of programmable computing for search tasks.\\n\\nOver the decades, the field has seen a convergence of statistical text analysis and natural language processing (NLP), which laid the groundwork for modern search engines like Google and cognitive technologies like IBM’s Watson.\\n\\nThe increased computing power and the development of sophisticated algorithms have enabled more efficient and accurate retrieval processes, which is crucial for RAG systems that rely on real-time data retrieval to enhance generative models.\\n\\nThese historical advancements in IR have shaped the capabilities of RAG, allowing it to dynamically access and integrate external semantic knowledge, thereby improving the accuracy and relevance of generated content.\\n\\nThe ***retrieval*** component fetches relevant information from external databases or non-parametric knowledge bases in an RAG system. In contrast, the ***generation*** component uses this information to produce coherent and contextually accurate responses.\\n\\n### RAG vs. LLM\\n\\nA comparative table comprehensively explains the differences between Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs). This table highlights each approach’s distinct features and advantages, focusing on core functionality, data sources, and application suitability.\\n\\n|  |  |  |\\n| --- | --- | --- |\\n| **Aspect** | **LLM** | **RAG** |\\n| Core Functionality | Generates text based on pre-trained data without external retrieval | Combines retrieval of external data with generative capabilities |\\n| Data Source | It relies on a vast amount of pre-trained data stored within the model | Utilizes real-time data retrieval from external databases |\\n| Handling Domain-Specific Knowledge | May lack domain-specific knowledge, especially in niche areas | Effective for tasks requiring up-to-date or specialized knowledge |\\n| Accuracy and Relevance | Can suffer from hallucinations and lack of context-specific accuracy | Enhances accuracy by grounding responses in factual data |\\n| Applications | Used for general-purpose text generation and common knowledge queries | Suitable for knowledge-intensive tasks like customer support and document summarization |\\n| Response Generatio | Generates responses based on internal model knowledge | Generates responses based on retrieved factual data |\\n\\n### RAG vs. Prompt-Based Models\\n\\nPrompt-based models rely on the pre-trained knowledge of large language models (LLMs) to generate responses. These models use prompts to guide the LLMs in generating text based on the internal knowledge encoded during training. However, this approach can lead to inaccuracies, especially when dealing with domain-specific or up-to-date information, as the model’s knowledge is static and limited to its training data.\\n\\nBy integrating real-time data retrieval, RAG models are more reliable and effective for applications requiring high accuracy, such as customer support and document summarization.\\n\\nRecent research underscores the advantages of RAG over prompt-based models. A recent research [study](https://arxiv.org/abs/2407.07321) highlighted how RAG models outperform traditional LLMs in handling complex question-answering tasks by leveraging external data sources to provide more accurate and contextually relevant responses.\\n\\nAnother [research](https://arxiv.org/pdf/2403.15729) on RAG-based Summarization Agent for the Electron-Ion Collider\\n\\ndemonstrated an interesting application of RAG in summarizing large volumes of scientific documents, showcasing its ability to condense information while maintaining accuracy through retrieval-based grounding.\\n\\n### Grounding vs. RAG\\n\\nGrounding refers to anchoring generated content in factual data. While both grounding and RAG aim to improve the factual accuracy of responses, RAG explicitly incorporates a retrieval step to fetch relevant data before generation. This makes RAG a more structured approach to grounding, ensuring the generated content is relevant and accurate.\\n\\nRecent research highlights the effectiveness of RAG in improving the factual accuracy of language models. For instance, a [study](https://arxiv.org/pdf/2407.00466) on biomedical AI agents demonstrated how RAG could identify and correct factual errors in large-scale knowledge graphs by leveraging domain-specific retrieval and generation techniques.\\n\\nBoth grounding and RAG aim to enhance the factual accuracy of generated content; RAG’s retrieval step provides a more robust and dynamic approach, making it particularly suitable for applications requiring precise and contextually relevant information.\\n\\n## What Does Deployment in Production Mean for RAG Systems?\\n\\nThe objectives of development and production often differ significantly. It is especially true for new technologies like Retrieval-augmented Generation (RAG), where organizations prioritize rapid experimentation to test before committing more resources.\\n\\nHowever, once key stakeholders are convinced, the goal shifts from demonstrating the potential of an application to generating real value by deploying it into production. Until this transition occurs, the ROI is typically zero.\\n\\nIn the context of RAG systems, productionizing involves migrating from a prototype or test environment to a robust, operational state. It also involves scaling the system to manage varying user demand and traffic, ensuring consistent performance and availability.\\n\\nIn production deployment, RAG systems address several challenges traditional search engines face, such as query diversity, retrieval accuracy, and latency management. They are designed to handle complex queries with nuanced understanding, providing users with direct and relevant answers rather than a list of documents to explore. This capability positions RAG systems as the potential future standard for search engines, offering a more efficient and user-friendly approach to accessing information.\\n\\nDeploying RAG systems in production introduces several unique challenges that need careful consideration:\\n\\n### Query Diversity\\n\\nRAG systems encounter various queries that may not have been anticipated during development in a production environment. This diversity requires the system to be robust and adaptable, capable of handling unexpected inputs without degradation in performance. Ensuring the system can generalize well to new types of queries is crucial.\\n\\nThe following table outlines the top 10 query types that RAG systems typically encounter, along with descriptions and examples.\\n\\n|  |  |  |\\n| --- | --- | --- |\\n| **Query Type** | **Description** | **Example with Context** |\\n| Fact-based Queries | Inquiries seeking specific factual information. | “What is the capital of France?” with context from geographic databases. |\\n| Procedural Queries | Questions about processes or methods. | “How do I reset my password?” using context from IT support documents. |\\n| Comparative Queries | Requests to compare two or more items. | “Compare the battery life of iPhone 13 and Samsung Galaxy S21” with context from tech reviews. |\\n| Analytical Queries | Queries requiring analysis or interpretation of data. | “What are the trends in global warming over the last decade?” with context from climate reports. |\\n| Opinion-based Queries | Requests for subjective information or opinions. | “What are the best practices for remote work?” using context from HR guidelines and expert articles. |\\n| Historical Queries | Questions about historical events or contexts. | “What caused the fall of the Roman Empire?” with context from historical texts. |\\n| Predictive Queries | Inquiries about future events or predictions. | “What is the forecast for the stock market next year?” using context from financial analyses. |\\n| Technical Queries | Questions requiring technical knowledge or specifications. | “What are the specifications of the Tesla Model S?” with context from automotive manuals. |\\n| Legal Queries | Inquiries about legal information or interpretations. | “What are the GDPR compliance requirements?” using context from legal documents and regulations. |\\n| Creative Queries | Requests for creative content generation or ideas. | “Generate a marketing slogan for a new product” using context from branding guidelines and market research. |\\n\\nThese query types highlight the versatility and adaptability required of RAG systems in production environments, where they must efficiently handle a wide range of user inquiries. By understanding and preparing for these diverse query types, developers can enhance the robustness and effectiveness of RAG systems.\\n\\n### Retrieval Accuracy\\n\\nThe retrieval stage system is crucial as it sets the foundation for the entire pipeline. The retrieved information’s accuracy and relevance directly impact the generated responses’ quality. Here’s how the retrieval stage can have cascading effects on the rest of the RAG pipeline:\\n\\n#### Cascading Effects of Retrieval Accuracy\\n\\n1. **Foundation for Generation:** The retrieval component fetches relevant information from external knowledge sources, which serves as the basis for the generation phase. Suppose the retrieved data is inaccurate or irrelevant. In that case, the generated responses will likely be flawed, as the language model relies heavily on the quality of input data to produce coherent and contextually appropriate answers.\\n2. **Error Propagation**: Inaccuracies in the retrieval phase can lead to error propagation throughout the pipeline. For instance, if the retrieval system fetches outdated or incorrect data, these errors can be magnified in the final output, resulting in misinformation or irrelevant answers, thereby undermining user trust.\\n3. **Impact on System Performance**: Efficient retrieval is critical for maintaining system performance, particularly latency and throughput. Poor retrieval strategies can lead to increased response times, affecting the user experience and the system’s ability to handle high query volumes efficiently.\\n4. **Influence on Generation Quality**: The quality of the generated content is directly linked to the relevance and accuracy of the retrieved information. Effective retrieval ensures the language model can access the most pertinent data, generating accurate and contextually rich responses.\\n5. **Adaptability and Robustness**: A robust retrieval system can adapt to diverse and unpredictable queries, ensuring the RAG system remains effective across various contexts and use cases. This adaptability is essential for maintaining the system’s reliability and user satisfaction in production environments.\\n\\nBy implementing efficient retrieval strategies, such as semantic search or hybrid keyword-vector approaches, and continuously refining these strategies based on user feedback, RAG systems can enhance their retrieval accuracy and, consequently, the quality of their outputs.\\n\\n### Latency Management\\n\\nIn production environments, especially for RAG systems, latency is a critical factor that directly impacts user experience and system effectiveness. Users expect near-instantaneous responses, mirroring the performance of traditional search engines. Google, for instance, has been serving search results with median latencies under 300 milliseconds for over a decade, setting a high bar for information retrieval systems. This expectation extends to RAG-based question-answering systems, where users anticipate quick, accurate responses despite the added complexity of natural language generation.\\n\\nRAG systems must scale with low latency and high throughput to meet production demands. Low latency ensures individual users receive timely responses, while high throughput allows the system to handle multiple concurrent requests efficiently. This dual requirement stems from the nature of RAG operations: retrieving relevant documents, processing them, and generating coherent responses—all in real-time.\\n\\nPerplexity AI, a leading real-time RAG-based search engine, emphasizes the importance of retrieval speed and efficient prompt construction in achieving low latency at scale. Their system architecture, which includes techniques like semantic caching and optimized retrieval models, demonstrates the feasibility of building state-of-the-art RAG systems that maintain low latency under high load.\\n\\nTracking tail latencies across the RAG pipeline is crucial for maintaining consistent performance. While median latencies provide a general performance picture, tail latencies (e.g., 95th or 99th percentile) reveal how the system behaves under stress or for particularly challenging queries.\\n\\nMost requests are within the acceptable response threshold, and a small number of requests are very much slower, exceeding the threshold, creating this “tail,” which is highlighted in red.\\n\\nGoogle’s [research](https://research.google/pubs/the-tail-at-scale/) on tail latency in distributed systems by Jeff Dean highlights that these outliers can significantly impact user experience and should be a key focus for optimization. I would highly recommend reading the paper as it covers the importance of measuring tail latencies.\\n\\nTechniques such as batching similar queries, caching frequent results, and employing parallel processing can help manage both average and tail latencies, ensuring a smooth user experience even under high loads or complex query scenarios.\\n\\n### Content Freshness\\n\\nContent freshness is crucial for maintaining the relevance and accuracy of responses. Content freshness is maintaining up-to-date indexes or databases that help retrieve relevant information.  \\n  \\nIn enterprise RAG systems with private knowledge bases, content freshness presents unique challenges and considerations. Unlike web-scale systems dealing with publicly available information, enterprise RAG systems often rely on proprietary, internal data sources that may update at varying frequencies.\\n\\nFor instance, financial institutions might need real-time updates for market data, while manufacturing companies might update product specifications less frequently. In these environments, content freshness is about rapid indexing and ensuring data consistency across different internal systems.\\n\\nIn real-time RAG systems where the index scales with the web, regular index updates are essential to prevent stale information as the web continuously expands, with an estimated [252,000 new websites](https://www.forbes.com/advisor/au/business/software/website-statistics/) created daily. This rapid indexing capability is vital for RAG systems to ensure that responses reflect the most current information available, particularly for queries related to recent events or rapidly evolving topics.\\n\\nMoreover, modern RAG systems must be capable of compiling and scraping fresh knowledge from diverse web sources, including video, audio, text, and images. This multi-modal approach to data ingestion allows for a more comprehensive and up-to-date knowledge base.\\n\\nHowever, this data collection process must adhere to legal and ethical standards. Respecting robots.txt files, which specify crawling permissions for websites, is a fundamental practice. Additionally, RAG system developers must navigate complex legal landscapes, such as the ongoing debates around web scraping legality, exemplified by cases like [HiQ Labs v. LinkedIn](https://en.wikipedia.org/wiki/HiQ_Labs_v._LinkedIn).\\n\\nEffective index management in RAG systems involves making strategic decisions about what information to retain and what to discard. This process relies on sophisticated heuristics that balance relevance, freshness, and storage constraints. Google’s [PageRank](https://www.cis.upenn.edu/~mkearns/teaching/NetworkedLife/pagerank.pdf) algorithm, which considers both the content and the network of links pointing to a page, provides a foundational approach to assessing content value.\\n\\nRecent advancements, such as [semantic caching](https://qdrant.tech/blog/semantic-cache-ai-data-retrieval/), demonstrate how RAG systems can efficiently manage their knowledge base by prioritizing frequently accessed and highly relevant information. These strategies help maintain content freshness and improve system efficiency, allowing faster retrieval and lower latency in query responses.\\n\\nHaving outlined these core challenges, we will now explore the architecture considerations crucial for ensuring scalable and reliable production deployment of RAG systems, offering a roadmap for creating high-performance, production-ready solutions that effectively address these challenges.\\n\\n## Setting Up a RAG Deployment Pipeline\\n\\nDeploying a Retrieval-Augmented Generation (RAG) system in production presents unique challenges due to the complex interplay of its components: document processing, embedding generation, vector storage, and language model inference. A well-designed deployment pipeline is crucial for maintaining system reliability, performance, and up-to-date knowledge.\\n\\n### What is a RAG Deployment Pipeline?\\n\\nA deployment pipeline is an automated process that orchestrates the updating and deploying of various RAG components, including code changes, model updates, and knowledge base refreshes. It ensures that all elements of the RAG system are synchronized and thoroughly validated before reaching the production environment.\\n\\n### Steps to Deploy a RAG Pipeline in Production\\n\\n1. **Versioning**: Implement versioning for code, models, and the knowledge base to ensure reproducibility.\\n2. **Embedding Pipeline**: Set up an efficient pipeline for generating and updating embeddings as new documents are added or existing ones are modified.\\n3. **Vector Database Management**: Implement strategies for updating the vector database without downtime, such as zero-downtime reindexing.\\n4. **Model Deployment**: Use model serving platforms like Seldon Core or KServe to deploy and scale embedding and language models.\\n5. **RAG-Specific Testing**: Develop tests that evaluate retrieval relevance, answer quality, and overall RAG pipeline performance.\\n6. **Knowledge Base Monitoring**: Implement systems to monitor the freshness and quality of the knowledge base.\\n7. **Gradual Rollout**: Use canary or blue-green deployments to update the RAG system components safely.\\n\\n### Differences Between Staging and Production RAG Environments\\n\\nWhile striving for environment parity, there are RAG-specific differences to consider:\\n\\n* **Knowledge Base**: Production uses the full, live knowledge base, while staging may use a representative subset.\\n* **Query Volume**: Production handles real user traffic, providing more comprehensive retrieval pattern analysis.\\n* **Latency Requirements**: Production environments often have stricter latency requirements, necessitating optimized retrieval and generation processes.\\n* **Feedback Loop**: Production environments can leverage real user feedback to improve the RAG system continuously.\\n\\nBy implementing an RAG-specific deployment pipeline, you can ensure that all components of your RAG system – from the knowledge base to the language model – remain synchronized, performant, and reliable as you continuously improve and expand the system’s capabilities.\\n\\n## Deployment Recipes for RAG Systems\\n\\nAs we transition to the technical considerations for deployment, focusing on each stage of a production RAG system is crucial.\\n\\n1. Retrieval\\n2. Generation\\n3. API Layer\\n4. Orchestration\\n5. Monitoring & Observability\\n\\nThe retrieval stage forms the foundation of an RAG system’s performance and scalability. Let’s explore key deployment strategies for the retrieval stage.\\n\\n### Retrieval Stage\\n\\n#### Vector Database\\n\\nEfficient vector storage and retrieval are paramount for RAG system performance. Consider the following strategies:\\n\\n* **Distributed Vector Databases**: Implement scalable solutions like Pinecone, Weaviate, or Milvus. These databases are designed for high-dimensional vector similarity search, which is crucial for RAG systems.\\n* **Sharding and Replication**: Employ horizontal scaling techniques to distribute your vector index across multiple nodes. This approach enhances query performance and provides fault tolerance.\\n* **Multi-Region Deployment**: For global applications, consider deploying vector databases across multiple geographical regions to reduce latency and improve availability.\\n\\n#### Document Processing\\n\\nA robust document processing pipeline ensures efficient ingestion and updating of your knowledge base:\\n\\n* **Scalable Ingestion**: Utilize distributed stream processing frameworks like Apache Kafka or Apache Flink for high-throughput document ingestion.\\n* **Asynchronous Processing**: Implement asynchronous workflows for document chunking, embedding generation, and vector storage to enhance system responsiveness.\\n* **Error Handling and Retries**: Develop comprehensive error handling mechanisms with automated retries to ensure data integrity and processing reliability.\\n\\nThe diagram illustrates a multi-tenant RAG document processing pipeline. It shows data flow from client applications through document ingestion, processing (including chunking and embedding), and storage in a vector database.\\n\\n#### Embedding Model Deployment\\n\\nOptimizing embedding model deployment is crucial for maintaining low latency in the retrieval process:\\n\\n* **GPU-Enabled Servers**: Deploy embedding models on GPU-accelerated infrastructure or leverage cloud services like [AWS SageMaker](https://aws.amazon.com/sagemaker/) or Azure Machine Learning for efficient inference.\\n* **Optimized Serving Frameworks**: Implement model serving using frameworks like [NVIDIA Triton](https://developer.nvidia.com/triton-inference-server) or TensorRT, designed for high-throughput, low-latency inference.\\n* **Model Optimization Techniques**: Consider quantization and pruning [techniques](https://arxiv.org/html/2402.09748v1) to reduce model size and increase inference speed without significant accuracy loss.\\n\\n#### Retrieval Optimization\\n\\nEnhancing the retrieval component can significantly improve RAG system performance:\\n\\n* **Efficient Search Algorithms**: Implement advanced approximate nearest neighbor search algorithms like [Hierarchical Navigable Small World](https://www.pinecone.io/learn/series/faiss/hnsw/) (HNSW) or [Inverted File with Product Quantization](https://lancedb.github.io/lancedb/concepts/index_ivfpq/) (IVF-PQ) for fast similarity search.\\n* **In-Memory Caching**: Utilize in-memory caching solutions like Redis to store frequently accessed vectors, reducing the load on your vector database and improving response times.\\n* **Query Optimization**: Develop sophisticated query preprocessing and optimization techniques, such as query expansion or semantic filtering, to enhance retrieval relevance.\\n  + [**Query Expansion**](https://arxiv.org/pdf/2305.03653) involves augmenting the original query with additional terms or synonyms to improve recall and ensure more comprehensive retrieval results.\\n  + [**Self-querying**](https://towardsdatascience.com/how-to-build-a-rag-system-with-a-self-querying-retriever-in-langchain-16b4fa23e9ad) leverages the system’s ability to reformulate queries based on initial results, refining the search to better align with user intent and improve precision.\\n\\n### Generation Stage\\n\\nAfter optimizing the retrieval stage, the generation stage is the next critical component in an RAG system. This stage involves deploying and managing the Large Language Model (LLM) that generates responses based on the retrieved context. Let’s explore key deployment strategies for this vital component.\\n\\n#### LLM Deployment and Scaling\\n\\nEfficient LLM deployment is crucial for maintaining low latency and high throughput in RAG systems:\\n\\n* **Managed Services**: Leverage managed services like OpenAI API or Azure OpenAI Service for scalable, production-ready LLM deployment. These services handle infrastructure management, allowing teams to focus on application logic.\\n* **Self-Hosted Solutions**: For organizations requiring more control or facing data privacy constraints, consider deploying LLMs using frameworks like DeepSpeed or Megatron-LM. These frameworks enable efficient distributed inference across multiple GPUs or nodes.\\n* **Load Balancing**: Implement intelligent load balancing to distribute requests across multiple LLM instances. This approach enhances system reliability and enables seamless scaling to handle varying workloads.\\n\\n#### Optimizing LLM Performance\\n\\nTo maximize the efficiency of your LLM deployment:\\n\\n* **Caching Mechanisms**: Implement robust caching strategies to store frequent query results. This can significantly reduce the load on your LLM and improve response times. Consider using distributed caching solutions like Redis or Memcached for scalability.\\n* **Batching**: Utilize [batching techniques](https://www.anyscale.com/blog/continuous-batching-llm-inference) to process multiple queries simultaneously, improving overall throughput. This is particularly effective when using GPU acceleration.\\n* **Model Quantization**: Apply quantization techniques to reduce model size and inference latency without significant accuracy loss. This can be particularly beneficial for edge deployments or resource-constrained environments.\\n\\n#### LLM  Security\\n\\nImplementing robust security measures is critical for the production of RAG systems. Coralogix, a state-of-the-art platform for AI observability, offers top-of-the-shelf solutions to address these security challenges in your RAG systems.\\n\\n* **Input Validation and Prompt Injection Prevention:** Develop comprehensive input validation and sanitization processes to protect against potential security vulnerabilities or adversarial attacks. Utilize Coralogix’s real-time prompt injection capabilities to alert you to malicious inputs. inputs.\\n* **Output Filtering and Content Moderation:** Implement content filtering mechanisms to ensure generated responses adhere to predefined safety and appropriateness guidelines. Leverage Coralogix’s advanced evaluator catalog, which includes pre-built evaluators for toxicity detection, PII protection and SQL injection prevention.\\n\\n**Real-time Monitoring and Alerting:** Employ Coralogix’s AI observability platform to monitor model inputs, outputs, and performance metrics in real time. Utilize Coralogix’s overview dashboard for live, actionable insights and analytical summaries of your RAG system’s performance.\\n\\n* **Compliance and Governance:** Leverage Coralogix’s capabilities to help ensure compliance with regulations such as the EU AI Act, which will be enforceable from August 2026.\\n\\nBy integrating Coralogix’s advanced observability solution, you can significantly enhance your RAG systems’ security, reliability, and compliance. It provides the visibility and control necessary to scale AI applications in production environments confidently.\\n\\n## API Layer\\n\\nWhen deploying an RAG system in production, the API layer requires special considerations beyond standard API development practices. Here are key strategies specific to RAG systems:\\n\\n### RAG-Specific API Design\\n\\n* **Context-Aware Endpoints**: Design endpoints that allow clients to provide additional context or constraints for the RAG process. This could include specifying the types of sources to prioritize or setting relevance thresholds.\\n* **Streaming Responses**: Implement API capabilities to handle long-running RAG queries, allowing for progressive result delivery and early termination if needed.\\n* **Feedback Loops**: Design endpoints for capturing user feedback on RAG responses, which can be crucial for continuous system improvement.\\n\\n### Caching Strategies\\n\\n* **Embedding Cache**: Implement caching for document embeddings to reduce computation time on frequent queries.\\n* **Result Cache**: Cache final RAG responses for common queries but implement intelligent cache invalidation based on knowledge base updates.\\n* **Distributed caching** can significantly reduce latency in RAG systems by storing frequently accessed embeddings or retrieval results.\\n\\n### Error Handling and Fallbacks\\n\\n* **Graceful Degradation**: Design your API to handle failures in different parts of the RAG pipeline gracefully. For instance, if retrieval fails, return to a pure generation approach.\\n* **Confidence Scores**: Include confidence scores or uncertainty estimates in API responses to help clients make informed decisions about using the results.\\n\\n### Versioning for RAG Components\\n\\n* **Model Versioning**: Implement versioning for the API and the underlying retrieval and generation models. This allows clients to specify or be aware of which model versions are being used.\\n* **Knowledge Base Versioning**: Consider versioning your knowledge base to allow reproducible results and support A/B testing of different knowledge base configurations.\\n\\n### Monitoring and Observability for RAG\\n\\n* **RAG-Specific Metrics**: Implement monitoring for RAG-specific metrics such as retrieval accuracy, generation quality, and end-to-end response relevance.\\n* **Component-Level Tracing**: Implement tracing that allows you to break down the performance and behavior of each component in the RAG pipeline (retrieval, context integration, generation).\\n\\nWhile these strategies focus on RAG-specific concerns, it’s important to note that building a resilient API layer for any software system, including RAG, also involves addressing:\\n\\n* Authentication and authorization\\n* Rate limiting and traffic management\\n* General API design principles and documentation\\n* Overall system monitoring and alerting\\n* Scalability and performance optimization\\n\\nThese aspects, while crucial, are not unique to RAG systems and should be implemented according to industry best practices.\\n\\n## Orchestration\\n\\nOrchestration in RAG systems involves coordinating various components, including document retrieval, embedding generation, and language model inference. Effective orchestration ensures scalability, reliability, and performance in production environments.\\n\\n### Containerization and Kubernetes\\n\\n**Recommendation:** Use Kubernetes for container orchestration and automated scaling.\\n\\n**Rationale:**\\n\\n* Kubernetes provides robust container orchestration, which is crucial for managing the complex, multi-component nature of the RAG system.\\n* It offers automated scaling capabilities, which are essential for handling varying loads in RAG applications, particularly in retrieval and generation components.\\n* Kubernetes facilitates easy deployment and updates of individual RAG components without system-wide downtime.\\n\\n### Event-Driven Architecture\\n\\n**Recommendation:** Adopt an event-driven architecture using technologies like Apache Kafka or RabbitMQ.\\n\\n**Rationale:**\\n\\n* Event-driven architectures allow for asynchronous processing in RAG pipelines, enhancing system responsiveness.\\n* They facilitate real-time updates to the knowledge base, ensuring that the retrieval component always has access to the latest information.\\n* This approach enables better scalability and decoupling of RAG components, allowing each to scale independently based on demand.\\n\\n### Auto-scaling Strategies\\n\\n**Recommendation:** Implement custom auto-scaling strategies based on RAG-specific metrics.\\n\\n**Rationale:**\\n\\n* Standard CPU/memory-based auto-scaling may not be sufficient for RAG systems due to the varying computational demands of different queries.\\n* Custom metrics, such as query complexity or retrieval time, can trigger more intelligent scaling decisions.\\n* This approach ensures optimal resource utilization and cost-efficiency in production RAG deployments.\\n\\n|  |  |  |\\n| --- | --- | --- |\\n| **Consideration** | **Implementation** | **Benefits for RAG Systems** |\\n| Container Orchestration | Kubernetes | Scalability, component isolation, easy updates |\\n| Service Communication | Service Mesh (e.g., Istio) | Enhanced observability, traffic management |\\n| Asynchronous Processing | Event-Driven Architecture | Real-time updates, scalability |\\n| Intelligent Scaling | Custom Auto-scaling | Optimized resource utilization |\\n\\n## Conclusion\\n\\nRAG deployment presents challenges in query handling, latency management, and retrieval accuracy. Implementing optimized vector databases, efficient LLM deployments, and specialized APIs can significantly enhance AI capabilities.\\n\\nA significant trend is the shift towards modular RAG frameworks, which decompose complex systems into independent modules and specialized operators, allowing for a highly reconfigurable architecture. This modular approach transcends traditional linear designs by integrating routing, scheduling, and fusion mechanisms, thus facilitating more flexible and efficient deployments.\\n\\nThe strategies outlined provide a foundation for implementing production-ready RAG systems. As context-aware AI advances, the effective deployment of RAG systems will likely become a key differentiator in various industries.\\n\\n## FAQ\\n\\n### What are the main challenges in deploying RAG systems at scale?\\n\\nQuery diversity, retrieval accuracy, latency management, and maintaining up-to-date content.\\n\\n### How can retrieval performance be optimized in RAG systems?\\n\\nImplementing distributed vector databases, efficient embedding models, and multi-tiered caching strategies.\\n\\n### What are the key considerations for LLM deployment in RAG systems?\\n\\nBalancing between managed services and self-hosted solutions, implementing load balancing, and optimizing for query complexity.\\n\\n### How should APIs be designed for production RAG systems?\\n\\nWith streaming capabilities, context-aware endpoints, robust error handling, and feedback loops for continuous improvement.\\n\\n### What role does orchestration play in RAG system deployment?\\n\\nIt coordinates components, enables auto-scaling based on RAG-specific metrics, and ensures system reliability and performance.\\n\\n## References\\n\\n1. <https://arxiv.org/abs/2404.04044>\\n2. <https://arxiv.org/abs/2404.19543>\\n3. <https://arxiv.org/abs/2407.07321>\\n4. <https://arxiv.org/abs/2407.15748>\\n5. <https://arxiv.org/abs/2405.07437>\\n6. <https://arxiv.org/abs/2202.01110>\\n7. <https://arxiv.org/abs/2402.05131>\\n8. <https://www.semanticscholar.org/paper/From-punched-cards-to-Google%3A-an-outline-history-of-Gilchrist/16138f57fffbbad032999d156ce10f703086a3cd>\\n9. <https://haystack.deepset.ai/blog/rag-deployment>\\n10. <https://www.databricks.com/glossary/retrieval-augmented-generation-rag>\\n11. <https://aws.amazon.com/what-is/retrieval-augmented-generation/>\\n12. <https://cloud.google.com/use-cases/retrieval-augmented-generation>\\n13. <https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/>\\n14. <https://haystack.deepset.ai/blog/rag-deployment>\\n15. <https://www.protecto.ai/blog/rag-production-deployment-strategies-practical-considerations>\\n16. <https://developer.nvidia.com/blog/how-to-take-a-rag-application-from-pilot-to-production-in-four-steps/>\\n17. <https://www.tonic.ai/blog/top-5-trends-in-enterprise-rag-in-2024>\\n18. https://stg-Coralogix-staging.kinsta.cloud/platform/\\n\\nShare article\\n\\nOn this page\\n\\n---\\n\\n## Related articles\\n\\n[**Building AI systems**\\n\\n### Ensuring Accuracy, Reliability, and Trust\\n\\nWhat is GenAI Observability? Not too long ago, identifying performance issues in systems was a relatively simple task. But as technology advances, systems become more complex, turning simple...\\n\\n18 mins read\\nRead Now](https://coralogix.com/ai-blog/ensuring-accuracy-reliability-and-trust/)\\n[**Building AI systems**\\n\\n### Step by Step: Building a RAG Chatbot with Minor Hallucinations\\n\\nIn the rapidly evolving landscape of artificial intelligence, Retrieval Augmented Generation (RAG) has emerged as...\\n\\n11 mins read\\nRead Now](https://coralogix.com/ai-blog/step-by-step-building-a-rag-chatbot-with-minor-hallucinations/)\\n[**Building AI systems**\\n\\n### Build vs Buy: How to Choose the Right Path for Your GenAI App’s Guardrails\\n\\nIn May 2023, Samsung employees unintentionally disclosed confidential source code by inputting it into ChatGPT,...\\n\\n7 mins read\\nRead Now](https://coralogix.com/ai-blog/build-vs-buy-how-to-choose-the-right-path-for-your-genai-apps-guardrails/)\"}, {\"url\": \"https://medium.com/@sahin.samia/modular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372\", \"title\": \"Modular RAG using LLMs: What is it and how does it work? - Medium\", \"content\": \"This pattern closely resembles the traditional RAG process but benefits from the modular approach by allowing individual modules to be optimized or replaced without altering the overall flow. For example, a linear RAG flow might begin with a query expansion module to refine the user’s input, followed by the retrieval module, which fetches the most relevant data chunks. As Retrieval-Augmented Generation (RAG) systems become increasingly complex and diverse, fine-tuning the individual components — such as retrievers and generators — has become essential for optimizing performance. The Modular RAG framework embraces this need through various tuning patterns that enhance the system’s ability to adapt to specific tasks and datasets. Managing these different data types and ensuring seamless integration within the RAG system can be difficult, requiring sophisticated data processing and retrieval strategies​(modular rag paper).\", \"score\": 0.60049355, \"raw_content\": \"[Sitemap](/sitemap/sitemap.xml)\\n\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd482ebb3d372&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\n[Sign in](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40sahin.samia%2Fmodular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[Sign in](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40sahin.samia%2Fmodular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n# Modular RAG using LLMs: What is it and how does it work?\\n\\n[Sahin Ahmed, Data Scientist](/@sahin.samia?source=post_page---byline--d482ebb3d372---------------------------------------)\\n\\n17 min readAug 14, 2024\\n\\n## Introduction\\n\\nLarge Language Models (LLMs) have taken AI to new heights, but they still stumble on challenges like hallucinations and outdated information. Retrieval-Augmented Generation (RAG) has been a game-changer, adding external knowledge to boost LLM performance. However, traditional RAG systems, with their rigid “retrieve-then-generate” process, often struggle with complex queries and diverse data sources.\\n\\nThis is where Modular RAG steps in. Imagine building with LEGO blocks — each piece independent yet part of a bigger, more flexible system. Modular RAG rethinks RAG systems by breaking them down into reconfigurable modules, offering the adaptability needed for today’s AI challenges. In this post, we’ll explore how Modular RAG works and why it’s set to reshape the future of AI.\\n\\n## Section 1: Understanding Traditional RAG Systems\\n\\n**1.1 The Basics of RAG**\\n\\nRetrieval-Augmented Generation (RAG) has significantly enhanced the capabilities of Large Language Models (LLMs) by providing access to external knowledge sources. This process allows LLMs to perform better in tasks that require in-depth knowledge, such as answering complex questions, making recommendations, and supporting customer service applications. The basic idea is simple: retrieve relevant information from a vast database and then use that information to generate a response. This “retrieve-then-generate” process has been the backbone of RAG systems since their inception.\\n\\nInitially, RAG systems were fairly straightforward, relying on basic indexing, retrieval, and generation processes. This approach, often referred to as “Naive RAG,” focused primarily on matching the semantic similarity between a user’s query and chunks of text from a database. However, as the complexity of tasks and the volume of data increased, the limitations of this simplistic approach became apparent.\\n\\n**1.2 Challenges with Traditional RAG**\\n\\nDespite the success of traditional RAG systems, they face several key challenges that limit their effectiveness in more demanding scenarios:\\n\\n* **Shallow Query Understanding:** Traditional RAG systems often struggle to fully grasp the nuances of complex queries. The reliance on basic similarity measures between a query and document chunks can result in superficial matches that fail to capture the deeper relationships within the data​.\\n* **Retrieval Redundancy and Noise:** Feeding all retrieved chunks directly into LLMs can overwhelm the system with redundant or irrelevant information. This not only wastes computational resources but also increases the risk of generating incorrect or hallucinated responses. The challenge is to filter out noise while preserving the most relevant data​.\\n* **Linear Process Limitations:** The linear “retrieve-then-generate” flow of traditional RAG systems doesn’t always align well with the needs of complex applications. For example, handling multifaceted queries or integrating diverse data sources often requires a more flexible, non-linear approach. Advanced RAG methods have attempted to address these issues through techniques like query rewriting and reranking, but the rigid structure still imposes significant constraints​.\\n\\nAs RAG technology continues to evolve, the growing complexity of tasks has driven the need for a more sophisticated and adaptable system — one that can break free from the limitations of a linear process and better handle the demands of real-world applications. This need has led to the development of the Modular RAG framework, which we’ll explore in the next section.\\n\\n## Section 2: The Emergence of Modular RAG\\n\\n**2.1 Introducing Modular RAG**\\n\\nAs RAG systems grew more complex, the traditional “retrieve-then-generate” approach began to show its limitations. The increasing demands of modern AI applications called for a more flexible and scalable framework — one that could adapt to the complexities of real-world tasks without being bogged down by the rigidities of a linear process. This is where Modular RAG comes in.\\n\\nModular RAG reimagines the RAG architecture by breaking it down into independent, yet interconnected modules. Think of it as building with LEGO blocks — each module can be independently developed, optimized, and then combined with others to create a highly adaptable system. This modular design allows for a much more flexible approach, where different modules can be reconfigured or swapped out depending on the specific needs of the task at hand​.\\n\\n**2.2 The Three-Tiered Architecture of Modular RAG**\\n\\nThe Modular RAG framework is organized into a three-tiered architecture, each tier serving a distinct role in the overall system:\\n\\n* **Modules:** At the top level, the system is divided into high-level stages such as indexing, retrieval, and generation. Each of these stages is treated as a standalone module, which can be independently controlled and orchestrated. This approach not only inherits the main processes from traditional RAG systems but also introduces an orchestration module that manages the coordination of these processes​.\\n* **Sub-modules:** Within each module, there are sub-modules that handle more specific tasks. For example, within the retrieval module, sub-modules might handle query expansion, reranking, or filtering. These sub-modules allow for more granular control over the RAG process, enabling the system to fine-tune its operations based on the specific requirements of the task​.\\n* **Operators:** At the most granular level, Modular RAG includes operators, which are the basic functional units within each sub-module. These operators carry out specific functions like embedding generation, similarity computation, and document chunking. By modularizing these operators, the system can be easily reconfigured or extended with new functionality as needed​.\\n\\n## Section 3: Modules and Operators in Modular RAG\\n\\n**3.1 Understanding Modules in Modular RAG**\\n\\nIn the Modular RAG framework, the concept of “modules” is central to how the system is structured and operates. Each module represents a high-level stage in the RAG process, handling specific tasks that are essential for the system’s overall functionality. The key modules within this framework include:\\n\\n* **Indexing Module:** This module is responsible for organizing and structuring the vast amounts of data that the RAG system will retrieve from. It involves breaking down documents into manageable chunks, creating embeddings, and storing these in a way that allows for efficient retrieval. One of the challenges addressed by this module is the accurate representation of content, ensuring that the system can effectively match queries with the most relevant chunks​.\\n* **Retrieval Module:** The retrieval module plays a critical role in selecting the most relevant data chunks based on the user’s query. This module can include sub-modules for tasks like query expansion, reranking of retrieved chunks, and filtering out noise. By refining the retrieval process, this module ensures that the system returns the most useful and contextually appropriate information for subsequent generation​.\\n* **Generation Module:** Once the relevant data has been retrieved, the generation module comes into play. This module utilizes the retrieved data to generate the final output, whether it’s a response to a query, a summary, or any other type of content. The generation module may also include sub-modules for fine-tuning the output, ensuring that it is accurate, coherent, and aligned with the user’s needs​.\\n* **Orchestration Module:** Perhaps the most innovative aspect of Modular RAG is the orchestration module, which manages the coordination between different modules. This module controls the flow of data and tasks within the system, deciding which modules to activate based on the specific requirements of the query. By orchestrating the process, this module allows the system to adapt dynamically to different scenarios, making it more versatile and efficient​.\\n\\n**3.2 The Role of Operators in Modular RAG**\\n\\nAt a finer level of granularity, the Modular RAG framework is built on “operators,” which are the basic functional units within each module. Operators are essentially the building blocks of the system, performing specific tasks that contribute to the larger goals of each module. Here’s how operators function within the Modular RAG system:\\n\\n* **Embedding Operators:** Within the indexing module, embedding operators are responsible for converting document chunks into vector representations. These embeddings capture the semantic essence of the text, enabling the retrieval module to efficiently match queries with the most relevant data​.\\n* **Similarity Computation Operators:** In the retrieval module, similarity computation operators calculate how closely a query matches with different document chunks. This process often involves comparing the embeddings of the query and the document chunks using methods like cosine similarity or dot product. The results of these computations are crucial for determining which chunks are most relevant and should be passed on to the generation module​.\\n* **Query Expansion and Transformation Operators:** These operators work within the pre-retrieval process to refine and enhance the user’s query. Query expansion operators might generate multiple versions of a query to improve retrieval coverage, while query transformation operators can rewrite or adjust queries to better align with the system’s retrieval capabilities. This step is vital for improving the accuracy and relevance of the retrieved data​.\\n* **Chunk Compression and Selection Operators:** After retrieval, the system often needs to manage large volumes of data. Chunk compression operators help reduce the size of the retrieved data without losing important information, making it easier for the generation module to process. Meanwhile, chunk selection operators filter out irrelevant or redundant information, ensuring that only the most pertinent data is used in the final output​.\\n\\n**3.3 Interplay Between Modules and Operators**\\n\\nThe real power of Modular RAG lies in how these modules and operators interact. By breaking down the RAG process into discrete, reconfigurable components, Modular RAG allows for greater flexibility and adaptability. For example, if a particular task requires more detailed retrieval, the orchestration module can prioritize operators that enhance retrieval precision, such as reranking and query transformation operators.\\n\\nMoreover, this modularity facilitates easier updates and maintenance. If a new method for similarity computation becomes available, it can be integrated as a new operator within the existing retrieval module without overhauling the entire system. This ability to adapt and evolve makes Modular RAG a forward-looking approach to handling complex, knowledge-intensive tasks in AI​\\n\\n## Section 4: Common Flow Patterns in Modular RAG\\n\\nThe Modular RAG framework’s flexibility and scalability come to life through its ability to support a variety of RAG flow patterns. These patterns determine how different modules and operators interact, allowing the system to adapt to a wide range of tasks and complexities. By breaking free from the traditional linear “retrieve-then-generate” model, Modular RAG introduces several innovative patterns that cater to different scenarios.\\n\\n**4.1 The Linear Pattern**\\n\\nThe linear pattern is the most straightforward RAG flow, where modules are arranged in a fixed, sequential order. This pattern closely resembles the traditional RAG process but benefits from the modular approach by allowing individual modules to be optimized or replaced without altering the overall flow.\\n\\nFor example, a linear RAG flow might begin with a query expansion module to refine the user’s input, followed by the retrieval module, which fetches the most relevant data chunks. These chunks are then passed through a post-retrieval reranking module to prioritize the most useful information, and finally, the generation module produces the output. This pattern is ideal for tasks where the process can be clearly defined from start to finish without needing dynamic adjustments​.\\n\\n**4.2 The Conditional Pattern**\\n\\nThe conditional pattern adds flexibility by allowing the RAG flow to branch based on specific conditions. This means that depending on the nature of the query or the retrieved data, different modules can be activated or bypassed.\\n\\nFor instance, a routing module might analyze a query and determine whether it requires standard retrieval or more complex processing involving additional modules like query rewriting or chunk compression. This pattern is particularly useful for handling diverse queries that may require different levels of processing complexity. The system’s ability to conditionally route tasks ensures that resources are used efficiently, and the most appropriate processing path is followed​\\n\\n**4.3 The Branching Pattern**\\n\\nIn the branching pattern, multiple RAG processes run in parallel, each handling different aspects of a query or processing different types of data. After each branch completes its task, the results are merged to form a comprehensive final output.\\n\\n## Get Sahin Ahmed, Data Scientist’s stories in your inbox\\n\\nJoin Medium for free to get updates from this writer.\\n\\nThis pattern is especially useful in scenarios where a query might need to pull in information from various sources or when the system needs to explore multiple hypotheses simultaneously. For example, in a multi-faceted query, one branch could focus on retrieving data related to the primary query, while another branch might retrieve background information or context from different data sources. The results from these branches are then combined, providing a richer and more nuanced response​.\\n\\n**4.4 The Looping Pattern**\\n\\nThe looping pattern introduces iterative processes where the system can revisit earlier steps in the RAG flow to refine its output. This pattern is particularly powerful when dealing with complex queries that require multiple rounds of retrieval and generation to fully address the user’s needs.\\n\\nIn a looping RAG flow, the system might generate an initial response, then evaluate whether additional retrieval is necessary. If the response is incomplete or could benefit from more information, the system loops back to the retrieval module to fetch additional data, which is then incorporated into a revised output. This iterative approach allows the system to gradually build up a more accurate and detailed response, making it well-suited for complex tasks that cannot be fully resolved in a single pass​\\n\\n**4.5 Adaptive (Active) Retrieval**\\n\\nOne of the most advanced patterns in Modular RAG is adaptive or active retrieval. This pattern allows the system to dynamically decide when and how to perform additional retrieval based on the ongoing generation process.\\n\\nFor example, during the generation phase, the system might encounter a gap in the information that it cannot resolve with the data at hand. Instead of proceeding with an incomplete response, the system can trigger a new round of retrieval, fetching the necessary data to fill in the gaps before continuing with the generation. This adaptability ensures that the system can handle unexpected complexities and provide more accurate and relevant outputs​\\n\\n**4.6 Fusion and Integration Patterns**\\n\\nFinally, the fusion pattern is crucial for integrating multiple branches or different types of data into a single, cohesive output. After data from various branches or retrieval rounds has been processed, the fusion module merges these results, ensuring that the final output is both comprehensive and coherent.\\n\\nFor instance, in a scenario where a query has been processed by several parallel branches, each focusing on different aspects of the query, the fusion module would combine the results from these branches, eliminating redundancies and resolving any conflicts to present a unified response. This pattern is essential for complex queries that require a broad range of information to be synthesized into a single, actionable output​.\\n\\n**4.7 Choosing the Right Pattern**\\n\\nThe choice of pattern in Modular RAG depends on the specific needs of the task at hand. Simple tasks may benefit from the linear pattern, while more complex scenarios might require conditional branching, looping, or adaptive retrieval. The modular nature of the framework means that different patterns can be combined or adjusted as needed, providing a tailored approach that maximizes efficiency and effectiveness in handling diverse queries.\\n\\n## Section 5: Tuning Patterns in Modular RAG\\n\\nAs Retrieval-Augmented Generation (RAG) systems become increasingly complex and diverse, fine-tuning the individual components — such as retrievers and generators — has become essential for optimizing performance. The Modular RAG framework embraces this need through various tuning patterns that enhance the system’s ability to adapt to specific tasks and datasets. These tuning patterns focus on refining the system’s components to achieve better alignment, accuracy, and efficiency.\\n\\n**5.1 Retriever Fine-Tuning**\\n\\nThe retriever is a crucial component in any RAG system, responsible for selecting the most relevant data chunks from a vast repository based on the user’s query. Fine-tuning the retriever is essential to improve retrieval accuracy, particularly when dealing with specialized domains or tasks that require a deep understanding of nuanced information.\\n\\nSeveral methods are employed for retriever fine-tuning within the Modular RAG framework:\\n\\n* **Supervised Fine-Tuning (SFT):** This involves training the retriever using a labeled dataset specific to the domain or task at hand. By reducing the distance between positive samples (relevant chunks) and increasing the distance from negative samples (irrelevant chunks), SFT enhances the retriever’s ability to distinguish between valuable and non-valuable information​.\\n* **LM-Supervised Retrieval (LSR):** In this approach, the retriever is fine-tuned based on feedback from a language model (LM). The LM generates supervisory signals during the RAG process, guiding the retriever to prioritize chunks that are most likely to contribute to a high-quality final output​.\\n* **Adapter Modules:** Sometimes, fine-tuning large retrievers directly can be resource-intensive. Instead, adding an adapter module — a smaller, trainable component — can enhance the retriever’s performance with less computational overhead. This approach is particularly useful when dealing with domain-specific tasks where full retriever fine-tuning may not be feasible​.\\n\\n**5.2 Generator Fine-Tuning**\\n\\nThe generator, which produces the final output based on the retrieved data, also benefits from fine-tuning. Fine-tuning the generator helps align its outputs with the specific requirements of the task, whether it’s generating text that follows a particular style or ensuring that the content is factually accurate.\\n\\nKey methods for generator fine-tuning in Modular RAG include:\\n\\n* **Instruct-Tuning:** This method involves fine-tuning the generator using domain-specific datasets or task-specific instructions. By providing additional knowledge or adjusting the input-output formats, instruct-tuning ensures that the generator produces outputs that are better suited to the task’s needs​.\\n* **Reinforcement Learning from Human or LM Feedback (RLHF):** This approach uses reinforcement learning to align the generator’s outputs with human preferences or feedback from a more advanced language model. By iteratively refining the generator based on feedback, this method ensures that the final outputs are not only accurate but also aligned with desired quality standards​.\\n* **Dual Fine-Tuning:** In some scenarios, it’s beneficial to fine-tune both the retriever and the generator simultaneously. This approach ensures that both components are aligned in terms of their preferences and outputs, leading to a more cohesive and effective RAG system. A typical method involves aligning the scoring functions between the retriever and the generator using techniques like KL-Divergence, ensuring that both components are working towards the same goal​\\n\\n**5.3 Benefits of Tuning Patterns in Modular RAG**\\n\\nThe tuning patterns in Modular RAG offer several advantages:\\n\\n* **Improved Accuracy:** By fine-tuning the retriever and generator, the system can more accurately match queries with relevant data and produce higher-quality outputs. This is especially important in specialized domains where precision is critical​.\\n* **Task-Specific Optimization:** Tuning allows the system to be customized for specific tasks, ensuring that both the retrieval and generation processes are tailored to the unique requirements of each application. This results in more relevant and useful outputs​.\\n* **Enhanced System Cohesion:** Dual fine-tuning ensures that the retriever and generator are well-aligned, reducing the likelihood of disjointed or incoherent outputs. This leads to a more seamless and integrated RAG system​.\\n\\n**5.4 Application of Tuning Patterns**\\n\\nThe choice of tuning pattern depends on the specific needs of the task. For instance, if the task involves handling highly specialized queries, supervised fine-tuning of the retriever may be necessary. If the goal is to produce outputs that adhere to specific stylistic or factual standards, instruct-tuning or RLHF for the generator might be more appropriate.\\n\\nIn some cases, a combination of these tuning methods may be required. For example, a RAG system deployed in a legal context might use supervised fine-tuning to enhance the retriever’s ability to identify relevant case law, while also applying instruct-tuning to ensure that the generated legal summaries are accurate and appropriately formatted.\\n\\n## Section 6: Challenges and Opportunities in Modular RAG\\n\\nAs promising as the Modular RAG framework is, implementing it effectively in real-world scenarios comes with its own set of challenges. However, these challenges also present unique opportunities for innovation and further development in the field of Retrieval-Augmented Generation.\\n\\n**6.1 Challenges in Implementing Modular RAG**\\n\\n* **Complex Data Integration:** One of the primary challenges in Modular RAG is integrating complex and diverse data sources. Traditional RAG systems often rely on a single type of unstructured text data, but Modular RAG expands this to include semi-structured data like tables and structured data such as knowledge graphs. Managing these different data types and ensuring seamless integration within the RAG system can be difficult, requiring sophisticated data processing and retrieval strategies​(modular rag paper).\\n* **System Orchestration and Workflow Management:** The modularity of the RAG framework introduces a new layer of complexity in orchestrating and managing workflows. Unlike traditional linear RAG systems, Modular RAG involves routing, scheduling, and decision-making processes that require precise control over how modules interact. Designing an effective orchestration module that can dynamically adjust workflows based on the task’s requirements is critical, but also challenging​.\\n* **Component Selection and Optimization:** With the introduction of multiple modules and operators, selecting the right components for a specific task becomes crucial. Each module or operator must be optimized not only to perform well individually but also to work efficiently with other components in the system. Ensuring that these components are well-integrated and collectively enhance system performance requires careful design and continuous optimization.\\n* **Maintenance and Debugging:** As Modular RAG systems grow in complexity, maintaining and debugging them can become more difficult. Isolating issues within a highly modular system requires sophisticated monitoring and diagnostic tools that can pinpoint problems at the module or operator level. Additionally, as new modules or operators are added, ensuring compatibility and maintaining system stability becomes an ongoing challenge​.\\n\\n**6.2 Opportunities for Innovation in Modular RAG**\\n\\n* **Scalability and Flexibility:** The modular nature of RAG systems offers significant opportunities for scalability. As tasks become more complex or as new data sources become available, additional modules or operators can be introduced without overhauling the entire system. This flexibility allows Modular RAG to scale with the growing demands of AI applications, making it a future-proof solution​.\\n* **Enhanced Personalization and Customization:** Modular RAG allows for highly personalized and customized solutions tailored to specific use cases. By selecting and configuring modules to meet the unique needs of a task or domain, developers can create RAG systems that are more effective and efficient than one-size-fits-all solutions. This customization potential opens up new avenues for specialized applications in fields like healthcare, finance, and law​.\\n* **Innovation in Operator Design:** The introduction of operators as basic functional units within modules provides a fertile ground for innovation. New operators can be developed to handle emerging challenges in data retrieval and generation, such as handling more complex queries, improving the accuracy of embeddings, or optimizing query transformation. As the field of AI evolves, the ability to innovate at the operator level ensures that Modular RAG remains at the cutting edge of technology​.\\n* **Integration with Emerging Technologies:** Modular RAG’s compatibility with emerging technologies such as knowledge graphs, reinforcement learning, and adaptive retrieval processes offers significant opportunities for advancement. By integrating these technologies into the RAG framework, developers can create more intelligent and adaptive systems that can better handle the complexities of real-world applications​.\\n* **Research and Development:** The modular approach of RAG systems encourages experimentation and research, particularly in optimizing the interaction between modules and operators. Researchers can explore new configurations, develop novel algorithms for specific modules, and refine orchestration strategies to push the boundaries of what RAG systems can achieve. This opens up vast opportunities for academic research and industry innovation​.\\n\\n## Conclusion\\n\\nModular RAG represents a significant leap forward in the evolution of Retrieval-Augmented Generation systems. By breaking down complex processes into independent, reconfigurable modules and operators, it offers unparalleled flexibility, scalability, and adaptability. While challenges such as data integration, workflow orchestration, and system maintenance present obstacles, they also pave the way for innovation. As AI continues to advance, Modular RAG provides a robust framework that is well-equipped to meet the growing demands of knowledge-intensive tasks, making it a key player in the future of AI-driven solutions.\\n\\n## References:\\n\\n1. Gao, Yunfan, et al. “Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks.” *arXiv preprint arXiv:2407.21059* (2024).\\n2. [https://medium.com/@yufan1602/modular-rag-and-rag-flow-part-%E2%85%B0-e69b32dc13a3](/@yufan1602/modular-rag-and-rag-flow-part-%E2%85%B0-e69b32dc13a3)\\n\\n[Artificial Intelligence](/tag/artificial-intelligence?source=post_page-----d482ebb3d372---------------------------------------)\\n\\n[Llm](/tag/llm?source=post_page-----d482ebb3d372---------------------------------------)\\n\\n[AI](/tag/ai?source=post_page-----d482ebb3d372---------------------------------------)\\n\\n[ChatGPT](/tag/chatgpt?source=post_page-----d482ebb3d372---------------------------------------)\\n\\n[NLP](/tag/nlp?source=post_page-----d482ebb3d372---------------------------------------)\\n\\n[## Written by Sahin Ahmed, Data Scientist](/@sahin.samia?source=post_page---post_author_info--d482ebb3d372---------------------------------------)\\n\\n[1.1K followers](/@sahin.samia/followers?source=post_page---post_author_info--d482ebb3d372---------------------------------------)\\n\\n·[182 following](/@sahin.samia/following?source=post_page---post_author_info--d482ebb3d372---------------------------------------)\\n\\nLifelong learner passionate about AI, LLMs, Machine Learning, Deep Learning, NLP, and Statistical Modeling to make a meaningful impact. MSc in Data Science.\\n\\n## Responses (1)\\n\\nWrite a response\\n\\n[What are your thoughts?](/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40sahin.samia%2Fmodular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372&source=---post_responses--d482ebb3d372---------------------respond_sidebar------------------)\\n\\n[Damien Berezenko\\n\\nhe/him](/@qdrddr?source=post_page---post_responses--d482ebb3d372----0-----------------------------------)\\n\\n[Aug 19, 2024](/@qdrddr/sometimes-ft-is-not-an-option-and-a-feasible-alternative-is-semantic-layer-bfb2ed19ae06?source=post_page---post_responses--d482ebb3d372----0-----------------------------------)\\n\\n```\\n\\n\\nSometimes FT is not an option and a feasible alternative is Semantic Layer. Basically it’s a dynamically pulled from a Vector DB with a collection of examples used as context to help model make better decisions, that shows good results if you generate a large amount of good quality examples.\\n\\n\\n```\\n\\n4\\n\\n## More from Sahin Ahmed, Data Scientist\\n\\n[Sahin Ahmed, Data Scientist](/@sahin.samia?source=post_page---author_recirc--d482ebb3d372----0---------------------76e0c5b5_d7f0_42d6_94d0_3e23f4a4f8c7--------------)\\n\\n[## What is Reranking in Retrieval-Augmented Generation (RAG)?\\n\\n### Reranking in Retrieval-Augmented Generation (RAG) refers to the process of reordering or refining a set of initially retrieved documents…](/@sahin.samia/what-is-reranking-in-retrieval-augmented-generation-rag-ee3dd93540ee?source=post_page---author_recirc--d482ebb3d372----0---------------------76e0c5b5_d7f0_42d6_94d0_3e23f4a4f8c7--------------)\\n\\nNov 2, 2024\\n\\n[60\\n\\n1](/@sahin.samia/what-is-reranking-in-retrieval-augmented-generation-rag-ee3dd93540ee?source=post_page---author_recirc--d482ebb3d372----0---------------------76e0c5b5_d7f0_42d6_94d0_3e23f4a4f8c7--------------)\\n\\n[Sahin Ahmed, Data Scientist](/@sahin.samia?source=post_page---author_recirc--d482ebb3d372----1---------------------76e0c5b5_d7f0_42d6_94d0_3e23f4a4f8c7--------------)\\n\\n[## What is Retrieval-Augmented Generation(RAG) in LLM and How it works?](/@sahin.samia/what-is-retrieval-augmented-generation-rag-in-llm-and-how-it-works-a8c79e35a172?source=post_page---author_recirc--d482ebb3d372----1---------------------76e0c5b5_d7f0_42d6_94d0_3e23f4a4f8c7--------------)\\n\\nApr 22, 2024\\n\\n[217\\n\\n5](/@sahin.samia/what-is-retrieval-augmented-generation-rag-in-llm-and-how-it-works-a8c79e35a172?source=post_page---author_recirc--d482ebb3d372----1---------------------76e0c5b5_d7f0_42d6_94d0_3e23f4a4f8c7--------------)\\n\\n[Sahin Ahmed, Data Scientist](/@sahin.samia?source=post_page---author_recirc--d482ebb3d372----2---------------------76e0c5b5_d7f0_42d6_94d0_3e23f4a4f8c7--------------)\\n\\n[## Mastering Document Chunking Strategies for Retrieval-Augmented Generation (RAG)\\n\\n### Why Document Chunking is the Secret Sauce of RAG](/@sahin.samia/mastering-document-chunking-strategies-for-retrieval-augmented-generation-rag-c9c16785efc7?source=post_page---author_recirc--d482ebb3d372----2---------------------76e0c5b5_d7f0_42d6_94d0_3e23f4a4f8c7--------------)\\n\\nJan 22\\n\\n[28](/@sahin.samia/mastering-document-chunking-strategies-for-retrieval-augmented-generation-rag-c9c16785efc7?source=post_page---author_recirc--d482ebb3d372----2---------------------76e0c5b5_d7f0_42d6_94d0_3e23f4a4f8c7--------------)\\n\\n[Sahin Ahmed, Data Scientist](/@sahin.samia?source=post_page---author_recirc--d482ebb3d372----3---------------------76e0c5b5_d7f0_42d6_94d0_3e23f4a4f8c7--------------)\\n\\n[## The Math Behind DeepSeek: A Deep Dive into Group Relative Policy Optimization (GRPO)](/@sahin.samia/the-math-behind-deepseek-a-deep-dive-into-group-relative-policy-optimization-grpo-8a75007491ba?source=post_page---author_recirc--d482ebb3d372----3---------------------76e0c5b5_d7f0_42d6_94d0_3e23f4a4f8c7--------------)\\n\\nJan 26\\n\\n[393\\n\\n8](/@sahin.samia/the-math-behind-deepseek-a-deep-dive-into-group-relative-policy-optimization-grpo-8a75007491ba?source=post_page---author_recirc--d482ebb3d372----3---------------------76e0c5b5_d7f0_42d6_94d0_3e23f4a4f8c7--------------)\\n\\n[See all from Sahin Ahmed, Data Scientist](/@sahin.samia?source=post_page---author_recirc--d482ebb3d372---------------------------------------)\\n\\n## Recommended from Medium\\n\\nIn\\n\\n[Dev Genius](https://medium.com/dev-genius?source=post_page---read_next_recirc--d482ebb3d372----0---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\nby\\n\\n[Nishad Ahamed](/@n-ahamed36?source=post_page---read_next_recirc--d482ebb3d372----0---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\n[## RAG vs Agentic RAG: A Comprehensive Comparison\\n\\n### Introduction](/dev-genius/rag-vs-agentic-rag-a-comprehensive-comparison-500fd5b05ec0?source=post_page---read_next_recirc--d482ebb3d372----0---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\nMar 6\\n\\n[30](/dev-genius/rag-vs-agentic-rag-a-comprehensive-comparison-500fd5b05ec0?source=post_page---read_next_recirc--d482ebb3d372----0---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\n[Sourav Banerjee](/@souravbanerjee423?source=post_page---read_next_recirc--d482ebb3d372----1---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\n[## Adaptive Agentic RAG: Teaching AI to Think Before It Searches — Concepts\\n\\n### Teaching AI to Think: How Adaptive Agentic RAG Makes Decisions Like a Human](/@souravbanerjee423/adaptive-agentic-rag-teaching-ai-to-think-before-it-searches-a2dd65c80c45?source=post_page---read_next_recirc--d482ebb3d372----1---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\nAug 15\\n\\n[119](/@souravbanerjee423/adaptive-agentic-rag-teaching-ai-to-think-before-it-searches-a2dd65c80c45?source=post_page---read_next_recirc--d482ebb3d372----1---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\nIn\\n\\n[Level Up Coding](https://medium.com/gitconnected?source=post_page---read_next_recirc--d482ebb3d372----0---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\nby\\n\\n[Fareed Khan](/@fareedkhandev?source=post_page---read_next_recirc--d482ebb3d372----0---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\n[## Building Three Pipelines to Select the Right LLMs for RAG, Multi-Agent Systems, and Vision\\n\\n### Agentic RAG, Multi-Agent and Vision with Reasoning](/gitconnected/building-three-pipelines-to-select-the-right-llms-for-rag-multi-agent-systems-and-vision-3e47e0563b76?source=post_page---read_next_recirc--d482ebb3d372----0---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\n6d ago\\n\\n[738\\n\\n10](/gitconnected/building-three-pipelines-to-select-the-right-llms-for-rag-multi-agent-systems-and-vision-3e47e0563b76?source=post_page---read_next_recirc--d482ebb3d372----0---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\n[Algo Insights](/@algoinsights?source=post_page---read_next_recirc--d482ebb3d372----1---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\n[## RAG Isn’t Memory: These 5 Open-Source Engines Give AI Real Memory\\n\\n### People love to say “RAG gives AI memory.” But let’s be real: it doesn’t.](/@algoinsights/rag-isnt-memory-these-5-open-source-engines-give-ai-real-memory-2aa9890f4f48?source=post_page---read_next_recirc--d482ebb3d372----1---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\n2d ago\\n\\n[4](/@algoinsights/rag-isnt-memory-these-5-open-source-engines-give-ai-real-memory-2aa9890f4f48?source=post_page---read_next_recirc--d482ebb3d372----1---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\n[Rizqi Mulki](/@rizqimulkisrc?source=post_page---read_next_recirc--d482ebb3d372----2---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\n[## Advanced RAG Techniques: Hybrid Search, Reranking, and Graph RAG\\n\\n### Retrieval-Augmented Generation (RAG) has transformed how we build knowledge-intensive AI applications, enabling language models to access…](/@rizqimulkisrc/advanced-rag-techniques-hybrid-search-reranking-and-graph-rag-24f8d9b8d7e2?source=post_page---read_next_recirc--d482ebb3d372----2---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\nMay 29\\n\\n[2](/@rizqimulkisrc/advanced-rag-techniques-hybrid-search-reranking-and-graph-rag-24f8d9b8d7e2?source=post_page---read_next_recirc--d482ebb3d372----2---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\n[Ajay Kumar mahto](/@dancerworld60?source=post_page---read_next_recirc--d482ebb3d372----3---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\n[## “Mastering LangChain: A Deep Dive into Models, Prompts, Chains, Indexes, Memory, and Agents” 🚀](/@dancerworld60/mastering-langchain-a-deep-dive-into-models-prompts-chains-indexes-memory-and-agents-a76f1b2040e4?source=post_page---read_next_recirc--d482ebb3d372----3---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\nAug 9\\n\\n[3](/@dancerworld60/mastering-langchain-a-deep-dive-into-models-prompts-chains-indexes-memory-and-agents-a76f1b2040e4?source=post_page---read_next_recirc--d482ebb3d372----3---------------------0ccfc176_3bb1_4942_b885_4bf5a5705a19--------------)\\n\\n[See more recommendations](/?source=post_page---read_next_recirc--d482ebb3d372---------------------------------------)\\n\\n[Text to speech](https://speechify.com/medium?source=post_page-----d482ebb3d372---------------------------------------)\"}, {\"url\": \"https://www.kdjingpai.com/en/mokuaihua-rag-xitong/\", \"title\": \"Application Evaluation of Using Inference Models in Modular RAG ...\", \"content\": \"Mar 2, 2025·While the modular inference-based RAG did not show significant advantages over traditional processes within the context of Kapa.ai's use\", \"score\": 0.5063723, \"raw_content\": \"![AI生产力工具](https://www.kdjingpai.com/wp-content/uploads/2025/05/541866eb44ac15c.png)\\n![AI生产力工具](https://www.kdjingpai.com/wp-content/uploads/2025/05/94b85eeb24b781f.png)\\n![AI生产力工具](https://www.kdjingpai.com/wp-content/uploads/2025/05/541866eb44ac15c.png)\\n![AI生产力工具](https://www.kdjingpai.com/wp-content/uploads/2025/05/94b85eeb24b781f.png)\\n\\n# Application Evaluation of the Use of Inference Models in Modular RAG Systems\\n\\nIn this paper, we present Kapa.ai's recent work on OpenAI's Retrieval-Augmented Generation (RAG) system in the [o3-mini](https://www.kdjingpai.com/en/openai-o3-mini-xiban/) A summary report of the exploration of the etymological model of reasoning.\\n\\n![模块化 RAG 系统中推理模型的实际应用评估-1](https://www.kdjingpai.com/wp-content/uploads/2025/03/822a24cd4e3afe7.png \\\"模块化 RAG 系统中推理模型的实际应用评估-1\\\")\\n\\n![模块化 RAG 系统中推理模型的实际应用评估-1](https://www.kdjingpai.com/wp-content/uploads/2025/03/822a24cd4e3afe7.png \\\"模块化 RAG 系统中推理模型的实际应用评估-1\\\")\\n\\nKapa.ai is an AI assistant powered by a large-scale language model (LLM), which can be used by the [RAG](https://www.kdjingpai.com/en/rag/) The process is integrated with the knowledge base so that technical questions from users can be answered and technical support work orders can be processed.\\n\\nBuilding and maintaining a stable and versatile RAG system is not an easy task. Many parameters and settings affect the quality of the final output, and there are complex interactions between these factors:\\n\\nWhen making adjustments to a RAG system, especially when integrating a new model, revisiting and optimizing these parameters is critical to maintaining good performance. However, this task is not only time-consuming, but also requires a great deal of experience to do well.\\n\\nlook as if [DeepSeek-R1](https://www.kdjingpai.com/en/deepseek-r1nenglixiang/) Novel reasoning models, such as OpenAI's o3-mini, have achieved impressive results by using built-in Chain-of-Thought (CoT) prompts to \\\"think\\\" about a problem, reason step-by-step, and even self-correct when necessary. The models reportedly perform better on complex tasks that require logical reasoning and verifiable answers. Related reading:[DeepSeek R1 in RAG: Practical Experience Summary](https://www.kdjingpai.com/en/deepseek-r1-zai-rag/) ,[Project-level code generation results are in! o3/Claude 3.7 leads the way, R1 is in the top tier!](https://www.kdjingpai.com/en/xiangmujidaimashengcheng/)\\n\\nThus, Kapa.ai proposes an idea: if inference models are able to decompose complex problems and self-correct, could they be applied to RAG processes to handle tasks such as query expansion, document retrieval, and reordering? By building an information retrieval toolkit and handing it over to the inference model, it may be possible to build a more adaptive system that reduces the need for manual tuning of parameters.\\n\\nThis paradigm is sometimes called Modular Retrieval-Augmented Generation (Modular RAG). In this paper, we share Kapa.ai's recent research results in refactoring the standard RAG process into an inference model-based process.\\n\\n## suppose that...\\n\\nKapa.ai's main goal in exploring this idea is to simplify the RAG process and reduce the reliance on manual fine-tuning of parameters.The core components of the RAG process are dense embedding and document retrieval. A typical high-level RAG process is as follows:\\n\\n![模块化 RAG 系统中推理模型的实际应用评估-1](https://www.kdjingpai.com/wp-content/uploads/2025/03/1e9deccac26285a.png \\\"模块化 RAG 系统中推理模型的实际应用评估-1\\\")\\n\\n![模块化 RAG 系统中推理模型的实际应用评估-1](https://www.kdjingpai.com/wp-content/uploads/2025/03/1e9deccac26285a.png \\\"模块化 RAG 系统中推理模型的实际应用评估-1\\\")\\n\\nEach step in the process is optimized through heuristics such as filtering rules and sorting adjustments to prioritize relevant data. These hard-coded optimizations define the behavior of the process but also limit its adaptability.\\n\\nIn order for the inference model to utilize the various components of the RAG process, Kapa.ai needed to set up the system differently. Instead of defining a linear sequence of steps, each component is treated as a separate module for the model to call.\\n\\n![模块化 RAG 系统中推理模型的实际应用评估-1](https://www.kdjingpai.com/wp-content/uploads/2025/03/8e562cb9738a24d.png \\\"模块化 RAG 系统中推理模型的实际应用评估-1\\\")\\n\\n![模块化 RAG 系统中推理模型的实际应用评估-1](https://www.kdjingpai.com/wp-content/uploads/2025/03/8e562cb9738a24d.png \\\"模块化 RAG 系统中推理模型的实际应用评估-1\\\")\\n\\nIn this architecture, instead of following a fixed process, models with reasoning capabilities can dynamically control their workflow to a greater extent. By utilizing tools, models can decide when and how often to run full or simplified searches, and what search parameters to use. If successful, this approach has the potential to replace traditional RAG orchestration frameworks such as LangGraph.\\n\\nIn addition, a more modular system offers some additional benefits:\\n\\nFinally, Kapa.ai also wants to explore whether this approach can help \\\"short-circuit\\\" abusive or off-topic queries more effectively. The most difficult cases usually involve ambiguity, i.e., it is not clear whether the query is relevant to the product. Abusive queries are often deliberately designed to evade detection. While simpler cases can already be handled effectively, Kapa.ai hopes that the inference model will help identify and exit more complex problems earlier.\\n\\n## test setup\\n\\nTo experiment with this workflow, Kapa.ai built a sandboxed RAG system containing the necessary components, static data, and an evaluation suite with LLM as the referee. In one configuration, Kapa.ai used a typical fixed linear process with hard-coded optimizations built in.\\n\\nFor the modular RAG process, Kapa.ai used the o3-mini model as an inference model and ran various configurations of the process under different policies to evaluate which approaches worked and which did not:\\n\\nIn all tests conducted by Kapa.ai, the number of tool calls was limited to a maximum of 20 - for any given query, the model only allows a maximum of 20 tool calls to be used.Kapa.ai also ran all tests at medium and high inference strengths:\\n\\nIn total, Kapa.ai conducted 58 evaluations of different modular RAG configurations.\\n\\n## in the end\\n\\nExperimental results were mixed. In some configurations, Kapa.ai observed some modest improvements, most notably in code generation and, to a limited extent, factoring. However, key metrics such as information retrieval quality and knowledge extraction remained largely unchanged compared to Kapa.ai's traditional manually-tuned workflow.\\n\\nA recurring issue throughout the testing process is that Chain of Thought (CoT) reasoning adds latency. While deeper reasoning allows the model to decompose complex queries and self-correct, this comes at the cost of additional time required for iterative tool calls.\\n\\nThe biggest challenge identified by Kapa.ai is the \\\"inference ≠ experience fallacy\\\": the inference model, despite being able to think step-by-step, lacks a priori experience with retrieval tools. Even with rigorous hints, it struggled to retrieve high-quality results and to distinguish between good and bad outputs. The model is often hesitant to use the tools provided by Kapa.ai, similar to the experiments Kapa.ai conducted last year using the o1 model. This highlights a broader problem: inference models excel at abstract problem solving, but optimizing the use of tools without prior training remains an outstanding challenge.\\n\\n## Main findings\\n\\n## Reasoning ≠ Empirical Fallacy\\n\\nThe key conclusion of the experiments is that the inference model does not naturally possess tool-specific knowledge. Unlike finely tuned RAG processes, which encode retrieval logic into predefined steps, inference models process each retrieval call from scratch. This leads to inefficiency, indecision, and sub-optimal tool usage.\\n\\nTo mitigate this, several possible strategies can be considered. Further refinement of the cueing strategy, i.e., constructing tool-specific instructions in such a way as to provide more explicit guidance to the model, may be helpful. Pre-training or fine-tuning the model for tool use could also familiarize it with specific retrieval mechanisms.\\n\\nIn addition, a hybrid approach can be considered, where predefined heuristics handle certain tasks and inference models selectively intervene when needed.\\n\\nThese ideas are still in the speculative stage, but they point to ways to bridge the gap between reasoning ability and actual tool implementation.\\n\\n## summarize\\n\\nWhile the modular inference-based RAG did not show significant advantages over traditional processes within the context of Kapa.ai's use cases, the experiment did provide valuable insights into its potential and limitations. The flexibility of a modular approach remains attractive. It can improve adaptability, simplify upgrades, and dynamically adjust to new models or data sources.\\n\\nLooking ahead, a number of promising technologies deserve further exploration:\\n\\nAt this stage, reasoning models like o3-mini have not outperformed traditional RAG processes for core retrieval tasks within reasonable time constraints. As models advance and tool usage strategies evolve, modular reasoning-based RAG systems may become a viable alternative, especially for domains requiring dynamic, logic-intensive workflows.\\n\\n### Related articles\\n\\n### Recommended\\n\\n![Claude Code 何以如此出色？解构其 Agent 设计的内在魔力](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n![RAGFlow 文档切片方法的基准测试与对比](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n![RAG 的成败关键：被忽视的文档解析环节](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n![Context Engineering（上下文工程）](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n![GEO：生成引擎优化](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n![AI 代理的上下文工程：来自 Manus 的一线经验](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n![从评分到对决：借鉴ELO思想重塑AI搜索排序](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n![深度剖析AI智能体记忆：从核心概念到LangGraph实战](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\n### Can't find AI tools? Try here!\\n\\nJust type in the keyword **Accessibility Bing Search**You can quickly find all the AI tools on this site.\\n\\n### Popular AI tools\\n\\n![Video Face Swap](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nVideo Face Swap\\n\\n![Codeium（Windsurf Editor）：免费的AI代码补全与聊天工具，Windsurf以对话方式编写完整项目代码](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nCodeium (Windsurf Editor): free AI code-completion and chat tool, Windsurf writes complete project code in a conversational manner\\n\\n![Cursor试用期重置工具：解决Cursor试用期限制问题，轻松重置试用期，避免升级到专业版](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nCursor Trial Period Reset Tool: Solve the problem of Cursor trial period limitations, easily reset the trial period to avoid upgrading to the professional version\\n\\n![PocketPal AI](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nPocketPal AI\\n\\n![Jan：开源离线AI助手，ChatGPT 替代品，运行本地AI模型或连接云端AI](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nJan: Open Source Offline AI Assistant, ChatGPT Replacement, Run Local AI Models or Connect to Cloud AI\\n\\n![Roo Code（Roo Cline）：基于Cline的增强版自主编程助手，智能化IDE编程助手](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nRoo Code (Roo Cline): Enhanced autonomous programming assistant based on Cline, intelligent IDE programming assistant\\n\\n![MagicQuill：智能交互式图像涂鸦编辑系统，精准局部涂鸦编辑](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nMagicQuill: Intelligent Interactive Image Graffiti Editing System, Precise Localized Graffiti Editing\\n\\n![FaceFusion：视频换脸增强工具|语音同步视频嘴型动作](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nFaceFusion: Video Face Swap Enhancement Tool | Voice Synchronized Video Mouth Moves\\n\\n![Cherry Studio：集成API/Web/本地模型的AI助手桌面客户端](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nCherry Studio: AI assistant desktop client with integrated API/web/local models\\n\\n![gibberlink：两个AI智能体间高效音频通信的演示项目](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\ngibberlink: a demonstration project for efficient audio communication between two AI intelligences\\n\\n![DeepMosaics：自动去除图像和视频中的马赛克，或向其添加马赛克](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nDeepMosaics: Automatically removing mosaics from, or adding mosaics to, images and videos\\n\\n![豆包](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nbeanbag\\n\\n### New Releases\\n\\n![怎样优化LKE在企业内部知识库的搜索体验？](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n![教育机构如何应用LKE实现个性化学习路径推荐？](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n![如何使用LKE的RAG功能防止金融行业问答出现事实性错误？](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n![怎样通过LKE的Agent功能优化电商客服响应效率？](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n![如何利用腾讯云LKE解决企业知识管理分散的难题？](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n![LKE的多轮对话改写功能优化了连续交互体验](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n![DeepSeek-R1在LKE平台展现优异的数学推理能力](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n![LKE的低代码特性使非技术人员也能构建复杂智能应用](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n![LKE的RAG应用支持显著提升大模型回答的专业准确性](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n![腾讯云LKE是整合大语言模型与企业数据的智能应用构建平台](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n![与同类产品相比，腾讯云LKE有哪些独特的优势？](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n![使用LKE平台时，如何确保知识问答的准确性和完整性？](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\n### Latest AI tools\\n\\n![X-faces：5分钟集成的AI身份验证服务](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nX-faces: an AI authentication service for 5-minute integration\\n\\n![Nano Banana AI：使用文本指令编辑图像的AI工具](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nNano Banana AI: an AI tool for editing images using text commands\\n\\n![TransyncAI (同言翻译)：提供会议实时翻译和语音转文字纪要的工具](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nTransyncAI (simultaneous translation): a tool that provides real-time translation and speech-to-text summaries of meetings\\n\\n![Frame0：用于将想法快速转换为线框图的设计工具](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nFrame0: a design tool for quickly converting ideas into wireframes\\n\\n![AI风水：分析家居布局以改善运气的智能工具](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nAI Feng Shui: a smart tool to analyze the layout of your home to improve your luck\\n\\n![神数AI：免费使用的AI八字排盘与合婚分析工具](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nDivine Numbers AI: A free-to-use AI Eight Character Charting and Marriage Analysis Tool\\n\\n![Kode：Claude Code 开源优化版](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nKode: Claude Code Open Source Optimized Version\\n\\n![MCP ECharts：生成ECharts可视化图表的MCP工具](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nMCP ECharts: MCP tool for generating ECharts visualization charts\\n\\n![Nanocoder：在本地终端运行的代码生成工具](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nNanocoder: code generation tool that runs in the local terminal\\n\\n![LlamaFarm：在本地快速部署AI模型与应用的开发框架](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nLlamaFarm: a development framework for rapid local deployment of AI models and applications\\n\\n![DbRheo-CLI：使用自然语言操作数据库和分析数据的命令行工具](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nDbRheo-CLI: Command-line tool for manipulating databases and analyzing data using natural language\\n\\n![M3-Agent：一个拥有长期记忆并能处理音视频的多模态智能体](https://www.kdjingpai.com/wp-content/themes/nab/assets/img/default-thumb.jpg)\\n\\nM3-Agent: a multimodal intelligence with long-term memory and capable of processing audio and video\\n\\nTop\\n\\n## Quick query station AI tool\\n\\n![qrcode](https://www.kdjingpai.com/wp-content/themes/nab/action/qrcode.php?t=https://www.kdjingpai.com/en/mokuaihua-rag-xitong/)\\n\\nWeChat Scan Code Share\\n\\n![en_US](https://www.kdjingpai.com/wp-content/plugins/translatepress-multilingual/assets/images/flags/en_US.png \\\"English\\\")\\n![zh_CN](https://www.kdjingpai.com/wp-content/plugins/translatepress-multilingual/assets/images/flags/zh_CN.png \\\"简体中文\\\")\\n![ja](https://www.kdjingpai.com/wp-content/plugins/translatepress-multilingual/assets/images/flags/ja.png \\\"日本語\\\")\\n![de_DE](https://www.kdjingpai.com/wp-content/plugins/translatepress-multilingual/assets/images/flags/de_DE.png \\\"Deutsch\\\")\\n![pt_BR](https://www.kdjingpai.com/wp-content/plugins/translatepress-multilingual/assets/images/flags/pt_BR.png \\\"Português do Brasil\\\")\\n![en_US](https://www.kdjingpai.com/wp-content/plugins/translatepress-multilingual/assets/images/flags/en_US.png \\\"English\\\")\"}]\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Modular RAG(모듈형 RAG)는 전통적인 RAG 시스템의 한계를 극복하기 위해 개발된 접근 방식으로, 여러 모듈로 구성되어 각 모듈을 독립적으로 최적화하거나 교체할 수 있는 유연성을 제공합니다. 이는 특히 복잡한 쿼리와 다양한 데이터 소스를 처리하는 데 유리합니다.\n",
            "\n",
            "### Modular RAG의 이점\n",
            "1. **유연성**: 모듈형 구조로 인해 각 모듈을 독립적으로 개발하고 최적화할 수 있어 시스템의 적응성을 높입니다.\n",
            "2. **확장성**: 새로운 데이터 소스나 모델이 추가될 때 전체 시스템을 재구성할 필요 없이 모듈을 추가하거나 교체할 수 있습니다.\n",
            "3. **효율성**: 각 모듈이 특정 작업을 담당하므로, 시스템의 전반적인 성능을 최적화할 수 있습니다.\n",
            "4. **적응성**: 쿼리의 복잡성에 따라 다른 모듈을 활성화하거나 비활성화할 수 있어 다양한 시나리오에 적응할 수 있습니다.\n",
            "\n",
            "### Production Level에서의 이점\n",
            "- **실시간 데이터 처리**: 모듈형 RAG는 실시간 데이터 처리에 유리하며, 최신 정보를 기반으로 한 정확한 응답을 제공합니다.\n",
            "- **낮은 지연 시간**: 효율적인 검색 알고리즘과 캐싱 전략을 통해 낮은 지연 시간을 유지할 수 있습니다.\n",
            "- **고급 쿼리 처리**: 복잡한 쿼리나 다양한 데이터 소스를 요구하는 작업에 대해 더 나은 성능을 발휘합니다.\n",
            "\n",
            "이러한 이점들은 특히 대규모 데이터 처리와 실시간 응답이 중요한 생산 환경에서 Modular RAG의 활용을 촉진합니다.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mexecute\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "('Modular RAG가 production level에서 가지는 이점을 조사한다.', 'Modular RAG(모듈형 RAG)는 전통적인 RAG 시스템의 한계를 극복하기 위해 개발된 접근 방식으로, 여러 모듈로 구성되어 각 모듈을 독립적으로 최적화하거나 교체할 수 있는 유연성을 제공합니다. 이는 특히 복잡한 쿼리와 다양한 데이터 소스를 처리하는 데 유리합니다.\\n\\n### Modular RAG의 이점\\n1. **유연성**: 모듈형 구조로 인해 각 모듈을 독립적으로 개발하고 최적화할 수 있어 시스템의 적응성을 높입니다.\\n2. **확장성**: 새로운 데이터 소스나 모델이 추가될 때 전체 시스템을 재구성할 필요 없이 모듈을 추가하거나 교체할 수 있습니다.\\n3. **효율성**: 각 모듈이 특정 작업을 담당하므로, 시스템의 전반적인 성능을 최적화할 수 있습니다.\\n4. **적응성**: 쿼리의 복잡성에 따라 다른 모듈을 활성화하거나 비활성화할 수 있어 다양한 시나리오에 적응할 수 있습니다.\\n\\n### Production Level에서의 이점\\n- **실시간 데이터 처리**: 모듈형 RAG는 실시간 데이터 처리에 유리하며, 최신 정보를 기반으로 한 정확한 응답을 제공합니다.\\n- **낮은 지연 시간**: 효율적인 검색 알고리즘과 캐싱 전략을 통해 낮은 지연 시간을 유지할 수 있습니다.\\n- **고급 쿼리 처리**: 복잡한 쿼리나 다양한 데이터 소스를 요구하는 작업에 대해 더 나은 성능을 발휘합니다.\\n\\n이러한 이점들은 특히 대규모 데이터 처리와 실시간 응답이 중요한 생산 환경에서 Modular RAG의 활용을 촉진합니다.')\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mreplan\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "Naive RAG와 Modular RAG의 이점을 비교하여 정리한다.\n",
            "Modular RAG의 이점을 production level에서의 사용 사례와 연결하여 설명한다.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Naive RAG와 Modular RAG의 이점을 비교하여 정리하겠습니다.\n",
            "\n",
            "### Naive RAG의 이점\n",
            "1. **단순성**: Naive RAG는 구조가 단순하여 구현이 비교적 쉽습니다. 기본적인 검색 및 생성 기능을 결합하여 빠르게 프로토타입을 만들 수 있습니다.\n",
            "2. **빠른 개발**: 복잡한 모듈화 없이 기본적인 기능을 빠르게 통합할 수 있어 초기 개발 속도가 빠릅니다.\n",
            "3. **적은 초기 비용**: 초기 단계에서 복잡한 시스템을 구축할 필요가 없으므로 비용이 적게 듭니다.\n",
            "\n",
            "### Modular RAG의 이점\n",
            "1. **유연성**: Modular RAG는 다양한 모듈을 조합하여 시스템을 구성할 수 있어 유연성이 높습니다. 필요에 따라 모듈을 추가하거나 교체할 수 있습니다.\n",
            "2. **확장성**: 시스템이 커지거나 복잡해질 때, 모듈 단위로 확장할 수 있어 관리가 용이합니다.\n",
            "3. **유지보수 용이성**: 모듈화된 구조 덕분에 특정 기능이나 모듈에 문제가 발생했을 때, 해당 부분만 수정하면 되므로 유지보수가 용이합니다.\n",
            "4. **재사용성**: 모듈화된 구성 요소는 다른 프로젝트나 시스템에서 재사용할 수 있어 개발 효율성을 높입니다.\n",
            "5. **성능 최적화**: 각 모듈을 독립적으로 최적화할 수 있어 전체 시스템의 성능을 향상시킬 수 있습니다.\n",
            "\n",
            "이와 같이 Naive RAG는 초기 개발 단계에서의 단순성과 빠른 구현이 장점인 반면, Modular RAG는 유연성과 확장성, 유지보수의 용이성 등 장기적인 관점에서의 이점이 큽니다.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mexecute\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "('Naive RAG와 Modular RAG의 이점을 비교하여 정리한다.', 'Naive RAG와 Modular RAG의 이점을 비교하여 정리하겠습니다.\\n\\n### Naive RAG의 이점\\n1. **단순성**: Naive RAG는 구조가 단순하여 구현이 비교적 쉽습니다. 기본적인 검색 및 생성 기능을 결합하여 빠르게 프로토타입을 만들 수 있습니다.\\n2. **빠른 개발**: 복잡한 모듈화 없이 기본적인 기능을 빠르게 통합할 수 있어 초기 개발 속도가 빠릅니다.\\n3. **적은 초기 비용**: 초기 단계에서 복잡한 시스템을 구축할 필요가 없으므로 비용이 적게 듭니다.\\n\\n### Modular RAG의 이점\\n1. **유연성**: Modular RAG는 다양한 모듈을 조합하여 시스템을 구성할 수 있어 유연성이 높습니다. 필요에 따라 모듈을 추가하거나 교체할 수 있습니다.\\n2. **확장성**: 시스템이 커지거나 복잡해질 때, 모듈 단위로 확장할 수 있어 관리가 용이합니다.\\n3. **유지보수 용이성**: 모듈화된 구조 덕분에 특정 기능이나 모듈에 문제가 발생했을 때, 해당 부분만 수정하면 되므로 유지보수가 용이합니다.\\n4. **재사용성**: 모듈화된 구성 요소는 다른 프로젝트나 시스템에서 재사용할 수 있어 개발 효율성을 높입니다.\\n5. **성능 최적화**: 각 모듈을 독립적으로 최적화할 수 있어 전체 시스템의 성능을 향상시킬 수 있습니다.\\n\\n이와 같이 Naive RAG는 초기 개발 단계에서의 단순성과 빠른 구현이 장점인 반면, Modular RAG는 유연성과 확장성, 유지보수의 용이성 등 장기적인 관점에서의 이점이 큽니다.')\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mreplan\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32mresponse\u001b[0m:\n",
            "현재까지의 진행 상황을 고려할 때, 추가로 수행해야 할 단계는 다음과 같습니다:\n",
            "\n",
            "1. **Modular RAG의 이점을 실제 사례와 연결하여 설명한다.**\n",
            "   - Modular RAG가 실제로 어떻게 사용되고 있는지, 구체적인 사례를 통해 설명합니다. 이를 통해 이론적인 이점이 실제로 어떻게 구현되고 있는지를 보여줍니다.\n",
            "\n",
            "2. **Naive RAG와 Modular RAG의 이점을 종합하여 결론을 도출한다.**\n",
            "   - 두 접근 방식의 장단점을 종합하여, 특정 상황에서 어떤 방식이 더 적합한지에 대한 결론을 도출합니다.\n",
            "\n",
            "이 두 단계를 완료하면, Naive RAG와 Modular RAG의 차이점과 이점에 대한 설명이 완성됩니다. 이후 사용자에게 최종 결과를 전달할 수 있습니다.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mfinal_report\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32mresponse\u001b[0m:\n",
            "# 최종 보고서: Naive RAG와 Modular RAG의 비교 및 생산 환경에서의 이점\n",
            "\n",
            "## 개요\n",
            "\n",
            "본 보고서는 Naive RAG와 Modular RAG의 구조적 차이점과 각각의 이점을 분석하고, 특히 Modular RAG가 생산 환경에서 가지는 이점을 중점적으로 설명합니다. 이를 통해 두 접근 방식의 장단점을 이해하고, 실제 응용에서의 활용 가능성을 평가합니다.\n",
            "\n",
            "## Naive RAG와 Modular RAG의 기본 개념\n",
            "\n",
            "### Naive RAG\n",
            "\n",
            "Naive RAG는 정보 검색과 자연어 생성 모델을 결합한 단순한 구조의 시스템입니다. 주로 대량의 비구조화된 데이터에서 정보를 검색하고, 이를 기반으로 자연어 생성 모델이 응답을 생성하는 방식으로 작동합니다. Naive RAG는 검색 단계와 생성 단계를 단순히 결합한 형태로, 구조가 비교적 단순합니다.\n",
            "\n",
            "### Modular RAG\n",
            "\n",
            "Modular RAG는 Naive RAG의 구조를 개선하여 모듈화된 형태로 발전시킨 모델입니다. 각 모듈이 독립적으로 최적화될 수 있도록 설계되어 있으며, 검색 모듈과 생성 모듈이 명확히 분리되어 있습니다. 이러한 모듈화는 시스템의 유연성을 높이고, 특정 모듈을 교체하거나 업그레이드하기 쉽게 만듭니다.\n",
            "\n",
            "## Naive RAG와 Modular RAG의 구조적 차이점\n",
            "\n",
            "- **Naive RAG**는 단순한 검색 및 생성 구조로 이루어져 있으며, 검색기와 생성기로 구성됩니다. 이는 구현이 간단하고 직관적이지만, 복잡한 문제 해결에는 한계가 있을 수 있습니다.\n",
            "- **Modular RAG**는 다양한 기능을 수행하는 여러 모듈로 구성되어 있어 더 복잡하고 유연한 구조를 가지고 있습니다. 모듈화된 설계를 통해 시스템의 확장성과 유지보수성을 높이며, 각 모듈이 독립적으로 작동할 수 있도록 설계되어 있습니다.\n",
            "\n",
            "## Naive RAG와 Modular RAG의 이점 비교\n",
            "\n",
            "### Naive RAG의 이점\n",
            "\n",
            "1. **단순성**: 구조가 단순하여 구현이 비교적 쉽습니다.\n",
            "2. **빠른 개발**: 복잡한 모듈화 없이 기본적인 기능을 빠르게 통합할 수 있습니다.\n",
            "3. **적은 초기 비용**: 초기 단계에서 복잡한 시스템을 구축할 필요가 없으므로 비용이 적게 듭니다.\n",
            "\n",
            "### Modular RAG의 이점\n",
            "\n",
            "1. **유연성**: 다양한 모듈을 조합하여 시스템을 구성할 수 있어 유연성이 높습니다.\n",
            "2. **확장성**: 시스템이 커지거나 복잡해질 때, 모듈 단위로 확장할 수 있습니다.\n",
            "3. **유지보수 용이성**: 모듈화된 구조 덕분에 특정 기능이나 모듈에 문제가 발생했을 때, 해당 부분만 수정하면 됩니다.\n",
            "4. **재사용성**: 모듈화된 구성 요소는 다른 프로젝트나 시스템에서 재사용할 수 있습니다.\n",
            "5. **성능 최적화**: 각 모듈을 독립적으로 최적화할 수 있어 전체 시스템의 성능을 향상시킬 수 있습니다.\n",
            "\n",
            "## Modular RAG의 생산 환경에서의 이점\n",
            "\n",
            "Modular RAG는 특히 대규모 데이터 처리와 실시간 응답이 중요한 생산 환경에서 다음과 같은 이점을 제공합니다:\n",
            "\n",
            "- **실시간 데이터 처리**: 최신 정보를 기반으로 한 정확한 응답을 제공합니다.\n",
            "- **낮은 지연 시간**: 효율적인 검색 알고리즘과 캐싱 전략을 통해 낮은 지연 시간을 유지할 수 있습니다.\n",
            "- **고급 쿼리 처리**: 복잡한 쿼리나 다양한 데이터 소스를 요구하는 작업에 대해 더 나은 성능을 발휘합니다.\n",
            "\n",
            "## 결론\n",
            "\n",
            "Naive RAG는 초기 개발 단계에서의 단순성과 빠른 구현이 장점인 반면, Modular RAG는 유연성과 확장성, 유지보수의 용이성 등 장기적인 관점에서의 이점이 큽니다. 특히, Modular RAG는 생산 환경에서의 실시간 데이터 처리와 고급 쿼리 처리에 유리하여, 대규모 데이터 처리 시스템에서의 활용 가능성이 높습니다.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "from langchain_teddynote.messages import invoke_graph, random_uuid\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "config = RunnableConfig(recursion_limit=50, configurable={\"thread_id\": random_uuid()})\n",
        "\n",
        "inputs = {\n",
        "    \"input\": \"Modular RAG 가 기존의 Naive RAG 와 어떤 차이가 있는지와 production level 에서 사용하는 이점을 설명해줘\"\n",
        "}\n",
        "\n",
        "invoke_graph(app, inputs, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e139d290",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e139d290",
        "outputId": "48e34774-c07c-40d1-87b7-10e2efc74992"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 최종 보고서: Naive RAG와 Modular RAG의 비교 및 생산 환경에서의 이점\n",
            "\n",
            "## 개요\n",
            "\n",
            "본 보고서는 Naive RAG와 Modular RAG의 구조적 차이점과 각각의 이점을 분석하고, 특히 Modular RAG가 생산 환경에서 가지는 이점을 중점적으로 설명합니다. 이를 통해 두 접근 방식의 장단점을 이해하고, 실제 응용에서의 활용 가능성을 평가합니다.\n",
            "\n",
            "## Naive RAG와 Modular RAG의 기본 개념\n",
            "\n",
            "### Naive RAG\n",
            "\n",
            "Naive RAG는 정보 검색과 자연어 생성 모델을 결합한 단순한 구조의 시스템입니다. 주로 대량의 비구조화된 데이터에서 정보를 검색하고, 이를 기반으로 자연어 생성 모델이 응답을 생성하는 방식으로 작동합니다. Naive RAG는 검색 단계와 생성 단계를 단순히 결합한 형태로, 구조가 비교적 단순합니다.\n",
            "\n",
            "### Modular RAG\n",
            "\n",
            "Modular RAG는 Naive RAG의 구조를 개선하여 모듈화된 형태로 발전시킨 모델입니다. 각 모듈이 독립적으로 최적화될 수 있도록 설계되어 있으며, 검색 모듈과 생성 모듈이 명확히 분리되어 있습니다. 이러한 모듈화는 시스템의 유연성을 높이고, 특정 모듈을 교체하거나 업그레이드하기 쉽게 만듭니다.\n",
            "\n",
            "## Naive RAG와 Modular RAG의 구조적 차이점\n",
            "\n",
            "- **Naive RAG**는 단순한 검색 및 생성 구조로 이루어져 있으며, 검색기와 생성기로 구성됩니다. 이는 구현이 간단하고 직관적이지만, 복잡한 문제 해결에는 한계가 있을 수 있습니다.\n",
            "- **Modular RAG**는 다양한 기능을 수행하는 여러 모듈로 구성되어 있어 더 복잡하고 유연한 구조를 가지고 있습니다. 모듈화된 설계를 통해 시스템의 확장성과 유지보수성을 높이며, 각 모듈이 독립적으로 작동할 수 있도록 설계되어 있습니다.\n",
            "\n",
            "## Naive RAG와 Modular RAG의 이점 비교\n",
            "\n",
            "### Naive RAG의 이점\n",
            "\n",
            "1. **단순성**: 구조가 단순하여 구현이 비교적 쉽습니다.\n",
            "2. **빠른 개발**: 복잡한 모듈화 없이 기본적인 기능을 빠르게 통합할 수 있습니다.\n",
            "3. **적은 초기 비용**: 초기 단계에서 복잡한 시스템을 구축할 필요가 없으므로 비용이 적게 듭니다.\n",
            "\n",
            "### Modular RAG의 이점\n",
            "\n",
            "1. **유연성**: 다양한 모듈을 조합하여 시스템을 구성할 수 있어 유연성이 높습니다.\n",
            "2. **확장성**: 시스템이 커지거나 복잡해질 때, 모듈 단위로 확장할 수 있습니다.\n",
            "3. **유지보수 용이성**: 모듈화된 구조 덕분에 특정 기능이나 모듈에 문제가 발생했을 때, 해당 부분만 수정하면 됩니다.\n",
            "4. **재사용성**: 모듈화된 구성 요소는 다른 프로젝트나 시스템에서 재사용할 수 있습니다.\n",
            "5. **성능 최적화**: 각 모듈을 독립적으로 최적화할 수 있어 전체 시스템의 성능을 향상시킬 수 있습니다.\n",
            "\n",
            "## Modular RAG의 생산 환경에서의 이점\n",
            "\n",
            "Modular RAG는 특히 대규모 데이터 처리와 실시간 응답이 중요한 생산 환경에서 다음과 같은 이점을 제공합니다:\n",
            "\n",
            "- **실시간 데이터 처리**: 최신 정보를 기반으로 한 정확한 응답을 제공합니다.\n",
            "- **낮은 지연 시간**: 효율적인 검색 알고리즘과 캐싱 전략을 통해 낮은 지연 시간을 유지할 수 있습니다.\n",
            "- **고급 쿼리 처리**: 복잡한 쿼리나 다양한 데이터 소스를 요구하는 작업에 대해 더 나은 성능을 발휘합니다.\n",
            "\n",
            "## 결론\n",
            "\n",
            "Naive RAG는 초기 개발 단계에서의 단순성과 빠른 구현이 장점인 반면, Modular RAG는 유연성과 확장성, 유지보수의 용이성 등 장기적인 관점에서의 이점이 큽니다. 특히, Modular RAG는 생산 환경에서의 실시간 데이터 처리와 고급 쿼리 처리에 유리하여, 대규모 데이터 처리 시스템에서의 활용 가능성이 높습니다.\n"
          ]
        }
      ],
      "source": [
        "snapshot = app.get_state(config).values\n",
        "print(snapshot[\"response\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "611af101",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        },
        "id": "611af101",
        "outputId": "fa0dcef4-7fbb-46a9-fc31-3ee55357e256"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "# 최종 보고서: Naive RAG와 Modular RAG의 비교 및 생산 환경에서의 이점\n",
              "\n",
              "## 개요\n",
              "\n",
              "본 보고서는 Naive RAG와 Modular RAG의 구조적 차이점과 각각의 이점을 분석하고, 특히 Modular RAG가 생산 환경에서 가지는 이점을 중점적으로 설명합니다. 이를 통해 두 접근 방식의 장단점을 이해하고, 실제 응용에서의 활용 가능성을 평가합니다.\n",
              "\n",
              "## Naive RAG와 Modular RAG의 기본 개념\n",
              "\n",
              "### Naive RAG\n",
              "\n",
              "Naive RAG는 정보 검색과 자연어 생성 모델을 결합한 단순한 구조의 시스템입니다. 주로 대량의 비구조화된 데이터에서 정보를 검색하고, 이를 기반으로 자연어 생성 모델이 응답을 생성하는 방식으로 작동합니다. Naive RAG는 검색 단계와 생성 단계를 단순히 결합한 형태로, 구조가 비교적 단순합니다.\n",
              "\n",
              "### Modular RAG\n",
              "\n",
              "Modular RAG는 Naive RAG의 구조를 개선하여 모듈화된 형태로 발전시킨 모델입니다. 각 모듈이 독립적으로 최적화될 수 있도록 설계되어 있으며, 검색 모듈과 생성 모듈이 명확히 분리되어 있습니다. 이러한 모듈화는 시스템의 유연성을 높이고, 특정 모듈을 교체하거나 업그레이드하기 쉽게 만듭니다.\n",
              "\n",
              "## Naive RAG와 Modular RAG의 구조적 차이점\n",
              "\n",
              "- **Naive RAG**는 단순한 검색 및 생성 구조로 이루어져 있으며, 검색기와 생성기로 구성됩니다. 이는 구현이 간단하고 직관적이지만, 복잡한 문제 해결에는 한계가 있을 수 있습니다.\n",
              "- **Modular RAG**는 다양한 기능을 수행하는 여러 모듈로 구성되어 있어 더 복잡하고 유연한 구조를 가지고 있습니다. 모듈화된 설계를 통해 시스템의 확장성과 유지보수성을 높이며, 각 모듈이 독립적으로 작동할 수 있도록 설계되어 있습니다.\n",
              "\n",
              "## Naive RAG와 Modular RAG의 이점 비교\n",
              "\n",
              "### Naive RAG의 이점\n",
              "\n",
              "1. **단순성**: 구조가 단순하여 구현이 비교적 쉽습니다.\n",
              "2. **빠른 개발**: 복잡한 모듈화 없이 기본적인 기능을 빠르게 통합할 수 있습니다.\n",
              "3. **적은 초기 비용**: 초기 단계에서 복잡한 시스템을 구축할 필요가 없으므로 비용이 적게 듭니다.\n",
              "\n",
              "### Modular RAG의 이점\n",
              "\n",
              "1. **유연성**: 다양한 모듈을 조합하여 시스템을 구성할 수 있어 유연성이 높습니다.\n",
              "2. **확장성**: 시스템이 커지거나 복잡해질 때, 모듈 단위로 확장할 수 있습니다.\n",
              "3. **유지보수 용이성**: 모듈화된 구조 덕분에 특정 기능이나 모듈에 문제가 발생했을 때, 해당 부분만 수정하면 됩니다.\n",
              "4. **재사용성**: 모듈화된 구성 요소는 다른 프로젝트나 시스템에서 재사용할 수 있습니다.\n",
              "5. **성능 최적화**: 각 모듈을 독립적으로 최적화할 수 있어 전체 시스템의 성능을 향상시킬 수 있습니다.\n",
              "\n",
              "## Modular RAG의 생산 환경에서의 이점\n",
              "\n",
              "Modular RAG는 특히 대규모 데이터 처리와 실시간 응답이 중요한 생산 환경에서 다음과 같은 이점을 제공합니다:\n",
              "\n",
              "- **실시간 데이터 처리**: 최신 정보를 기반으로 한 정확한 응답을 제공합니다.\n",
              "- **낮은 지연 시간**: 효율적인 검색 알고리즘과 캐싱 전략을 통해 낮은 지연 시간을 유지할 수 있습니다.\n",
              "- **고급 쿼리 처리**: 복잡한 쿼리나 다양한 데이터 소스를 요구하는 작업에 대해 더 나은 성능을 발휘합니다.\n",
              "\n",
              "## 결론\n",
              "\n",
              "Naive RAG는 초기 개발 단계에서의 단순성과 빠른 구현이 장점인 반면, Modular RAG는 유연성과 확장성, 유지보수의 용이성 등 장기적인 관점에서의 이점이 큽니다. 특히, Modular RAG는 생산 환경에서의 실시간 데이터 처리와 고급 쿼리 처리에 유리하여, 대규모 데이터 처리 시스템에서의 활용 가능성이 높습니다."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(snapshot[\"response\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "0fde7887",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fde7887",
        "outputId": "d3765467-1046-472d-c73f-53ae4d09c9d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 최종 보고서: Naive RAG와 Modular RAG의 비교 및 생산 환경에서의 이점\n",
            "\n",
            "## 개요\n",
            "\n",
            "본 보고서는 Naive RAG와 Modular RAG의 구조적 차이점과 각각의 이점을 분석하고, 특히 Modular RAG가 생산 환경에서 가지는 이점을 중점적으로 설명합니다. 이를 통해 두 접근 방식의 장단점을 이해하고, 실제 응용에서의 활용 가능성을 평가합니다.\n",
            "\n",
            "## Naive RAG와 Modular RAG의 기본 개념\n",
            "\n",
            "### Naive RAG\n",
            "\n",
            "Naive RAG는 정보 검색과 자연어 생성 모델을 결합한 단순한 구조의 시스템입니다. 주로 대량의 비구조화된 데이터에서 정보를 검색하고, 이를 기반으로 자연어 생성 모델이 응답을 생성하는 방식으로 작동합니다. Naive RAG는 검색 단계와 생성 단계를 단순히 결합한 형태로, 구조가 비교적 단순합니다.\n",
            "\n",
            "### Modular RAG\n",
            "\n",
            "Modular RAG는 Naive RAG의 구조를 개선하여 모듈화된 형태로 발전시킨 모델입니다. 각 모듈이 독립적으로 최적화될 수 있도록 설계되어 있으며, 검색 모듈과 생성 모듈이 명확히 분리되어 있습니다. 이러한 모듈화는 시스템의 유연성을 높이고, 특정 모듈을 교체하거나 업그레이드하기 쉽게 만듭니다.\n",
            "\n",
            "## Naive RAG와 Modular RAG의 구조적 차이점\n",
            "\n",
            "- **Naive RAG**는 단순한 검색 및 생성 구조로 이루어져 있으며, 검색기와 생성기로 구성됩니다. 이는 구현이 간단하고 직관적이지만, 복잡한 문제 해결에는 한계가 있을 수 있습니다.\n",
            "- **Modular RAG**는 다양한 기능을 수행하는 여러 모듈로 구성되어 있어 더 복잡하고 유연한 구조를 가지고 있습니다. 모듈화된 설계를 통해 시스템의 확장성과 유지보수성을 높이며, 각 모듈이 독립적으로 작동할 수 있도록 설계되어 있습니다.\n",
            "\n",
            "## Naive RAG와 Modular RAG의 이점 비교\n",
            "\n",
            "### Naive RAG의 이점\n",
            "\n",
            "1. **단순성**: 구조가 단순하여 구현이 비교적 쉽습니다.\n",
            "2. **빠른 개발**: 복잡한 모듈화 없이 기본적인 기능을 빠르게 통합할 수 있습니다.\n",
            "3. **적은 초기 비용**: 초기 단계에서 복잡한 시스템을 구축할 필요가 없으므로 비용이 적게 듭니다.\n",
            "\n",
            "### Modular RAG의 이점\n",
            "\n",
            "1. **유연성**: 다양한 모듈을 조합하여 시스템을 구성할 수 있어 유연성이 높습니다.\n",
            "2. **확장성**: 시스템이 커지거나 복잡해질 때, 모듈 단위로 확장할 수 있습니다.\n",
            "3. **유지보수 용이성**: 모듈화된 구조 덕분에 특정 기능이나 모듈에 문제가 발생했을 때, 해당 부분만 수정하면 됩니다.\n",
            "4. **재사용성**: 모듈화된 구성 요소는 다른 프로젝트나 시스템에서 재사용할 수 있습니다.\n",
            "5. **성능 최적화**: 각 모듈을 독립적으로 최적화할 수 있어 전체 시스템의 성능을 향상시킬 수 있습니다.\n",
            "\n",
            "## Modular RAG의 생산 환경에서의 이점\n",
            "\n",
            "Modular RAG는 특히 대규모 데이터 처리와 실시간 응답이 중요한 생산 환경에서 다음과 같은 이점을 제공합니다:\n",
            "\n",
            "- **실시간 데이터 처리**: 최신 정보를 기반으로 한 정확한 응답을 제공합니다.\n",
            "- **낮은 지연 시간**: 효율적인 검색 알고리즘과 캐싱 전략을 통해 낮은 지연 시간을 유지할 수 있습니다.\n",
            "- **고급 쿼리 처리**: 복잡한 쿼리나 다양한 데이터 소스를 요구하는 작업에 대해 더 나은 성능을 발휘합니다.\n",
            "\n",
            "## 결론\n",
            "\n",
            "Naive RAG는 초기 개발 단계에서의 단순성과 빠른 구현이 장점인 반면, Modular RAG는 유연성과 확장성, 유지보수의 용이성 등 장기적인 관점에서의 이점이 큽니다. 특히, Modular RAG는 생산 환경에서의 실시간 데이터 처리와 고급 쿼리 처리에 유리하여, 대규모 데이터 처리 시스템에서의 활용 가능성이 높습니다.\n"
          ]
        }
      ],
      "source": [
        "print(snapshot[\"response\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "langchain-kr-lwwSZlnu-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
