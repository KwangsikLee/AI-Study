{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ê°ì„±ë¶„ì„ \n",
        "\n",
        "\n",
        "## Graph Attention Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ì½”ë“œ ìš”ì•½\n",
        "\n",
        "GraphEmotionNetwork í´ë˜ìŠ¤ëŠ” BERTì™€ ê·¸ë˜í”„ ì‹ ê²½ë§(GNN)ì„ ê²°í•©í•´ ê°ì • ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.\n",
        "\n",
        "êµ¬ì„± ë° ë™ì‘:\n",
        "\n",
        "BERT ì¸ì½”ë”ë¡œ ì…ë ¥ ë¬¸ì¥ì„ ì„ë² ë”©(ë²¡í„°í™”)í•©ë‹ˆë‹¤.  \n",
        "ê° í† í°ë³„ ê°ì • ì ìˆ˜(6ê°œ ê°ì •)ë¥¼ ì¶”ê°€í•´ íŠ¹ì§•ì„ ë§Œë“­ë‹ˆë‹¤.  \n",
        "íŠ¹ì§•ì„ ë³€í™˜í•œ ë’¤, 3ê°œì˜ Graph Attention Layer(GAT)ë¥¼ ê±°ì³ í† í° ê°„ ê´€ê³„ì™€ ê°ì • ì „íŒŒë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.  \n",
        "ê° ê°ì •ë³„ë¡œ ì¶”ê°€ì ì¸ íŠ¹ì§• ì¶”ì¶œ ë ˆì´ì–´(emotion_propagation)ë¥¼ ì ìš©í•©ë‹ˆë‹¤.  \n",
        "ëª¨ë“  íŠ¹ì§•ì„ í•©ì³ ìµœì¢… ë¶„ë¥˜ê¸°(classifier)ì—ì„œ ê°ì •(7ì¢…: ê¸°ì¨, ìŠ¬í””, ë¶„ë…¸, ê³µí¬, ë†€ëŒ, í˜ì˜¤, ì¤‘ë¦½)ë³„ í™•ë¥ ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.  \n",
        "ì¦‰, ë¬¸ì¥ ë‚´ í† í° ê°„ì˜ ê°ì •ì  ì—°ê²°ê³¼ BERTì˜ ì–¸ì–´ì  íŠ¹ì§•ì„ ë™ì‹œì— í™œìš©í•´, ë” ì •êµí•œ ê°ì • ë¶„ì„ì„ ëª©í‘œë¡œ í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Tls-gyeNK9w",
        "outputId": "c5b152e3-06e3-490c-b4c4-81e8ccb771cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸŒ Graph Neural Network ê¸°ë°˜ ê°ì • ì „íŒŒ ëª¨ë¸\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
            "Epoch 5/10, Loss: 1.3338\n",
            "Epoch 10/10, Loss: 0.8100\n",
            "\n",
            "============================================================\n",
            "ê·¸ë˜í”„ ê¸°ë°˜ ê°ì • ë¶„ì„ ê²°ê³¼\n",
            "============================================================\n",
            "\n",
            "ğŸ“ ë¶„ì„ í…ìŠ¤íŠ¸: ê¸°ìœ ì†Œì‹ì„ ë“£ê³  í–‰ë³µí•´ì„œ ëˆˆë¬¼ì´ ë‚¬ì–´ìš”\n",
            "\n",
            "ğŸ”— ê·¸ë˜í”„ ê¸°ë°˜ ê°ì • ë¶„ì„:\n",
            "  ê¸°ìœ           â†’ ğŸ˜Š ê¸°ì¨   (ì‹ ë¢°ë„: 25.84%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ì†Œì‹           â†’ ğŸ˜Š ê¸°ì¨   (ì‹ ë¢°ë„: 23.48%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ##ì„          â†’ ğŸ˜Š ê¸°ì¨   (ì‹ ë¢°ë„: 21.56%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ë“£            â†’ ğŸ˜Š ê¸°ì¨   (ì‹ ë¢°ë„: 21.11%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ##ê³           â†’ ğŸ˜Š ê¸°ì¨   (ì‹ ë¢°ë„: 22.31%, ê·¸ë˜í”„ ì˜í–¥: ê¸°ì¨)\n",
            "  í–‰ë³µ           â†’ ğŸ˜Š ê¸°ì¨   (ì‹ ë¢°ë„: 22.63%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ##í•´ì„œ         â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 22.66%, ê·¸ë˜í”„ ì˜í–¥: ìŠ¬í””)\n",
            "  ëˆˆë¬¼           â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 23.60%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ##ì´          â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 24.13%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ë‚¬            â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 24.41%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ##ì–´ìš”         â†’ ğŸ˜Š ê¸°ì¨   (ì‹ ë¢°ë„: 27.92%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "\n",
            "ğŸŒ ê°ì • ë„¤íŠ¸ì›Œí¬ íŠ¹ì„±:\n",
            "  - ê¸°ì¨: 7ê°œ ë…¸ë“œê°€ ì—°ê²°ë¨\n",
            "  - ë¶„ë…¸: 4ê°œ ë…¸ë“œê°€ ì—°ê²°ë¨\n",
            "  â†’ ê·¸ë˜í”„ ì—£ì§€ ìˆ˜: 20ê°œ\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ“ ë¶„ì„ í…ìŠ¤íŠ¸: ë¬´ì„œìš´ ì´ì•¼ê¸°ë¥¼ ë“£ê³  ë„ˆë¬´ ë†€ë¼ê³  ë‘ë ¤ì› ì–´ìš”\n",
            "\n",
            "ğŸ”— ê·¸ë˜í”„ ê¸°ë°˜ ê°ì • ë¶„ì„:\n",
            "  ë¬´ì„œìš´          â†’ ğŸ˜¨ ê³µí¬   (ì‹ ë¢°ë„: 30.19%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ì´ì•¼ê¸°          â†’ ğŸ˜¨ ê³µí¬   (ì‹ ë¢°ë„: 28.93%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ##ë¥¼          â†’ ğŸ˜¨ ê³µí¬   (ì‹ ë¢°ë„: 26.51%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ë“£            â†’ ğŸ˜¨ ê³µí¬   (ì‹ ë¢°ë„: 24.38%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ##ê³           â†’ ğŸ˜¨ ê³µí¬   (ì‹ ë¢°ë„: 22.99%, ê·¸ë˜í”„ ì˜í–¥: ê¸°ì¨)\n",
            "  ë„ˆë¬´           â†’ ğŸ˜¨ ê³µí¬   (ì‹ ë¢°ë„: 22.19%, ê·¸ë˜í”„ ì˜í–¥: ë†€ëŒ)\n",
            "  ë†€ë¼           â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 22.29%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ##ê³           â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 23.99%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ë‘ë ¤ì›           â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 24.77%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ##ì–´ìš”         â†’ ğŸ˜¨ ê³µí¬   (ì‹ ë¢°ë„: 24.04%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "\n",
            "ğŸŒ ê°ì • ë„¤íŠ¸ì›Œí¬ íŠ¹ì„±:\n",
            "  - ê³µí¬: 7ê°œ ë…¸ë“œê°€ ì—°ê²°ë¨\n",
            "  - ë¶„ë…¸: 3ê°œ ë…¸ë“œê°€ ì—°ê²°ë¨\n",
            "  â†’ ê·¸ë˜í”„ ì—£ì§€ ìˆ˜: 20ê°œ\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ“ ë¶„ì„ í…ìŠ¤íŠ¸: í™”ê°€ ë‚˜ì„œ ì§œì¦ì´ ë‚˜ì§€ë§Œ ì°¸ê³  ìˆì–´ìš”\n",
            "\n",
            "ğŸ”— ê·¸ë˜í”„ ê¸°ë°˜ ê°ì • ë¶„ì„:\n",
            "  í™”ê°€           â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 53.52%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ë‚˜ì„œ           â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 51.64%, ê·¸ë˜í”„ ì˜í–¥: ë¶„ë…¸)\n",
            "  ì§œì¦           â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 49.14%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ##ì´          â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 47.86%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ë‚˜ì§€           â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 47.83%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ##ë§Œ          â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 47.84%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ì°¸ê³            â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 47.80%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ìˆ            â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 47.61%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ##ì–´ìš”         â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 51.97%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "\n",
            "ğŸŒ ê°ì • ë„¤íŠ¸ì›Œí¬ íŠ¹ì„±:\n",
            "  - ë¶„ë…¸: 9ê°œ ë…¸ë“œê°€ ì—°ê²°ë¨\n",
            "  â†’ ê·¸ë˜í”„ ì—£ì§€ ìˆ˜: 16ê°œ\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ“ ë¶„ì„ í…ìŠ¤íŠ¸: ë†€ë¼ìš´ ê²°ê³¼ì— ê¸°ì˜ë©´ì„œë„ ë¯¿ê¸°ì§€ ì•Šì•„ìš”\n",
            "\n",
            "ğŸ”— ê·¸ë˜í”„ ê¸°ë°˜ ê°ì • ë¶„ì„:\n",
            "  ë†€ë¼ìš´          â†’ ğŸ˜Š ê¸°ì¨   (ì‹ ë¢°ë„: 25.77%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ê²°ê³¼           â†’ ğŸ˜Š ê¸°ì¨   (ì‹ ë¢°ë„: 25.25%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ##ì—          â†’ ğŸ˜Š ê¸°ì¨   (ì‹ ë¢°ë„: 23.00%, ê·¸ë˜í”„ ì˜í–¥: ê¸°ì¨)\n",
            "  ê¸°ì˜           â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 21.67%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ##ë©´ì„œ         â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 23.36%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ##ë„          â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 24.58%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ë¯¿ê¸°           â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 25.28%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ##ì§€          â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 25.40%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ì•Š            â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 24.99%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ##ì•„          â†’ ğŸ˜  ë¶„ë…¸   (ì‹ ë¢°ë„: 24.79%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "  ##ìš”          â†’ ğŸ˜Š ê¸°ì¨   (ì‹ ë¢°ë„: 29.71%, ê·¸ë˜í”„ ì˜í–¥: ì—†ìŒ)\n",
            "\n",
            "ğŸŒ ê°ì • ë„¤íŠ¸ì›Œí¬ íŠ¹ì„±:\n",
            "  - ë¶„ë…¸: 7ê°œ ë…¸ë“œê°€ ì—°ê²°ë¨\n",
            "  - ê¸°ì¨: 4ê°œ ë…¸ë“œê°€ ì—°ê²°ë¨\n",
            "  â†’ ê·¸ë˜í”„ ì—£ì§€ ìˆ˜: 20ê°œ\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple\n",
        "# Graph Neural Network êµ¬í˜„ (torch_geometric ì—†ì´)\n",
        "\n",
        "# í•œêµ­ì–´ ê°ì • ì–´íœ˜ ê·¸ë˜í”„\n",
        "EMOTION_GRAPH = {\n",
        "    'joy': {\n",
        "        'core': ['ê¸°ì˜', 'í–‰ë³µ', 'ì¦ê²', 'ì¢‹'],\n",
        "        'related': ['ì›ƒ', 'ì‹ ë‚˜', 'ê°ë™', 'ì‚¬ë‘', 'í¬ë§', 'ë§Œì¡±'],\n",
        "        'intensifiers': ['ë§¤ìš°', 'ì •ë§', 'ë„ˆë¬´', 'ì•„ì£¼', 'ì§„ì§œ']\n",
        "    },\n",
        "    'sadness': {\n",
        "        'core': ['ìŠ¬í”„', 'ìš°ìš¸', 'ëˆˆë¬¼', 'ì•„í”„'],\n",
        "        'related': ['ì™¸ë¡­', 'ê·¸ë¦½', 'í›„íšŒ', 'ì‹¤ë§', 'ì ˆë§', 'ë¹„ì°¸'],\n",
        "        'intensifiers': ['ë„ˆë¬´', 'ì •ë§', 'ë§¤ìš°', 'ëª¹ì‹œ']\n",
        "    },\n",
        "    'anger': {\n",
        "        'core': ['í™”', 'ë¶„ë…¸', 'ì§œì¦', 'ì‹«'],\n",
        "        'related': ['ë¯¸ì›Œ', 'ì—´ë°›', 'ë‹µë‹µ', 'ì–µìš¸', 'ë¶ˆì¾Œ'],\n",
        "        'intensifiers': ['ì •ë§', 'ë„ˆë¬´', 'ì§„ì§œ', 'ì™„ì „']\n",
        "    },\n",
        "    'fear': {\n",
        "        'core': ['ë¬´ì„œ', 'ë‘ë µ', 'ê²', 'ê³µí¬'],\n",
        "        'related': ['ë¶ˆì•ˆ', 'ê±±ì •', 'ë–¨', 'ê¸´ì¥', 'ìœ„í—˜'],\n",
        "        'intensifiers': ['ë„ˆë¬´', 'ì •ë§', 'ë§¤ìš°']\n",
        "    },\n",
        "    'surprise': {\n",
        "        'core': ['ë†€ë¼', 'ê¹œì§', 'ì¶©ê²©', 'ê°‘ì‘'],\n",
        "        'related': ['ë¯¿ê¸°ì§€', 'ì˜ì™¸', 'ëœ»ë°–', 'ì˜ˆìƒì™¸'],\n",
        "        'intensifiers': ['ì •ë§', 'ë„ˆë¬´', 'ì™„ì „']\n",
        "    },\n",
        "    'disgust': {\n",
        "        'core': ['ì—­ê²¹', 'ë”ëŸ½', 'í˜ì˜¤', 'êµ¬ì—­ì§ˆ'],\n",
        "        'related': ['ë¶ˆì¾Œ', 'ë”ì°', 'ì§•ê·¸ëŸ½', 'ë©”ìŠ¤ê»'],\n",
        "        'intensifiers': ['ë„ˆë¬´', 'ì •ë§', 'ì§„ì§œ']\n",
        "    }\n",
        "}\n",
        "\n",
        "class EmotionGraphBuilder:\n",
        "    \"\"\"í…ìŠ¤íŠ¸ë¥¼ ê°ì • ê·¸ë˜í”„ë¡œ ë³€í™˜\"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.emotion_vocab = self._build_emotion_vocab()\n",
        "\n",
        "    def _build_emotion_vocab(self):\n",
        "        \"\"\"ê°ì • ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•\"\"\"\n",
        "        vocab = {}\n",
        "        for emotion, words_dict in EMOTION_GRAPH.items():\n",
        "            for word_list in words_dict.values():\n",
        "                for word in word_list:\n",
        "                    if word not in vocab:\n",
        "                        vocab[word] = []\n",
        "                    vocab[word].append(emotion)\n",
        "        return vocab\n",
        "\n",
        "    def build_graph(self, text):\n",
        "        \"\"\"í…ìŠ¤íŠ¸ë¥¼ ê·¸ë˜í”„ë¡œ ë³€í™˜\"\"\"\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "\n",
        "        # ë…¸ë“œ íŠ¹ì§• (ê°ì • ì´ˆê¸°ê°’)\n",
        "        node_features = []\n",
        "        emotion_scores = []\n",
        "\n",
        "        for token in tokens:\n",
        "            token_clean = token.replace('##', '')\n",
        "\n",
        "            # ê°ì • ì ìˆ˜ ê³„ì‚°\n",
        "            scores = np.zeros(6)  # 6ê°œ ê°ì •\n",
        "            for emotion_idx, emotion in enumerate(['joy', 'sadness', 'anger', 'fear', 'surprise', 'disgust']):\n",
        "                for category, words in EMOTION_GRAPH[emotion].items():\n",
        "                    for word in words:\n",
        "                        if word in token_clean:\n",
        "                            if category == 'core':\n",
        "                                scores[emotion_idx] += 2.0\n",
        "                            elif category == 'related':\n",
        "                                scores[emotion_idx] += 1.0\n",
        "                            elif category == 'intensifiers':\n",
        "                                scores[emotion_idx] += 0.5\n",
        "\n",
        "            emotion_scores.append(scores)\n",
        "\n",
        "        # ì—£ì§€ êµ¬ì„± (ì¸ì ‘ í† í° + ì˜ë¯¸ì  ì—°ê²°)\n",
        "        edges = []\n",
        "        edge_weights = []\n",
        "\n",
        "        for i in range(len(tokens)):\n",
        "            # ì¸ì ‘ í† í° ì—°ê²°\n",
        "            if i > 0:\n",
        "                edges.append([i-1, i])\n",
        "                edge_weights.append(1.0)\n",
        "            if i < len(tokens) - 1:\n",
        "                edges.append([i, i+1])\n",
        "                edge_weights.append(1.0)\n",
        "\n",
        "            # ê°™ì€ ê°ì • í‚¤ì›Œë“œë¼ë¦¬ ì—°ê²°\n",
        "            for j in range(i+1, min(i+5, len(tokens))):  # 5í† í° ì´ë‚´\n",
        "                if self._are_emotionally_related(tokens[i], tokens[j]):\n",
        "                    edges.append([i, j])\n",
        "                    edges.append([j, i])\n",
        "                    edge_weights.extend([0.5, 0.5])\n",
        "\n",
        "        return tokens, np.array(emotion_scores), edges, edge_weights\n",
        "\n",
        "    def _are_emotionally_related(self, token1, token2):\n",
        "        \"\"\"ë‘ í† í°ì´ ê°ì •ì ìœ¼ë¡œ ì—°ê´€ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\"\"\"\n",
        "        t1_clean = token1.replace('##', '')\n",
        "        t2_clean = token2.replace('##', '')\n",
        "\n",
        "        emotions1 = self.emotion_vocab.get(t1_clean, [])\n",
        "        emotions2 = self.emotion_vocab.get(t2_clean, [])\n",
        "\n",
        "        return len(set(emotions1) & set(emotions2)) > 0\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    \"\"\"Graph Attention Layer ì§ì ‘ êµ¬í˜„\"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, dropout=0.1, alpha=0.2):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.dropout = dropout\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "\n",
        "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        \"\"\"\n",
        "        x: [num_nodes, in_features]\n",
        "        adj: [num_nodes, num_nodes] adjacency matrix\n",
        "        \"\"\"\n",
        "        h = torch.mm(x, self.W)  # [num_nodes, out_features]\n",
        "        num_nodes = h.size(0)\n",
        "\n",
        "        # Attention mechanism\n",
        "        h_repeat = h.repeat(num_nodes, 1)  # [num_nodes*num_nodes, out_features]\n",
        "        h_repeat_interleave = h.repeat_interleave(num_nodes, dim=0)\n",
        "        h_concat = torch.cat([h_repeat_interleave, h_repeat], dim=1)  # [num_nodes*num_nodes, 2*out_features]\n",
        "\n",
        "        e = self.leakyrelu(torch.matmul(h_concat, self.a).squeeze(1))\n",
        "        e = e.view(num_nodes, num_nodes)\n",
        "\n",
        "        # Mask attention scores\n",
        "        zero_vec = -9e15 * torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "\n",
        "        h_prime = torch.matmul(attention, h)\n",
        "\n",
        "        return F.elu(h_prime)\n",
        "\n",
        "class GraphEmotionNetwork(nn.Module):\n",
        "    \"\"\"Graph Neural Network ê¸°ë°˜ ê°ì • ë¶„ì„ ëª¨ë¸\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='klue/bert-base', hidden_dim=256, num_emotions=7):\n",
        "        super().__init__()\n",
        "\n",
        "        # BERT ì¸ì½”ë”\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        bert_dim = self.bert.config.hidden_size\n",
        "\n",
        "        # ì´ˆê¸° íŠ¹ì§• ë³€í™˜\n",
        "        self.input_transform = nn.Linear(bert_dim + 6, hidden_dim)  # BERT + ê°ì • ì ìˆ˜\n",
        "\n",
        "        # Graph Attention Layers\n",
        "        self.gat1 = GraphAttentionLayer(hidden_dim, hidden_dim, dropout=0.1)\n",
        "        self.gat2 = GraphAttentionLayer(hidden_dim, hidden_dim, dropout=0.1)\n",
        "        self.gat3 = GraphAttentionLayer(hidden_dim, hidden_dim, dropout=0.1)\n",
        "\n",
        "        # ê°ì • ì „íŒŒ ë ˆì´ì–´\n",
        "        self.emotion_propagation = nn.ModuleList([\n",
        "            nn.Linear(hidden_dim, 64) for _ in range(num_emotions)\n",
        "        ])\n",
        "\n",
        "        # ìµœì¢… ë¶„ë¥˜ê¸°\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim + 64 * num_emotions, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, num_emotions)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, emotion_scores, edge_index, edge_weight=None):\n",
        "        # BERT ì¸ì½”ë”©\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        bert_features = outputs.last_hidden_state  # [batch, seq_len, bert_dim]\n",
        "\n",
        "        # ê·¸ë˜í”„ ë°ì´í„° ì¤€ë¹„\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        node_features = []\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            # BERT íŠ¹ì§•ê³¼ ê°ì • ì ìˆ˜ ê²°í•©\n",
        "            bert_feat = bert_features[b]  # [seq_len, bert_dim]\n",
        "            emotion_feat = emotion_scores[b] if b < len(emotion_scores) else torch.zeros(seq_len, 6)\n",
        "\n",
        "            if isinstance(emotion_feat, np.ndarray):\n",
        "                emotion_feat = torch.tensor(emotion_feat, dtype=torch.float32)\n",
        "\n",
        "            # í¬ê¸° ë§ì¶”ê¸°\n",
        "            if emotion_feat.shape[0] < seq_len:\n",
        "                padding = torch.zeros(seq_len - emotion_feat.shape[0], 6)\n",
        "                emotion_feat = torch.cat([emotion_feat, padding], dim=0)\n",
        "            elif emotion_feat.shape[0] > seq_len:\n",
        "                emotion_feat = emotion_feat[:seq_len]\n",
        "\n",
        "            combined = torch.cat([bert_feat, emotion_feat], dim=-1)\n",
        "            node_features.append(combined)\n",
        "\n",
        "        # ë…¸ë“œ íŠ¹ì§• ë³€í™˜\n",
        "        x = torch.stack(node_features).view(-1, bert_features.shape[-1] + 6)\n",
        "        x = self.input_transform(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Adjacency matrix ìƒì„±\n",
        "        num_nodes = x.shape[0]\n",
        "        adj = torch.zeros(num_nodes, num_nodes)\n",
        "\n",
        "        if edge_index is not None and len(edge_index) > 0:\n",
        "            for edge in edge_index:\n",
        "                if edge[0] < num_nodes and edge[1] < num_nodes:\n",
        "                    adj[edge[0], edge[1]] = 1.0\n",
        "                    adj[edge[1], edge[0]] = 1.0  # ë¬´ë°©í–¥ ê·¸ë˜í”„\n",
        "\n",
        "        # ìê¸° ì—°ê²° ì¶”ê°€\n",
        "        adj = adj + torch.eye(num_nodes)\n",
        "\n",
        "        # GAT ë ˆì´ì–´ í†µê³¼\n",
        "        x = self.gat1(x, adj)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.gat2(x, adj)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.gat3(x, adj)\n",
        "\n",
        "        # ê°ì •ë³„ íŠ¹ì§• ì¶”ì¶œ\n",
        "        emotion_features = []\n",
        "        for emotion_layer in self.emotion_propagation:\n",
        "            emotion_feat = emotion_layer(x)\n",
        "            emotion_features.append(emotion_feat)\n",
        "\n",
        "        # íŠ¹ì§• ê²°í•©\n",
        "        combined_features = torch.cat([x] + emotion_features, dim=-1)\n",
        "\n",
        "        # ìµœì¢… ë¶„ë¥˜\n",
        "        logits = self.classifier(combined_features)\n",
        "\n",
        "        # Reshape back to [batch, seq_len, num_emotions]\n",
        "        logits = logits.view(batch_size, seq_len, -1)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class GNNEmotionAnalyzer:\n",
        "    \"\"\"GNN ê¸°ë°˜ ê°ì • ë¶„ì„ê¸°\"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, graph_builder):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.graph_builder = graph_builder\n",
        "        self.emotion_names = ['ê¸°ì¨', 'ìŠ¬í””', 'ë¶„ë…¸', 'ê³µí¬', 'ë†€ëŒ', 'í˜ì˜¤', 'ì¤‘ë¦½']\n",
        "\n",
        "    def analyze(self, text):\n",
        "        \"\"\"í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„\"\"\"\n",
        "        # ê·¸ë˜í”„ êµ¬ì„±\n",
        "        tokens, emotion_scores, edges, edge_weights = self.graph_builder.build_graph(text)\n",
        "\n",
        "        # í† í¬ë‚˜ì´ì§•\n",
        "        encoded = self.tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
        "\n",
        "        # ì—£ì§€ í…ì„œ ë³€í™˜\n",
        "        if edges:\n",
        "            edge_index = torch.tensor(edges, dtype=torch.long)\n",
        "            edge_weight = torch.tensor(edge_weights, dtype=torch.float)\n",
        "        else:\n",
        "            edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
        "            edge_weight = torch.tensor([1.0], dtype=torch.float)\n",
        "\n",
        "        # ì˜ˆì¸¡\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(\n",
        "                encoded['input_ids'],\n",
        "                encoded['attention_mask'],\n",
        "                [emotion_scores],\n",
        "                edge_index,\n",
        "                edge_weight\n",
        "            )\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # ê²°ê³¼ ì •ë¦¬\n",
        "        results = []\n",
        "        valid_tokens = self.tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
        "\n",
        "        for i, token in enumerate(valid_tokens):\n",
        "            if token not in ['[CLS]', '[SEP]', '[PAD]']:\n",
        "                token_probs = probs[0, i]\n",
        "                max_prob, max_idx = torch.max(token_probs, dim=0)\n",
        "\n",
        "                results.append({\n",
        "                    'token': token,\n",
        "                    'emotion': self.emotion_names[max_idx.item()],\n",
        "                    'confidence': max_prob.item(),\n",
        "                    'all_probs': token_probs.numpy(),\n",
        "                    'graph_influence': emotion_scores[min(i, len(emotion_scores)-1)] if i < len(emotion_scores) else np.zeros(6)\n",
        "                })\n",
        "\n",
        "        return results, edges\n",
        "\n",
        "    def visualize_emotion_graph(self, text, results, edges):\n",
        "        \"\"\"ê°ì • ê·¸ë˜í”„ ì‹œê°í™”\"\"\"\n",
        "        G = nx.Graph()\n",
        "\n",
        "        # ë…¸ë“œ ì¶”ê°€\n",
        "        for i, r in enumerate(results):\n",
        "            G.add_node(i,\n",
        "                      label=r['token'],\n",
        "                      emotion=r['emotion'],\n",
        "                      confidence=r['confidence'])\n",
        "\n",
        "        # ì—£ì§€ ì¶”ê°€\n",
        "        for edge in edges:\n",
        "            if edge[0] < len(results) and edge[1] < len(results):\n",
        "                G.add_edge(edge[0], edge[1])\n",
        "\n",
        "        # ì‹œê°í™”\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        pos = nx.spring_layout(G, k=2, iterations=50)\n",
        "\n",
        "        # ë…¸ë“œ ìƒ‰ìƒ (ê°ì •ë³„)\n",
        "        emotion_colors = {\n",
        "            'ê¸°ì¨': '#FFD700', 'ìŠ¬í””': '#4169E1', 'ë¶„ë…¸': '#DC143C',\n",
        "            'ê³µí¬': '#8B008B', 'ë†€ëŒ': '#FF69B4', 'í˜ì˜¤': '#228B22', 'ì¤‘ë¦½': '#C0C0C0'\n",
        "        }\n",
        "\n",
        "        node_colors = [emotion_colors[r['emotion']] for r in results]\n",
        "        node_sizes = [r['confidence'] * 3000 for r in results]\n",
        "\n",
        "        # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\n",
        "        nx.draw_networkx_nodes(G, pos, node_color=node_colors,\n",
        "                              node_size=node_sizes, alpha=0.7)\n",
        "        nx.draw_networkx_edges(G, pos, alpha=0.3)\n",
        "\n",
        "        # ë¼ë²¨\n",
        "        labels = {i: r['token'] + '\\n' + r['emotion'][:2]\n",
        "                 for i, r in enumerate(results)}\n",
        "        nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
        "\n",
        "        plt.title(f'ê°ì • ê·¸ë˜í”„: \"{text}\"')\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def train_gnn_model(model, tokenizer, graph_builder, epochs=20):\n",
        "    \"\"\"GNN ëª¨ë¸ ê°„ë‹¨ í›ˆë ¨\"\"\"\n",
        "    # í›ˆë ¨ ë°ì´í„°\n",
        "    train_data = [\n",
        "        (\"ì •ë§ ê¸°ìœ ì†Œì‹ì´ì—ìš” ë„ˆë¬´ í–‰ë³µí•´ìš”\", \"joy\"),\n",
        "        (\"ìŠ¬í”ˆ ì´ë³„ì˜ ìˆœê°„ì´ ì™”ì–´ìš”\", \"sadness\"),\n",
        "        (\"í™”ê°€ ë‚˜ì„œ ì°¸ì„ ìˆ˜ê°€ ì—†ì–´ìš”\", \"anger\"),\n",
        "        (\"ë¬´ì„œìš´ ì¼ì´ ìƒê¸¸ê¹Œë´ ê±±ì •ë¼ìš”\", \"fear\"),\n",
        "        (\"ê¹œì§ ë†€ë„ë§Œí•œ ì¼ì´ ì¼ì–´ë‚¬ì–´ìš”\", \"surprise\"),\n",
        "        (\"ì—­ê²¨ìš´ ëƒ„ìƒˆê°€ ë‚˜ì„œ í˜ë“¤ì–´ìš”\", \"disgust\")\n",
        "    ]\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    emotion_to_idx = {'joy': 0, 'sadness': 1, 'anger': 2,\n",
        "                     'fear': 3, 'surprise': 4, 'disgust': 5}\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for text, emotion in train_data:\n",
        "            # ê·¸ë˜í”„ êµ¬ì„±\n",
        "            tokens, emotion_scores, edges, edge_weights = graph_builder.build_graph(text)\n",
        "\n",
        "            # í† í¬ë‚˜ì´ì§•\n",
        "            encoded = tokenizer(text, return_tensors='pt', truncation=True,\n",
        "                              padding='max_length', max_length=128)\n",
        "\n",
        "            # ë ˆì´ë¸” ìƒì„±\n",
        "            emotion_idx = emotion_to_idx[emotion]\n",
        "            labels = torch.full((1, 128), emotion_idx, dtype=torch.long)\n",
        "\n",
        "            # ì—£ì§€ ì²˜ë¦¬\n",
        "            if edges:\n",
        "                edge_index = torch.tensor(edges, dtype=torch.long)\n",
        "            else:\n",
        "                edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
        "\n",
        "            # ìˆœì „íŒŒ\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(\n",
        "                encoded['input_ids'],\n",
        "                encoded['attention_mask'],\n",
        "                [emotion_scores],\n",
        "                edge_index\n",
        "            )\n",
        "\n",
        "            # Loss ê³„ì‚°\n",
        "            loss = criterion(logits.view(-1, 7), labels.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_data):.4f}')\n",
        "\n",
        "    return model\n",
        "\n",
        "def display_gnn_results(text, results):\n",
        "    \"\"\"GNN ê²°ê³¼ ì¶œë ¥\"\"\"\n",
        "    print(f\"\\nğŸ“ ë¶„ì„ í…ìŠ¤íŠ¸: {text}\")\n",
        "    print(\"\\nğŸ”— ê·¸ë˜í”„ ê¸°ë°˜ ê°ì • ë¶„ì„:\")\n",
        "\n",
        "    emotion_emoji = {\n",
        "        'ê¸°ì¨': 'ğŸ˜Š', 'ìŠ¬í””': 'ğŸ˜¢', 'ë¶„ë…¸': 'ğŸ˜ ',\n",
        "        'ê³µí¬': 'ğŸ˜¨', 'ë†€ëŒ': 'ğŸ˜²', 'í˜ì˜¤': 'ğŸ¤¢', 'ì¤‘ë¦½': 'ğŸ˜'\n",
        "    }\n",
        "\n",
        "    for r in results:\n",
        "        emoji = emotion_emoji.get(r['emotion'], '')\n",
        "        graph_influence = r['graph_influence']\n",
        "        max_influence_idx = np.argmax(graph_influence)\n",
        "        influence_emotions = ['ê¸°ì¨', 'ìŠ¬í””', 'ë¶„ë…¸', 'ê³µí¬', 'ë†€ëŒ', 'í˜ì˜¤']\n",
        "\n",
        "        if r['emotion'] != 'ì¤‘ë¦½':\n",
        "            print(f\"  {r['token']:12s} â†’ {emoji} {r['emotion']:4s} \"\n",
        "                  f\"(ì‹ ë¢°ë„: {r['confidence']:.2%}, \"\n",
        "                  f\"ê·¸ë˜í”„ ì˜í–¥: {influence_emotions[max_influence_idx] if graph_influence[max_influence_idx] > 0 else 'ì—†ìŒ'})\")\n",
        "        else:\n",
        "            print(f\"  {r['token']:12s} â†’ {emoji} {r['emotion']:4s} ({r['confidence']:.2%})\")\n",
        "\n",
        "    # ê°ì • ë„¤íŠ¸ì›Œí¬ ìš”ì•½\n",
        "    print(\"\\nğŸŒ ê°ì • ë„¤íŠ¸ì›Œí¬ íŠ¹ì„±:\")\n",
        "    emotion_counts = {}\n",
        "    for r in results:\n",
        "        if r['emotion'] != 'ì¤‘ë¦½':\n",
        "            emotion_counts[r['emotion']] = emotion_counts.get(r['emotion'], 0) + 1\n",
        "\n",
        "    if emotion_counts:\n",
        "        for emotion, count in sorted(emotion_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"  - {emotion}: {count}ê°œ ë…¸ë“œê°€ ì—°ê²°ë¨\")\n",
        "\n",
        "# ë©”ì¸ ì‹¤í–‰\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ğŸŒ Graph Neural Network ê¸°ë°˜ ê°ì • ì „íŒŒ ëª¨ë¸\\n\")\n",
        "\n",
        "    # ì´ˆê¸°í™”\n",
        "    tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
        "    graph_builder = EmotionGraphBuilder(tokenizer)\n",
        "    model = GraphEmotionNetwork('klue/bert-base')\n",
        "    analyzer = GNNEmotionAnalyzer(model, tokenizer, graph_builder)\n",
        "\n",
        "    # ê°„ë‹¨ í›ˆë ¨\n",
        "    print(\"ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n",
        "    model = train_gnn_model(model, tokenizer, graph_builder, epochs=10)\n",
        "\n",
        "    # í…ŒìŠ¤íŠ¸\n",
        "    test_texts = [\n",
        "        \"ê¸°ìœ ì†Œì‹ì„ ë“£ê³  í–‰ë³µí•´ì„œ ëˆˆë¬¼ì´ ë‚¬ì–´ìš”\",\n",
        "        \"ë¬´ì„œìš´ ì´ì•¼ê¸°ë¥¼ ë“£ê³  ë„ˆë¬´ ë†€ë¼ê³  ë‘ë ¤ì› ì–´ìš”\",\n",
        "        \"í™”ê°€ ë‚˜ì„œ ì§œì¦ì´ ë‚˜ì§€ë§Œ ì°¸ê³  ìˆì–´ìš”\",\n",
        "        \"ë†€ë¼ìš´ ê²°ê³¼ì— ê¸°ì˜ë©´ì„œë„ ë¯¿ê¸°ì§€ ì•Šì•„ìš”\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ê·¸ë˜í”„ ê¸°ë°˜ ê°ì • ë¶„ì„ ê²°ê³¼\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for text in test_texts:\n",
        "        results, edges = analyzer.analyze(text)\n",
        "        display_gnn_results(text, results)\n",
        "        print(f\"  â†’ ê·¸ë˜í”„ ì—£ì§€ ìˆ˜: {len(edges)}ê°œ\")\n",
        "        print(\"-\"*60)\n",
        "\n",
        "        # ê·¸ë˜í”„ ì‹œê°í™” (ì„ íƒì )\n",
        "        # analyzer.visualize_emotion_graph(text, results, edges)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
