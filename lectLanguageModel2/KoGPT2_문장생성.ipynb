{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==== 설치(버전 호환 수정) ====\n",
        "!pip -q install torch==2.6.0 torchaudio==2.6.0 torchvision==0.21.0 --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "!pip -q install transformers datasets accelerate sentencepiece --upgrade\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBUQ0ZmtsxLd",
        "outputId": "bf90766b-b141-4582-c529-6a3ef7f319f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.7/374.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qmYyW3pq4I7",
        "outputId": "a4b653b6-7dc2-4526-9ac2-9689d622ec68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[프롬프트 1-1] 오늘은 날씨가 좋아서\n",
            "=> 오늘은 날씨가 좋아서 그런지 오늘 저녁이 아주 잘 먹었어요.\"\n",
            "\"그래, 그럼. 뭐든 다 먹어.\"\n",
            "나는 녀석이 내 앞에 놓인 맥주를 들이켰다.\n",
            "그것은 곧 녀석의 입에 들어가기 시작했다.\n",
            "\"너무 맛있어! 그냥 먹어도 된다니까?\"\n",
            "\"응,\n",
            "\n",
            "[프롬프트 2-1] 인공지능이 바꿀 미래는\n",
            "=> 인공지능이 바꿀 미래는 없다.\n",
            "우리가 사는 세상도 마찬가지다.\n",
            "모든 인간은 기계처럼 살고 싶어한다.\n",
            "하지만 사람들은 로봇을 좋아하지 않는다.\n",
            "그들은 우리를 위한 기계를 만들 수도 있고, 세상을 변화시키기 위해 다른 사람들을 도울 수도 있다.\n",
            "사람들은 우리가 하는 일을 스스로 만들고 싶다는 욕망에 사로잡힌다.\n",
            "그래서 그들은 결국 자신의 이익을 지키기 위해,\n",
            "\n",
            "[프롬프트 3-1] 한국 영화 역사에서 가장 인상 깊었던 장면은\n",
            "=> 한국 영화 역사에서 가장 인상 깊었던 장면은 바로 '장면'이다.\n",
            "19세기 전반까지만 해도 흑백으로만 이뤄졌던 흑백 영화의 시대는 20세기에 접어들면서 다시 하나의 물결이 일기 시작했다.\n",
            "영화가 가진 힘과 영향력은 21세기까지 지속된다.\n",
            "그렇게 되면 전 세계 관객은 영화나 TV 드라마, 다큐멘터리 등을 통해 보다 쉽게 자신의 정체성을 확인할 수 있게 된다.\n",
            "현재\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch, random, numpy as np\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "MODEL_NAME = \"skt/kogpt2-base-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "gen = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "prompts = [\n",
        "    \"오늘은 날씨가 좋아서\",\n",
        "    \"인공지능이 바꿀 미래는\",\n",
        "    \"한국 영화 역사에서 가장 인상 깊었던 장면은\"\n",
        "]\n",
        "\n",
        "outputs = gen(\n",
        "    prompts,\n",
        "    max_new_tokens=60,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        "    temperature=0.8,\n",
        "    repetition_penalty=1.2,\n",
        "    num_return_sequences=1,          # 한 프롬프트당 1개 생성\n",
        "    return_full_text=True            # 프롬프트 포함해 출력(원하면 False)\n",
        ")\n",
        "\n",
        "for i, outs in enumerate(outputs):   # outs 는 리스트\n",
        "    # 한 프롬프트당 여러 결과일 수 있으므로 다시 loop\n",
        "    for j, o in enumerate(outs):\n",
        "        print(f\"\\n[프롬프트 {i+1}-{j+1}] {prompts[i]}\\n=> {o['generated_text']}\")\n"
      ]
    }
  ]
}