{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 기계번역\n",
        "\n",
        "### pyTorch\n",
        "### encoder & decoder (GRU model base)\n",
        "\n",
        "GRU (Gated Recurrent Unit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PwGgmwbUVhZ",
        "outputId": "1d66a4f6-cc4c-4777-b4c5-a6e83f7779aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리 설치\n",
        "!pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miFTjKcZUTUw",
        "outputId": "a5c2401e-568f-470f-ade0-5e84b047f29c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "한국어 어휘 크기: 39\n",
            "영어 어휘 크기: 47\n",
            "번역 모델 훈련 시작...\n",
            "Epoch 0, Loss: 57.2339\n",
            "Epoch 200, Loss: 0.1105\n",
            "Epoch 400, Loss: 0.0465\n",
            "Epoch 600, Loss: 0.0288\n",
            "Epoch 800, Loss: 0.0206\n",
            "\n",
            "=== 번역 결과 ===\n",
            "한국어: 안녕하세요\n",
            "번역: hello\n",
            "--------------------------------------------------\n",
            "한국어: 고맙습니다\n",
            "번역: thank you\n",
            "--------------------------------------------------\n",
            "한국어: 나는 학생입니다\n",
            "번역: i am a student\n",
            "--------------------------------------------------\n",
            "한국어: 한국 음식을 좋아해요\n",
            "번역: i like korean food\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# 샘플 한영 번역 데이터 (더 많은 데이터로 확장)\n",
        "korean_sentences = [\n",
        "    \"안녕하세요\",\n",
        "    \"오늘 날씨가 좋습니다\",\n",
        "    \"나는 학생입니다\",\n",
        "    \"이 책이 재미있어요\",\n",
        "    \"내일 만나요\",\n",
        "    \"고맙습니다\",\n",
        "    \"죄송합니다\",\n",
        "    \"어디에 가세요\",\n",
        "    \"물 한 잔 주세요\",\n",
        "    \"시간이 몇 시예요\",\n",
        "    \"한국 음식을 좋아해요\",\n",
        "    \"영화를 보러 갑시다\",\n",
        "    \"공부를 열심히 해요\",\n",
        "    \"친구와 만났어요\",\n",
        "    \"집에 가고 싶어요\"\n",
        "]\n",
        "\n",
        "english_sentences = [\n",
        "    \"hello\",\n",
        "    \"the weather is nice today\",\n",
        "    \"i am a student\",\n",
        "    \"this book is interesting\",\n",
        "    \"see you tomorrow\",\n",
        "    \"thank you\",\n",
        "    \"i am sorry\",\n",
        "    \"where are you going\",\n",
        "    \"give me a glass of water\",\n",
        "    \"what time is it\",\n",
        "    \"i like korean food\",\n",
        "    \"let us go watch a movie\",\n",
        "    \"study hard\",\n",
        "    \"i met a friend\",\n",
        "    \"i want to go home\"\n",
        "]\n",
        "\n",
        "# 특수 토큰 정의\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "PAD_token = 2\n",
        "\n",
        "# 언어 클래스\n",
        "class Language:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {\"SOS\": 0, \"EOS\": 1, \"PAD\": 2}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n",
        "        self.n_words = 3\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "# 데이터 전처리\n",
        "def preprocess_sentence(sentence):\n",
        "    # 간단한 전처리: 소문자 변환, 구두점 제거\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"[.!?]\", \"\", sentence)\n",
        "    return sentence\n",
        "\n",
        "# 언어 객체 생성 및 어휘 구축\n",
        "input_lang = Language('korean')\n",
        "output_lang = Language('english')\n",
        "\n",
        "# 전처리된 문장 쌍\n",
        "pairs = []\n",
        "for ko, en in zip(korean_sentences, english_sentences):\n",
        "    ko_processed = preprocess_sentence(ko)\n",
        "    en_processed = preprocess_sentence(en)\n",
        "    pairs.append([ko_processed, en_processed])\n",
        "    input_lang.addSentence(ko_processed)\n",
        "    output_lang.addSentence(en_processed)\n",
        "\n",
        "print(f\"한국어 어휘 크기: {input_lang.n_words}\")\n",
        "print(f\"영어 어휘 크기: {output_lang.n_words}\")\n",
        "\n",
        "# 기본 인코더 모델\n",
        "class BasicEncoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(BasicEncoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        embedded = self.embedding(input_seq)\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "# 기본 디코더 모델\n",
        "class BasicDecoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(BasicDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input_token, hidden):\n",
        "        # input_token이 스칼라일 수 있으므로 명시적으로 차원 조정\n",
        "        if input_token.dim() == 0:\n",
        "            input_token = input_token.view(1, 1)\n",
        "        elif input_token.dim() == 1:\n",
        "            input_token = input_token.view(1, -1)\n",
        "\n",
        "        embedded = self.embedding(input_token)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden\n",
        "\n",
        "# 문장을 텐서로 변환\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index.get(word, 2) for word in sentence.split(' ')]  # 2는 PAD\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long)\n",
        "\n",
        "# 모델 초기화\n",
        "hidden_size = 256\n",
        "encoder = BasicEncoder(input_lang.n_words, hidden_size)\n",
        "decoder = BasicDecoder(hidden_size, output_lang.n_words)\n",
        "\n",
        "# 훈련 함수\n",
        "def train_pair(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "    encoder_hidden = torch.zeros(1, 1, encoder.hidden_size)\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    # 인코더 실행\n",
        "    input_tensor = input_tensor.unsqueeze(0)  # [1, seq_len]\n",
        "    encoder_output, encoder_hidden = encoder(input_tensor)\n",
        "\n",
        "    # 디코더 실행\n",
        "    decoder_input = torch.tensor([SOS_token])\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # Teacher forcing 사용\n",
        "    for di in range(target_length):\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "        loss += criterion(decoder_output.view(-1, decoder_output.size(-1)), target_tensor[di:di+1])\n",
        "        decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "# 훈련 설정\n",
        "learning_rate = 0.01\n",
        "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 훈련 실행\n",
        "print(\"번역 모델 훈련 시작...\")\n",
        "for epoch in range(1000):\n",
        "    total_loss = 0\n",
        "    for pair in pairs:\n",
        "        input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "        target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "        loss = train_pair(input_tensor, target_tensor, encoder, decoder,\n",
        "                         encoder_optimizer, decoder_optimizer, criterion)\n",
        "        total_loss += loss\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {total_loss:.4f}')\n",
        "\n",
        "# 번역 함수\n",
        "def translate(sentence, max_length=20):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_tensor = input_tensor.unsqueeze(0)  # [1, seq_len]\n",
        "\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor)\n",
        "\n",
        "        decoder_input = torch.tensor([SOS_token])\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "\n",
        "            if topi.item() == EOS_token:\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return ' '.join(decoded_words)\n",
        "\n",
        "# 테스트\n",
        "print(\"\\n=== 번역 결과 ===\")\n",
        "test_sentences = [\n",
        "    \"안녕하세요\",\n",
        "    \"고맙습니다\",\n",
        "    \"나는 학생입니다\",\n",
        "    \"한국 음식을 좋아해요\"\n",
        "]\n",
        "\n",
        "for korean in test_sentences:\n",
        "    english = translate(preprocess_sentence(korean))\n",
        "    print(f\"한국어: {korean}\")\n",
        "    print(f\"번역: {english}\")\n",
        "    print(\"-\" * 50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
