{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 개체명 인식(NER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSwU8aCVT--P",
        "outputId": "241cd61f-65fc-49a1-b62b-18c87e91246d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리 설치\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42Vq9juCTF6P",
        "outputId": "a13f56f2-48c1-40ec-b3b6-d2b77687e226"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "훈련 시작...\n",
            "Epoch 0, Loss: 9.5637\n",
            "Epoch 20, Loss: 0.0069\n",
            "Epoch 40, Loss: 0.0023\n",
            "Epoch 60, Loss: 0.0007\n",
            "Epoch 80, Loss: 0.0010\n",
            "\n",
            "훈련 정확도: 0.3448\n",
            "\n",
            "=== NER 예측 결과 ===\n",
            "입력: 손흥민은 토트넘에서 축구를 합니다\n",
            "결과: [('손흥민은', 'B-PER'), ('토트넘에서', 'B-ORG'), ('축구를', 'B-ORG'), ('합니다', 'B-ORG')]\n",
            "추출된 개체명: [('손흥민은', 'PER'), ('토트넘에서', 'ORG'), ('축구를', 'ORG'), ('합니다', 'ORG')]\n",
            "--------------------------------------------------\n",
            "입력: 김연아는 피겨스케이팅 선수입니다\n",
            "결과: [('김연아는', 'B-PER'), ('피겨스케이팅', 'B-FIELD'), ('선수입니다', 'B-ORG')]\n",
            "추출된 개체명: [('김연아는', 'PER'), ('피겨스케이팅', 'FIELD'), ('선수입니다', 'ORG')]\n",
            "--------------------------------------------------\n",
            "입력: 삼성전자에서 스마트폰을 개발합니다\n",
            "결과: [('삼성전자에서', 'B-ORG'), ('스마트폰을', 'B-ORG'), ('개발합니다', 'B-ORG')]\n",
            "추출된 개체명: [('삼성전자에서', 'ORG'), ('스마트폰을', 'ORG'), ('개발합니다', 'ORG')]\n",
            "--------------------------------------------------\n",
            "입력: BTS는 전세계적으로 유명한 그룹입니다\n",
            "결과: [('BTS', 'B-PER'), ('는', 'B-PER'), ('전세계적으로', 'B-ORG'), ('유명한', 'B-ORG'), ('그룹입니다', 'B-ORG')]\n",
            "추출된 개체명: [('BTS', 'PER'), ('는', 'PER'), ('전세계적으로', 'ORG'), ('유명한', 'ORG'), ('그룹입니다', 'ORG')]\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "\n",
        "# 샘플 데이터 준비 (단어 단위로 분리)\n",
        "sentences = [\n",
        "    [\"김철수\", \"는\", \"서울대학교\", \"에서\", \"컴퓨터과학\", \"을\", \"전공\", \"했습니다\"],\n",
        "    [\"이영희\", \"가\", \"부산\", \"에\", \"있는\", \"회사\", \"에\", \"취직\", \"했어요\"],\n",
        "    [\"박민수\", \"는\", \"삼성전자\", \"에서\", \"일\", \"합니다\"],\n",
        "    [\"최지영\", \"이\", \"구글\", \"코리아\", \"에\", \"지원\", \"했습니다\"],\n",
        "    [\"손흥민\", \"은\", \"토트넘\", \"에서\", \"축구\", \"를\", \"합니다\"],\n",
        "    [\"정현\", \"이\", \"윔블던\", \"테니스\", \"대회\", \"에\", \"출전\", \"했다\"],\n",
        "    [\"김연아\", \"는\", \"피겨스케이팅\", \"선수\", \"입니다\"],\n",
        "    [\"이순신\", \"장군\", \"은\", \"조선\", \"시대\", \"의\", \"인물\", \"이다\"]\n",
        "]\n",
        "\n",
        "\n",
        "# B-PER: 사람 이름의 시작\n",
        "# I-PER: 사람 이름의 연속\n",
        "# B-ORG: 조직명의 시작\n",
        "# O: 일반 단어\n",
        "# BIO 태깅 레이블\n",
        "ner_labels = [\n",
        "    [\"B-PER\", \"O\", \"B-ORG\", \"O\", \"B-FIELD\", \"O\", \"O\", \"O\"],\n",
        "    [\"B-PER\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n",
        "    [\"B-PER\", \"O\", \"B-ORG\", \"O\", \"O\", \"O\"],\n",
        "    [\"B-PER\", \"O\", \"B-ORG\", \"I-ORG\", \"O\", \"O\", \"O\"],\n",
        "    [\"B-PER\", \"O\", \"B-ORG\", \"O\", \"B-FIELD\", \"O\", \"O\"],\n",
        "    [\"B-PER\", \"O\", \"B-EVENT\", \"B-FIELD\", \"O\", \"O\", \"O\", \"O\"],\n",
        "    [\"B-PER\", \"O\", \"B-FIELD\", \"O\", \"O\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"O\"]\n",
        "]\n",
        "\n",
        "# 어휘 사전 구축\n",
        "def build_vocab(sentences):\n",
        "    word_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "    char_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "\n",
        "    # 단어 어휘 구축\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            if word not in word_to_idx:\n",
        "                word_to_idx[word] = len(word_to_idx)\n",
        "            # 문자 어휘 구축\n",
        "            for char in word:\n",
        "                if char not in char_to_idx:\n",
        "                    char_to_idx[char] = len(char_to_idx)\n",
        "\n",
        "    return word_to_idx, char_to_idx\n",
        "\n",
        "word_to_idx, char_to_idx = build_vocab(sentences)\n",
        "\n",
        "# 레이블 매핑\n",
        "label_names = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-FIELD\", \"I-FIELD\", \"B-EVENT\", \"I-EVENT\"]\n",
        "label_to_idx = {label: i for i, label in enumerate(label_names)}\n",
        "idx_to_label = {i: label for label, i in label_to_idx.items()}\n",
        "\n",
        "# BiLSTM 모델 (CRF 없이 단순화)\n",
        "class BiLSTM_NER(nn.Module):\n",
        "    def __init__(self, vocab_size, char_vocab_size, tagset_size, embedding_dim=100, hidden_dim=128, char_embedding_dim=25, char_hidden_dim=25):\n",
        "        super(BiLSTM_NER, self).__init__()\n",
        "\n",
        "        # 단어 임베딩\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # 문자 레벨 LSTM\n",
        "        self.char_embeds = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
        "        self.char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim, bidirectional=True, batch_first=True)\n",
        "\n",
        "        # 단어 레벨 BiLSTM\n",
        "        self.lstm = nn.LSTM(embedding_dim + char_hidden_dim * 2, hidden_dim // 2,\n",
        "                           bidirectional=True, batch_first=True)\n",
        "\n",
        "        # 드롭아웃\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # 출력층\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def get_char_features(self, chars):\n",
        "        # 문자 임베딩\n",
        "        char_embeds = self.char_embeds(chars)\n",
        "        char_lstm_out, _ = self.char_lstm(char_embeds)\n",
        "        # 마지막과 첫번째 hidden state 연결\n",
        "        char_features = torch.cat([char_lstm_out[:, -1, :char_lstm_out.size(2)//2],\n",
        "                                  char_lstm_out[:, 0, char_lstm_out.size(2)//2:]], dim=1)\n",
        "        return char_features\n",
        "\n",
        "    def forward(self, words, chars):\n",
        "        # 단어 임베딩\n",
        "        word_embeds = self.word_embeds(words)\n",
        "\n",
        "        # 문자 특성 추출\n",
        "        batch_size, seq_len, word_len = chars.size()\n",
        "        chars_flat = chars.view(-1, word_len)\n",
        "        char_features = self.get_char_features(chars_flat)\n",
        "        char_features = char_features.view(batch_size, seq_len, -1)\n",
        "\n",
        "        # 단어와 문자 특성 결합\n",
        "        combined_embeds = torch.cat([word_embeds, char_features], dim=2)\n",
        "        combined_embeds = self.dropout(combined_embeds)\n",
        "\n",
        "        # BiLSTM\n",
        "        lstm_out, _ = self.lstm(combined_embeds)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "\n",
        "        # 태그 점수\n",
        "        tag_scores = self.hidden2tag(lstm_out)\n",
        "\n",
        "        return tag_scores\n",
        "\n",
        "# 데이터셋 클래스\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, word_to_idx, char_to_idx, label_to_idx, max_word_len=10):\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.char_to_idx = char_to_idx\n",
        "        self.label_to_idx = label_to_idx\n",
        "        self.max_word_len = max_word_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label_seq = self.labels[idx]\n",
        "\n",
        "        # 단어를 인덱스로 변환\n",
        "        word_indices = [self.word_to_idx.get(word, self.word_to_idx[\"<UNK>\"]) for word in sentence]\n",
        "\n",
        "        # 문자를 인덱스로 변환\n",
        "        char_indices = []\n",
        "        for word in sentence:\n",
        "            word_chars = [self.char_to_idx.get(char, self.char_to_idx[\"<UNK>\"]) for char in word]\n",
        "            # 패딩 또는 자르기\n",
        "            if len(word_chars) < self.max_word_len:\n",
        "                word_chars.extend([self.char_to_idx[\"<PAD>\"]] * (self.max_word_len - len(word_chars)))\n",
        "            else:\n",
        "                word_chars = word_chars[:self.max_word_len]\n",
        "            char_indices.append(word_chars)\n",
        "\n",
        "        # 레이블을 인덱스로 변환\n",
        "        label_indices = [self.label_to_idx[label] for label in label_seq]\n",
        "\n",
        "        return {\n",
        "            'words': torch.LongTensor(word_indices),\n",
        "            'chars': torch.LongTensor(char_indices),\n",
        "            'labels': torch.LongTensor(label_indices)\n",
        "        }\n",
        "\n",
        "# 패딩 함수\n",
        "def collate_fn(batch):\n",
        "    # 최대 길이 찾기\n",
        "    max_len = max([len(item['words']) for item in batch])\n",
        "\n",
        "    # 패딩\n",
        "    words_batch = []\n",
        "    chars_batch = []\n",
        "    labels_batch = []\n",
        "    masks_batch = []\n",
        "\n",
        "    for item in batch:\n",
        "        words = item['words']\n",
        "        chars = item['chars']\n",
        "        labels = item['labels']\n",
        "\n",
        "        # 패딩\n",
        "        pad_len = max_len - len(words)\n",
        "        if pad_len > 0:\n",
        "            words = torch.cat([words, torch.zeros(pad_len, dtype=torch.long)])\n",
        "            chars = torch.cat([chars, torch.zeros(pad_len, chars.size(1), dtype=torch.long)])\n",
        "            labels = torch.cat([labels, torch.zeros(pad_len, dtype=torch.long)])\n",
        "\n",
        "        # 마스크 생성 (실제 단어에 대해서만 True)\n",
        "        mask = torch.cat([torch.ones(len(item['words'])), torch.zeros(pad_len)])\n",
        "\n",
        "        words_batch.append(words)\n",
        "        chars_batch.append(chars)\n",
        "        labels_batch.append(labels)\n",
        "        masks_batch.append(mask)\n",
        "\n",
        "    return {\n",
        "        'words': torch.stack(words_batch),\n",
        "        'chars': torch.stack(chars_batch),\n",
        "        'labels': torch.stack(labels_batch),\n",
        "        'mask': torch.stack(masks_batch).bool()\n",
        "    }\n",
        "\n",
        "# 데이터셋 및 데이터로더 생성\n",
        "dataset = NERDataset(sentences, ner_labels, word_to_idx, char_to_idx, label_to_idx)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# 모델 초기화\n",
        "model = BiLSTM_NER(len(word_to_idx), len(char_to_idx), len(label_to_idx))\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # 패딩 토큰 무시\n",
        "\n",
        "# 훈련\n",
        "print(\"훈련 시작...\")\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        words = batch['words']\n",
        "        chars = batch['chars']\n",
        "        labels = batch['labels']\n",
        "        mask = batch['mask']\n",
        "\n",
        "        # Forward pass\n",
        "        tag_scores = model(words, chars)\n",
        "\n",
        "        # 마스크된 위치만 고려하여 손실 계산\n",
        "        active_loss = mask.view(-1) == 1\n",
        "        active_logits = tag_scores.view(-1, len(label_to_idx))[active_loss]\n",
        "        active_labels = labels.view(-1)[active_loss]\n",
        "\n",
        "        loss = criterion(active_logits, active_labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {total_loss:.4f}')\n",
        "\n",
        "# 예측 함수\n",
        "def predict_ner(sentence):\n",
        "    model.eval()\n",
        "\n",
        "    # 문장 전처리\n",
        "    if isinstance(sentence, str):\n",
        "        # 간단한 한국어 토큰화 (실제로는 더 정교한 방법 필요)\n",
        "        words = re.findall(r'[가-힣]+|[a-zA-Z]+|[0-9]+', sentence)\n",
        "    else:\n",
        "        words = sentence\n",
        "\n",
        "    # 인덱스 변환\n",
        "    word_indices = [word_to_idx.get(word, word_to_idx[\"<UNK>\"]) for word in words]\n",
        "\n",
        "    char_indices = []\n",
        "    for word in words:\n",
        "        word_chars = [char_to_idx.get(char, char_to_idx[\"<UNK>\"]) for char in word]\n",
        "        if len(word_chars) < 10:\n",
        "            word_chars.extend([char_to_idx[\"<PAD>\"]] * (10 - len(word_chars)))\n",
        "        else:\n",
        "            word_chars = word_chars[:10]\n",
        "        char_indices.append(word_chars)\n",
        "\n",
        "    # 텐서로 변환\n",
        "    words_tensor = torch.LongTensor(word_indices).unsqueeze(0)\n",
        "    chars_tensor = torch.LongTensor(char_indices).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tag_scores = model(words_tensor, chars_tensor)\n",
        "        predictions = torch.argmax(tag_scores, dim=2)\n",
        "\n",
        "    # 결과 반환\n",
        "    result = []\n",
        "    for word, tag_idx in zip(words, predictions[0]):\n",
        "        result.append((word, idx_to_label[tag_idx.item()]))\n",
        "\n",
        "    return result\n",
        "\n",
        "# 정확도 계산 함수\n",
        "def calculate_accuracy(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            words = batch['words']\n",
        "            chars = batch['chars']\n",
        "            labels = batch['labels']\n",
        "            mask = batch['mask']\n",
        "\n",
        "            tag_scores = model(words, chars)\n",
        "            predictions = torch.argmax(tag_scores, dim=2)\n",
        "\n",
        "            # 마스크된 위치만 고려\n",
        "            active_mask = mask.view(-1) == 1\n",
        "            active_predictions = predictions.view(-1)[active_mask]\n",
        "            active_labels = labels.view(-1)[active_mask]\n",
        "\n",
        "            correct += (active_predictions == active_labels).sum().item()\n",
        "            total += active_labels.size(0)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "# 최종 정확도 출력\n",
        "accuracy = calculate_accuracy(model, dataloader)\n",
        "print(f\"\\n훈련 정확도: {accuracy:.4f}\")\n",
        "\n",
        "# 테스트\n",
        "print(\"\\n=== NER 예측 결과 ===\")\n",
        "test_sentences = [\n",
        "    \"손흥민은 토트넘에서 축구를 합니다\",\n",
        "    \"김연아는 피겨스케이팅 선수입니다\",\n",
        "    \"삼성전자에서 스마트폰을 개발합니다\",\n",
        "    \"BTS는 전세계적으로 유명한 그룹입니다\"\n",
        "]\n",
        "\n",
        "for test_sentence in test_sentences:\n",
        "    result = predict_ner(test_sentence)\n",
        "    print(f\"입력: {test_sentence}\")\n",
        "    print(f\"결과: {result}\")\n",
        "\n",
        "    # 개체명만 추출해서 보여주기\n",
        "    entities = []\n",
        "    for word, tag in result:\n",
        "        if tag.startswith('B-'):\n",
        "            entities.append((word, tag[2:]))\n",
        "    if entities:\n",
        "        print(f\"추출된 개체명: {entities}\")\n",
        "    print(\"-\" * 50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
