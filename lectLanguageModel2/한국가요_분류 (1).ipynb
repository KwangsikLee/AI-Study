{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 나눔고딕 폰트 설치 및 설정\n",
        "!apt-get update -qq\n",
        "!apt-get install fonts-nanum -qq\n",
        "!fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 폰트 설정\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path, size=10)\n",
        "plt.rcParams['font.family'] = 'NanumGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1LdTF0UNVal",
        "outputId": "1b982f2b-c7ac-42e1-a92d-d1215204e47c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 126380 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f612YlhPMqCi",
        "outputId": "8db8f3e6-634b-457e-bb18-130934c6deb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (496 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m496.6/496.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.6.0 konlpy-0.6.0\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (3.1.45)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.2)\n",
            " GPU 사용 가능\n",
            " K-Pop 가사 GRU 분류기 \n",
            "==================================================\n",
            " K-Pop 가사 데이터셋 로드...\n",
            "K-Pop 가사 데이터셋 다운로드 중...\n",
            "데이터셋 다운로드 완료!\n",
            "2010년부터 2023년까지의 데이터 로드 중...\n",
            "[data_parser] Parsed data to DataFrame: 2010 >> 2023.\n",
            "데이터 파서 방식 실패: Length mismatch: Expected axis has 0 elements, new values have 16 elements\n",
            "수동 방식으로 JSON 파일들을 파싱 중...\n",
            " 2017년 데이터 처리 중...\n",
            " 2011년 데이터 처리 중...\n",
            " 2022년 데이터 처리 중...\n",
            " 2016년 데이터 처리 중...\n",
            " 2012년 데이터 처리 중...\n",
            " 2018년 데이터 처리 중...\n",
            " 2013년 데이터 처리 중...\n",
            " 2014년 데이터 처리 중...\n",
            " 2015년 데이터 처리 중...\n",
            " 2021년 데이터 처리 중...\n",
            " 2020년 데이터 처리 중...\n",
            " 2023년 데이터 처리 중...\n",
            " 2019년 데이터 처리 중...\n",
            " 2010년 데이터 처리 중...\n",
            " 수동 로드 완료: 15744곡\n",
            " 데이터 후처리 중...\n",
            "중복 제거: 15744 → 4803곡\n",
            "최종 데이터: 4417곡\n",
            "장르별 분포:\n",
            "genre\n",
            "발라드           1527\n",
            "댄스            1187\n",
            "랩/힙합           677\n",
            "발라드, 국내드라마     391\n",
            "R&B/Soul       365\n",
            "록/메탈           214\n",
            "포크/블루스          56\n",
            "Name: count, dtype: int64\n",
            "\n",
            " 로드된 데이터 정보:\n",
            "  - 총 곡 수: 4,417곡\n",
            "  - 컬럼: ['title', 'artist', 'lyrics', 'genre', 'album', 'release_date', 'year', 'month', 'rank', 'lyric_writer', 'composer']\n",
            "  - 기간: 2010년 ~ 2023년\n",
            "\n",
            " 장르별 분포:\n",
            "  - 발라드: 1,527곡\n",
            "  - 댄스: 1,187곡\n",
            "  - 랩/힙합: 677곡\n",
            "  - 발라드, 국내드라마: 391곡\n",
            "  - R&B/Soul: 365곡\n",
            "  - 록/메탈: 214곡\n",
            "  - 포크/블루스: 56곡\n",
            "\n",
            " 분류기 초기화...\n",
            "\n",
            "  데이터 전처리...\n",
            " 데이터 전처리 시작...\n",
            " 데이터 정제 중...\n",
            "정제 후 데이터 크기: 4417\n",
            "장르별 분포:\n",
            "genre\n",
            "발라드           1527\n",
            "댄스            1187\n",
            "랩/힙합           677\n",
            "발라드, 국내드라마     391\n",
            "R&B/Soul       365\n",
            "록/메탈           214\n",
            "포크/블루스          56\n",
            "Name: count, dtype: int64\n",
            "최종 데이터 크기: 4417\n",
            "사용할 장르: ['발라드', '댄스', '랩/힙합', '발라드, 국내드라마', 'R&B/Soul', '록/메탈', '포크/블루스']\n",
            " 텍스트 전처리 중...\n",
            "전처리 후 데이터 크기: 4363\n",
            " 토크나이저 학습 중...\n",
            "전체 어휘 크기: 14260\n",
            "사용할 어휘 크기: 14260\n",
            " 시퀀스 변환 중...\n",
            "시퀀스 길이 - 평균: 125.5, 중간값: 116.0, 최대: 456\n",
            "\n",
            "상위 빈도 단어:\n",
            "  하다: 26897\n",
            "  사랑: 11650\n",
            "  있다: 9505\n",
            "  없다: 9242\n",
            "  보다: 9042\n",
            "  않다: 5165\n",
            "  그대: 4847\n",
            "  같다: 4634\n",
            "  우리: 4270\n",
            "  싶다: 3883\n",
            "전처리 완료:\n",
            "  - 입력 데이터 형태: (4363, 500)\n",
            "  - 클래스 수: 7\n",
            "  - 클래스: ['R&B/Soul', '댄스', '랩/힙합', '록/메탈', '발라드', '발라드, 국내드라마', '포크/블루스']\n",
            "\n",
            " 데이터 분할:\n",
            "  - 훈련 데이터: 3,490곡\n",
            "  - 테스트 데이터: 873곡\n",
            "\n",
            "  GRU 모델 구축...\n",
            "  고급 GRU 모델 시도...\n",
            "\n",
            " 모델 구축 성공!\n",
            "모델 정보:\n",
            "  - 파라미터 수: 6,394,823\n",
            "  - 입력 크기: 500\n",
            "  - 어휘 크기: 25000\n",
            "  - 클래스 수: 7\n",
            "  - 클래스: ['R&B/Soul', '댄스', '랩/힙합', '록/메탈', '발라드', '발라드, 국내드라마', '포크/블루스']\n",
            "\n",
            " 모델 레이어:\n",
            "  1. Embedding\n",
            "  2. Bidirectional\n",
            "  3. Bidirectional\n",
            "  4. Bidirectional\n",
            "  5. Dense\n",
            "  6. Dropout\n",
            "  7. Dense\n",
            "  8. Dropout\n",
            "  9. Dense\n",
            "  10. Dropout\n",
            "  11. Dense\n",
            "\n",
            " 모델 훈련 시작...\n",
            " 중간 규모 데이터 설정: 20 에포크, 배치 크기 8\n",
            " 첫 번째 훈련 시도...\n",
            " K-Pop 가사 분류 모델 훈련 시작...\n",
            "클래스 가중치 적용: 7개 클래스\n",
            "Epoch 1/20\n",
            "\u001b[1m  5/371\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37:30\u001b[0m 6s/step - accuracy: 0.2900 - loss: 2.4287 - top_k_accuracy: 0.5175\n",
            "  사용자에 의해 중단되었습니다.\n"
          ]
        }
      ],
      "source": [
        "# K-Pop 가사 데이터셋을 활용한 GRU 음악 분류기\n",
        "\n",
        "# 필요한 라이브러리 설치\n",
        "!pip install konlpy\n",
        "!pip install tensorflow\n",
        "!pip install scikit-learn\n",
        "!pip install pandas numpy matplotlib seaborn\n",
        "!pip install requests beautifulsoup4\n",
        "!pip install openpyxl\n",
        "!pip install gitpython\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Bidirectional, LSTM\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from konlpy.tag import Okt\n",
        "import re\n",
        "import warnings\n",
        "import os\n",
        "import json\n",
        "import git\n",
        "from collections import Counter\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 한글 폰트 설정 (Colab용)\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "\n",
        "class KPopLyricsDataLoader:\n",
        "    \"\"\"K-Pop 가사 데이터 로더 (EX3exp/Kpop-lyric-datasets 활용)\"\"\"\n",
        "\n",
        "    def __init__(self, repo_path=\"Kpop-lyric-datasets\"):\n",
        "        self.repo_path = repo_path\n",
        "        self.repo_url = \"https://github.com/EX3exp/Kpop-lyric-datasets.git\"\n",
        "\n",
        "    def clone_repository(self):\n",
        "        \"\"\"GitHub 레포지토리 클론\"\"\"\n",
        "        if not os.path.exists(self.repo_path):\n",
        "            print(\"K-Pop 가사 데이터셋 다운로드 중...\")\n",
        "            try:\n",
        "                # Git이 설치되어 있는지 확인\n",
        "                import subprocess\n",
        "                result = subprocess.run(['git', '--version'], capture_output=True, text=True)\n",
        "                if result.returncode != 0:\n",
        "                    raise Exception(\"Git이 설치되어 있지 않습니다.\")\n",
        "\n",
        "                # 레포지토리 클론\n",
        "                git.Repo.clone_from(self.repo_url, self.repo_path)\n",
        "                print(\"데이터셋 다운로드 완료!\")\n",
        "                return True\n",
        "\n",
        "            except ImportError:\n",
        "                print(\" gitpython 라이브러리가 없습니다.\")\n",
        "                print(\"다음 명령어로 설치해주세요: !pip install gitpython\")\n",
        "                return False\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\" 다운로드 실패: {e}\")\n",
        "                print(\"\\n 수동 해결 방법:\")\n",
        "                print(\"1. Colab에서 다음 명령어 실행:\")\n",
        "                print(f\"   !git clone {self.repo_url}\")\n",
        "                print(\"2. 또는 직접 GitHub에서 다운로드:\")\n",
        "                print(f\"   {self.repo_url}\")\n",
        "                print(\"3. 압축 해제 후 'Kpop-lyric-datasets' 폴더명 확인\")\n",
        "                return False\n",
        "        else:\n",
        "            print(\" 데이터셋이 이미 존재합니다.\")\n",
        "            return True\n",
        "\n",
        "    def load_data_parser_method(self, start_year=2010, end_year=2023):\n",
        "        \"\"\"데이터 파서를 사용한 데이터 로드 (원본 방식)\"\"\"\n",
        "        try:\n",
        "            # utils 모듈 임포트 시도\n",
        "            import sys\n",
        "            sys.path.append(self.repo_path)\n",
        "            from utils import data_parser\n",
        "\n",
        "            print(f\"{start_year}년부터 {end_year}년까지의 데이터 로드 중...\")\n",
        "            df = data_parser.get_df(start_year, end_year)\n",
        "            print(f\" 데이터 로드 완료: {len(df)}곡\")\n",
        "            return df\n",
        "\n",
        "        except ImportError as e:\n",
        "            print(f\" utils 모듈 로드 실패: {e}\")\n",
        "            print(\"대체 방법으로 데이터를 로드합니다...\")\n",
        "            return self.load_data_manual_method(start_year, end_year)\n",
        "\n",
        "    def load_data_manual_method(self, start_year=2010, end_year=2023):\n",
        "        \"\"\"수동으로 JSON 파일들을 파싱하여 데이터 로드\"\"\"\n",
        "        print(\"수동 방식으로 JSON 파일들을 파싱 중...\")\n",
        "\n",
        "        data_list = []\n",
        "        base_path = os.path.join(self.repo_path, \"melon\", \"monthly-chart\")\n",
        "\n",
        "        if not os.path.exists(base_path):\n",
        "            print(f\" 경로를 찾을 수 없습니다: {base_path}\")\n",
        "            return self.create_sample_data()\n",
        "\n",
        "        # 연도별 폴더 탐색\n",
        "        for year_folder in os.listdir(base_path):\n",
        "            try:\n",
        "                year = int(year_folder.split('-')[-1])  # melon-2020 -> 2020\n",
        "                if year < start_year or year > end_year:\n",
        "                    continue\n",
        "\n",
        "                year_path = os.path.join(base_path, year_folder)\n",
        "                if not os.path.isdir(year_path):\n",
        "                    continue\n",
        "\n",
        "                print(f\" {year}년 데이터 처리 중...\")\n",
        "\n",
        "                # 월별 폴더 탐색\n",
        "                for month_folder in os.listdir(year_path):\n",
        "                    month_path = os.path.join(year_path, month_folder)\n",
        "                    if not os.path.isdir(month_path):\n",
        "                        continue\n",
        "\n",
        "                    # JSON 파일들 처리\n",
        "                    for json_file in os.listdir(month_path):\n",
        "                        if not json_file.endswith('.json'):\n",
        "                            continue\n",
        "\n",
        "                        file_path = os.path.join(month_path, json_file)\n",
        "                        try:\n",
        "                            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                                song_data = json.load(f)\n",
        "\n",
        "                                # 가사 텍스트 추출\n",
        "                                lyrics_lines = song_data.get('lyrics', {}).get('lines', [])\n",
        "                                lyrics_text = ' '.join([line for line in lyrics_lines if line.strip()])\n",
        "\n",
        "                                # 빈 가사 제외\n",
        "                                if len(lyrics_text.strip()) < 10:\n",
        "                                    continue\n",
        "\n",
        "                                # 데이터 추출\n",
        "                                data_list.append({\n",
        "                                    'title': song_data.get('song_name', ''),\n",
        "                                    'artist': song_data.get('artist', ''),\n",
        "                                    'lyrics': lyrics_text,\n",
        "                                    'genre': song_data.get('genre', ''),\n",
        "                                    'album': song_data.get('album', ''),\n",
        "                                    'release_date': song_data.get('release_date', ''),\n",
        "                                    'year': song_data.get('info', [{}])[0].get('year', year),\n",
        "                                    'month': song_data.get('info', [{}])[0].get('month', 1),\n",
        "                                    'rank': song_data.get('info', [{}])[0].get('rank', 0),\n",
        "                                    'lyric_writer': song_data.get('lyric_writer', ''),\n",
        "                                    'composer': song_data.get('composer', '')\n",
        "                                })\n",
        "\n",
        "                        except (json.JSONDecodeError, KeyError, Exception) as e:\n",
        "                            print(f\"  파일 처리 오류 ({json_file}): {e}\")\n",
        "                            continue\n",
        "\n",
        "            except (ValueError, Exception) as e:\n",
        "                print(f\"  연도 폴더 처리 오류 ({year_folder}): {e}\")\n",
        "                continue\n",
        "\n",
        "        if not data_list:\n",
        "            print(\" 데이터를 로드할 수 없습니다. 샘플 데이터를 생성합니다.\")\n",
        "            return self.create_sample_data()\n",
        "\n",
        "        df = pd.DataFrame(data_list)\n",
        "        print(f\" 수동 로드 완료: {len(df)}곡\")\n",
        "        return df\n",
        "\n",
        "    def load_kpop_dataset(self, start_year=2010, end_year=2023):\n",
        "        \"\"\"K-Pop 데이터셋 로드 (메인 함수)\"\"\"\n",
        "        # 1. 레포지토리 클론\n",
        "        if not self.clone_repository():\n",
        "            return self.create_sample_data()\n",
        "\n",
        "        # 2. 데이터 파서 방식 시도\n",
        "        try:\n",
        "            df = self.load_data_parser_method(start_year, end_year)\n",
        "            if df is not None and len(df) > 0:\n",
        "                return self.process_dataframe(df)\n",
        "        except Exception as e:\n",
        "            print(f\"데이터 파서 방식 실패: {e}\")\n",
        "\n",
        "        # 3. 수동 방식으로 대체\n",
        "        df = self.load_data_manual_method(start_year, end_year)\n",
        "        return self.process_dataframe(df)\n",
        "\n",
        "    def process_dataframe(self, df):\n",
        "        \"\"\"데이터프레임 후처리\"\"\"\n",
        "        print(\" 데이터 후처리 중...\")\n",
        "\n",
        "        # 중복 제거\n",
        "        initial_count = len(df)\n",
        "        df = df.drop_duplicates(subset=['title', 'artist'], keep='first')\n",
        "        print(f\"중복 제거: {initial_count} → {len(df)}곡\")\n",
        "\n",
        "        # 빈 값 처리\n",
        "        df = df.dropna(subset=['lyrics', 'genre'])\n",
        "        df = df[df['lyrics'].str.len() > 20]  # 너무 짧은 가사 제외\n",
        "\n",
        "        # 장르 정리\n",
        "        df['genre'] = df['genre'].str.strip()\n",
        "        genre_counts = df['genre'].value_counts()\n",
        "\n",
        "        # 최소 곡 수 이상인 장르만 유지 (분류 성능을 위해)\n",
        "        min_songs = 50\n",
        "        valid_genres = genre_counts[genre_counts >= min_songs].index\n",
        "        df = df[df['genre'].isin(valid_genres)]\n",
        "\n",
        "        print(f\"최종 데이터: {len(df)}곡\")\n",
        "        print(f\"장르별 분포:\\n{df['genre'].value_counts()}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_sample_data(self):\n",
        "        \"\"\"샘플 데이터 생성 (백업용)\"\"\"\n",
        "        print(\"  실제 데이터를 로드할 수 없어 샘플 데이터를 생성합니다.\")\n",
        "\n",
        "        sample_data = {\n",
        "            'title': ['샘플곡1', '샘플곡2', '샘플곡3', '샘플곡4', '샘플곡5'] * 10,\n",
        "            'artist': ['아티스트A', '아티스트B', '아티스트C', '아티스트D', '아티스트E'] * 10,\n",
        "            'lyrics': [\n",
        "                '사랑하는 마음이 전해지길 바라며 오늘도 노래해',\n",
        "                '신나는 음악에 맞춰 모두 함께 춤춰봐 즐거운 하루',\n",
        "                '힘든 세상 속에서도 꿈을 포기하지 않고 앞으로 나아가',\n",
        "                '조용한 밤 혼자 앉아 그리운 사람을 생각해',\n",
        "                '랩으로 전하는 진실한 이야기 들어봐'\n",
        "            ] * 10,\n",
        "            'genre': ['발라드', '댄스', '힙합', '인디', '랩'] * 10,\n",
        "            'year': [2020, 2021, 2022, 2023, 2024] * 10\n",
        "        }\n",
        "\n",
        "        return pd.DataFrame(sample_data)\n",
        "\n",
        "class KoreanLyricsClassifier:\n",
        "    def __init__(self, max_features=20000, max_length=400, embedding_dim=150):\n",
        "        self.max_features = max_features\n",
        "        self.max_length = max_length\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.tokenizer = Tokenizer(num_words=max_features, oov_token=\"<OOV>\")\n",
        "        self.okt = Okt()\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"한글 텍스트 전처리 (개선된 버전)\"\"\"\n",
        "        if pd.isna(text) or text == \"\":\n",
        "            return \"\"\n",
        "\n",
        "        # 특수문자 제거 (한글, 영어, 숫자, 공백만 유지)\n",
        "        text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', str(text))\n",
        "\n",
        "        # 길이 제한 (너무 긴 텍스트 방지)\n",
        "        if len(text) > 5000:\n",
        "            text = text[:5000]\n",
        "\n",
        "        try:\n",
        "            # 형태소 분석 및 품사 태깅\n",
        "            morphs = self.okt.pos(text, stem=True)\n",
        "\n",
        "            # 의미있는 품사만 선별 (명사, 동사, 형용사, 부사)\n",
        "            meaningful_words = []\n",
        "            for word, pos in morphs:\n",
        "                if (pos in ['Noun', 'Verb', 'Adjective', 'Adverb'] and\n",
        "                    len(word) > 1 and\n",
        "                    not word.isdigit()):\n",
        "                    meaningful_words.append(word)\n",
        "\n",
        "            return ' '.join(meaningful_words)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"형태소 분석 오류: {e}\")\n",
        "            # 형태소 분석 실패시 간단한 전처리만 수행\n",
        "            words = text.split()\n",
        "            return ' '.join([word for word in words if len(word) > 1])\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        \"\"\"데이터 전처리 및 준비 (K-Pop 데이터 최적화)\"\"\"\n",
        "        print(\" 데이터 전처리 시작...\")\n",
        "\n",
        "        # 필수 컬럼 확인\n",
        "        if 'lyrics' not in df.columns or 'genre' not in df.columns:\n",
        "            print(\" 필수 컬럼 (lyrics, genre)이 없습니다.\")\n",
        "            print(f\"현재 컬럼: {df.columns.tolist()}\")\n",
        "            return None, None\n",
        "\n",
        "        # 데이터 정제\n",
        "        print(\" 데이터 정제 중...\")\n",
        "        df = df.dropna(subset=['lyrics', 'genre'])\n",
        "        df = df[df['lyrics'].str.len() > 20]  # 최소 길이 확보\n",
        "\n",
        "        print(f\"정제 후 데이터 크기: {len(df)}\")\n",
        "\n",
        "        # 장르별 분포 확인 및 균형 맞추기\n",
        "        genre_counts = df['genre'].value_counts()\n",
        "        print(f\"장르별 분포:\\n{genre_counts}\")\n",
        "\n",
        "        # 각 장르별로 최소 30개 이상의 곡이 있는지 확인\n",
        "        min_songs_per_genre = 30\n",
        "        valid_genres = genre_counts[genre_counts >= min_songs_per_genre].index\n",
        "        df = df[df['genre'].isin(valid_genres)]\n",
        "\n",
        "        if len(df) < 100:\n",
        "            print(\" 충분한 데이터가 없습니다. 최소 100곡 이상 필요합니다.\")\n",
        "            return None, None\n",
        "\n",
        "        print(f\"최종 데이터 크기: {len(df)}\")\n",
        "        print(f\"사용할 장르: {list(valid_genres)}\")\n",
        "\n",
        "        # 텍스트 전처리\n",
        "        print(\" 텍스트 전처리 중...\")\n",
        "        df['processed_lyrics'] = df['lyrics'].apply(self.preprocess_text)\n",
        "\n",
        "        # 전처리 후 빈 문자열 제거\n",
        "        df = df[df['processed_lyrics'].str.len() > 5]\n",
        "\n",
        "        print(f\"전처리 후 데이터 크기: {len(df)}\")\n",
        "\n",
        "        # 토크나이저 학습\n",
        "        print(\" 토크나이저 학습 중...\")\n",
        "        self.tokenizer.fit_on_texts(df['processed_lyrics'])\n",
        "\n",
        "        # 어휘 크기 정보\n",
        "        word_index = self.tokenizer.word_index\n",
        "        print(f\"전체 어휘 크기: {len(word_index)}\")\n",
        "        print(f\"사용할 어휘 크기: {min(len(word_index), self.max_features)}\")\n",
        "\n",
        "        # 텍스트를 시퀀스로 변환\n",
        "        print(\" 시퀀스 변환 중...\")\n",
        "        sequences = self.tokenizer.texts_to_sequences(df['processed_lyrics'])\n",
        "        X = pad_sequences(sequences, maxlen=self.max_length, padding='post', truncating='post')\n",
        "\n",
        "        # 레이블 인코딩\n",
        "        y = self.label_encoder.fit_transform(df['genre'])\n",
        "\n",
        "        # 시퀀스 길이 분석\n",
        "        seq_lengths = [len(seq) for seq in sequences]\n",
        "        print(f\"시퀀스 길이 - 평균: {np.mean(seq_lengths):.1f}, \"\n",
        "              f\"중간값: {np.median(seq_lengths):.1f}, \"\n",
        "              f\"최대: {np.max(seq_lengths)}\")\n",
        "\n",
        "        # 상위 빈도 단어 출력\n",
        "        print(\"\\n상위 빈도 단어:\")\n",
        "        word_freq = Counter()\n",
        "        for seq in sequences:\n",
        "            word_freq.update(seq)\n",
        "\n",
        "        top_words = word_freq.most_common(10)\n",
        "        word_to_index = {v: k for k, v in word_index.items()}\n",
        "        for word_id, freq in top_words:\n",
        "            if word_id in word_to_index:\n",
        "                print(f\"  {word_to_index[word_id]}: {freq}\")\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def build_advanced_model(self, num_classes):\n",
        "        \"\"\"K-Pop 데이터에 최적화된 고급 GRU 모델\"\"\"\n",
        "        model = Sequential([\n",
        "            # Embedding Layer\n",
        "            Embedding(\n",
        "                input_dim=self.max_features,\n",
        "                output_dim=self.embedding_dim,\n",
        "                input_length=self.max_length,\n",
        "                mask_zero=True\n",
        "            ),\n",
        "\n",
        "            # Multi-layer Bidirectional GRU\n",
        "            Bidirectional(GRU(256, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)),\n",
        "            Bidirectional(GRU(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)),\n",
        "            Bidirectional(GRU(64, dropout=0.3, recurrent_dropout=0.2)),\n",
        "\n",
        "            # Dense Layers with Regularization\n",
        "            Dense(256, activation='relu'),\n",
        "            Dropout(0.5),\n",
        "            Dense(128, activation='relu'),\n",
        "            Dropout(0.4),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            # Output Layer\n",
        "            Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        # 모델 빌드 (input_shape 명시적 지정)\n",
        "        model.build(input_shape=(None, self.max_length))\n",
        "\n",
        "        # 최적화된 컴파일 설정\n",
        "        optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=0.001,\n",
        "            clipnorm=1.0  # 그래디언트 클리핑\n",
        "        )\n",
        "\n",
        "        # 클래스 수에 따라 메트릭 선택\n",
        "        metrics = ['accuracy']\n",
        "        if num_classes > 3:  # 클래스가 3개 이상일 때만 top-k 메트릭 추가\n",
        "            metrics.append(tf.keras.metrics.SparseTopKCategoricalAccuracy(k=min(3, num_classes-1), name='top_k_accuracy'))\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=metrics\n",
        "        )\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "\n",
        "    def build_simple_model(self, num_classes):\n",
        "        \"\"\"간단한 GRU 모델 (백업용)\"\"\"\n",
        "        try:\n",
        "            print(\" 간단한 모델로 대체합니다...\")\n",
        "\n",
        "            model = Sequential([\n",
        "                Embedding(\n",
        "                    input_dim=self.max_features,\n",
        "                    output_dim=64,  # 더 작은 임베딩 차원\n",
        "                    input_length=self.max_length\n",
        "                ),\n",
        "                GRU(128, dropout=0.3, recurrent_dropout=0.2),\n",
        "                Dense(64, activation='relu'),\n",
        "                Dropout(0.5),\n",
        "                Dense(num_classes, activation='softmax')\n",
        "            ])\n",
        "\n",
        "            # 모델 빌드\n",
        "            model.build(input_shape=(None, self.max_length))\n",
        "\n",
        "            # 간단한 컴파일\n",
        "            model.compile(\n",
        "                optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "\n",
        "            self.model = model\n",
        "            print(\" 간단한 GRU 모델 생성 성공\")\n",
        "            return model\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" 간단한 모델 생성도 실패: {e}\")\n",
        "            self.model = None\n",
        "            raise e\n",
        "\n",
        "    def build_basic_model(self, num_classes):\n",
        "        \"\"\"기본 모델 (백업용) - build_advanced_model과 build_simple_model 사이에 추가\"\"\"\n",
        "        try:\n",
        "            print(\" 기본 모델로 대체합니다...\")\n",
        "\n",
        "            model = Sequential([\n",
        "                Embedding(\n",
        "                    input_dim=self.max_features,\n",
        "                    output_dim=100, # 중간 임베딩 차원\n",
        "                    input_length=self.max_length\n",
        "                ),\n",
        "                GRU(192, dropout=0.3, recurrent_dropout=0.2), # 중간 GRU 유닛\n",
        "                Dense(100, activation='relu'),\n",
        "                Dropout(0.5),\n",
        "                Dense(num_classes, activation='softmax')\n",
        "            ])\n",
        "\n",
        "            # 모델 빌드\n",
        "            model.build(input_shape=(None, self.max_length))\n",
        "\n",
        "            # 컴파일\n",
        "            model.compile(\n",
        "                optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "\n",
        "            self.model = model\n",
        "            print(\" 기본 GRU 모델 생성 성공\")\n",
        "            return model\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" 기본 모델 생성 실패: {e}\")\n",
        "            self.model = None\n",
        "            raise e\n",
        "\n",
        "\n",
        "    def build_ultra_simple_model(self, num_classes):\n",
        "        \"\"\"가장 간단한 모델 (최종 백업)\"\"\"\n",
        "        try:\n",
        "            print(\" 가장 간단한 모델을 생성합니다...\")\n",
        "\n",
        "            # 최소한의 설정\n",
        "            vocab_size = min(1000, self.max_features)\n",
        "            seq_length = min(50, self.max_length)\n",
        "\n",
        "            model = Sequential()\n",
        "            model.add(Embedding(vocab_size, 16, input_length=seq_length))\n",
        "            model.add(GRU(32))\n",
        "            model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "            # 수동으로 빌드\n",
        "            model.build(input_shape=(None, seq_length))\n",
        "\n",
        "            # 컴파일\n",
        "            model.compile(\n",
        "                optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "\n",
        "            self.model = model\n",
        "            print(\" 초간단 모델 생성 성공\")\n",
        "            return model\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" 초간단 모델도 실패: {e}\")\n",
        "            self.model = None\n",
        "            return None\n",
        "\n",
        "    def train_with_kpop_data(self, X, y, validation_split=0.2, epochs=100, batch_size=32):\n",
        "        \"\"\"K-Pop 데이터에 최적화된 훈련\"\"\"\n",
        "        print(\" K-Pop 가사 분류 모델 훈련 시작...\")\n",
        "\n",
        "        # 모델이 제대로 생성되었는지 확인\n",
        "        if self.model is None:\n",
        "            print(\" 모델이 생성되지 않았습니다. 훈련을 중단합니다.\")\n",
        "            raise ValueError(\"모델이 None입니다. 먼저 모델을 생성해주세요.\")\n",
        "\n",
        "        # 고급 콜백 설정\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=20,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.3,\n",
        "                patience=10,\n",
        "                min_lr=0.00001,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ModelCheckpoint(\n",
        "                'best_kpop_classifier.h5',\n",
        "                monitor='val_accuracy',\n",
        "                save_best_only=True,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # 클래스 가중치 계산 (불균형 데이터 대응)\n",
        "        try:\n",
        "            from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "            class_weights = compute_class_weight(\n",
        "                'balanced',\n",
        "                classes=np.unique(y),\n",
        "                y=y\n",
        "            )\n",
        "            class_weight_dict = dict(enumerate(class_weights))\n",
        "            print(f\"클래스 가중치 적용: {len(class_weight_dict)}개 클래스\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  클래스 가중치 계산 실패: {e}\")\n",
        "            print(\"균등 가중치로 훈련을 진행합니다.\")\n",
        "            class_weight_dict = None\n",
        "\n",
        "        # 모델 훈련\n",
        "        try:\n",
        "            history = self.model.fit(\n",
        "                X, y,\n",
        "                validation_split=validation_split,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                callbacks=callbacks,\n",
        "                class_weight=class_weight_dict,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            self.history = history\n",
        "            print(\" 모델 훈련 완료!\")\n",
        "            return history\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" 훈련 중 오류 발생: {e}\")\n",
        "            print(\"배치 크기를 줄이거나 에포크 수를 줄여서 다시 시도해보세요.\")\n",
        "            raise e\n",
        "\n",
        "    def predict_song_genre(self, lyrics):\n",
        "        \"\"\"가사로 장르 예측\"\"\"\n",
        "        if self.model is None:\n",
        "            return \"모델 없음\", 0.0, [(\"모델 없음\", 0.0)]\n",
        "\n",
        "        processed_lyrics = self.preprocess_text(lyrics)\n",
        "        if not processed_lyrics:\n",
        "            return \"알 수 없음\", 0.0, [(\"알 수 없음\", 0.0)]\n",
        "\n",
        "        try:\n",
        "            sequence = self.tokenizer.texts_to_sequences([processed_lyrics])\n",
        "            padded_sequence = pad_sequences(sequence, maxlen=self.max_length, padding='post', truncating='post')\n",
        "\n",
        "            prediction = self.model.predict(padded_sequence, verbose=0)\n",
        "            predicted_class = np.argmax(prediction, axis=1)[0]\n",
        "            confidence = np.max(prediction)\n",
        "\n",
        "            genre = self.label_encoder.inverse_transform([predicted_class])[0]\n",
        "\n",
        "            # Top 3 예측 결과도 반환\n",
        "            top_3_indices = np.argsort(prediction[0])[-3:][::-1]\n",
        "            top_3_genres = [(self.label_encoder.inverse_transform([idx])[0], prediction[0][idx])\n",
        "                           for idx in top_3_indices]\n",
        "\n",
        "            return genre, confidence, top_3_genres\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"예측 오류: {e}\")\n",
        "            return \"예측 실패\", 0.0, [(\"예측 실패\", 0.0)]\n",
        "\n",
        "    def analyze_model_performance(self, X_test, y_test):\n",
        "        \"\"\"모델 성능 상세 분석\"\"\"\n",
        "        print(\"\\n 모델 성능 분석 시작...\")\n",
        "\n",
        "        # 기본 평가\n",
        "        evaluation_results = self.model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "        # 결과 파싱\n",
        "        if isinstance(evaluation_results, list):\n",
        "            test_loss = evaluation_results[0]\n",
        "            test_accuracy = evaluation_results[1]\n",
        "            # top-k 정확도가 있는 경우\n",
        "            test_top_k = evaluation_results[2] if len(evaluation_results) > 2 else None\n",
        "        else:\n",
        "            test_loss = evaluation_results\n",
        "            test_accuracy = None\n",
        "            test_top_k = None\n",
        "\n",
        "\n",
        "        print(f\"테스트 손실: {test_loss:.4f}\")\n",
        "        if test_accuracy:\n",
        "            print(f\"테스트 정확도: {test_accuracy:.4f}\")\n",
        "        if test_top_k:\n",
        "            print(f\"Top-K 정확도: {test_top_k:.4f}\")\n",
        "\n",
        "        # 예측 수행\n",
        "        y_pred = self.model.predict(X_test, verbose=0)\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "        # 장르별 상세 분석\n",
        "        target_names = self.label_encoder.classes_\n",
        "        print(\"\\n 장르별 분류 리포트:\")\n",
        "        print(classification_report(y_test, y_pred_classes, target_names=target_names))\n",
        "\n",
        "        # 혼동 행렬 시각화\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        cm = confusion_matrix(y_test, y_pred_classes)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=target_names, yticklabels=target_names)\n",
        "        plt.title('K-Pop 장르 분류 혼동 행렬')\n",
        "        plt.xlabel('예측 장르')\n",
        "        plt.ylabel('실제 장르')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return test_accuracy if test_accuracy else test_loss, y_pred_classes\n",
        "\n",
        "    def plot_training_history_advanced(self):\n",
        "        \"\"\"고급 훈련 과정 시각화\"\"\"\n",
        "        if self.history is None:\n",
        "            print(\"훈련 기록이 없습니다.\")\n",
        "            return\n",
        "\n",
        "        # 사용 가능한 메트릭 확인\n",
        "        available_metrics = list(self.history.history.keys())\n",
        "        has_top_k = any('top_k' in metric for metric in available_metrics)\n",
        "\n",
        "        # 서브플롯 개수 결정\n",
        "        if has_top_k:\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        else:\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "            # 1x2 배치인 경우 axes를 2D로 변환\n",
        "            axes = axes.reshape(1, -1)\n",
        "\n",
        "        # 손실 그래프\n",
        "        axes[0, 0].plot(self.history.history['loss'], label='Training Loss', linewidth=2)\n",
        "        axes[0, 0].plot(self.history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "        axes[0, 0].set_title('Model Loss')\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # 정확도 그래프\n",
        "        axes[0, 1].plot(self.history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
        "        axes[0, 1].plot(self.history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "        axes[0, 1].set_title('Model Accuracy')\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('Accuracy')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Top-K 정확도 (있는 경우)\n",
        "        if has_top_k:\n",
        "            top_k_metric = [m for m in available_metrics if 'top_k' in m and 'val_' not in m][0]\n",
        "            val_top_k_metric = f'val_{top_k_metric}'\n",
        "\n",
        "            if top_k_metric in self.history.history and val_top_k_metric in self.history.history:\n",
        "                axes[1, 0].plot(self.history.history[top_k_metric],\n",
        "                               label='Training Top-K Acc', linewidth=2)\n",
        "                axes[1, 0].plot(self.history.history[val_top_k_metric],\n",
        "                               label='Validation Top-K Acc', linewidth=2)\n",
        "                axes[1, 0].set_title('Top-K Accuracy')\n",
        "                axes[1, 0].set_xlabel('Epoch')\n",
        "                axes[1, 0].set_ylabel('Top-K Accuracy')\n",
        "                axes[1, 0].legend()\n",
        "                axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            # 성능 요약\n",
        "            axes[1, 1].text(0.5, 0.5, 'K-Pop 가사 분류기\\n성능 요약',\n",
        "                            horizontalalignment='center', verticalalignment='center',\n",
        "                            transform=axes[1, 1].transAxes, fontsize=14)\n",
        "            axes[1, 1].axis('off')\n",
        "\n",
        "        plt.suptitle('K-Pop 가사 분류 모델 훈련 결과', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # 최고 성능 출력\n",
        "        best_val_acc = max(self.history.history['val_accuracy'])\n",
        "        best_epoch = self.history.history['val_accuracy'].index(best_val_acc) + 1\n",
        "        print(f\" 최고 검증 정확도: {best_val_acc:.4f} (에포크 {best_epoch})\")\n",
        "\n",
        "        # Top-K 정확도 최고치 출력 (있는 경우)\n",
        "        if has_top_k:\n",
        "            top_k_metric = [m for m in available_metrics if 'val_' in m and 'top_k' in m]\n",
        "            if top_k_metric:\n",
        "                best_val_top_k = max(self.history.history[top_k_metric[0]])\n",
        "                best_top_k_epoch = self.history.history[top_k_metric[0]].index(best_val_top_k) + 1\n",
        "                print(f\" 최고 Top-K 정확도: {best_val_top_k:.4f} (에포크 {best_top_k_epoch})\")\n",
        "\n",
        "def main():\n",
        "    print(\" K-Pop 가사 GRU 분류기 \")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 1. K-Pop 데이터셋 로드\n",
        "    print(\" K-Pop 가사 데이터셋 로드...\")\n",
        "    data_loader = KPopLyricsDataLoader()\n",
        "\n",
        "    # 2010년부터 2023년까지의 데이터 로드\n",
        "    df = data_loader.load_kpop_dataset(start_year=2010, end_year=2023)\n",
        "\n",
        "    if df is None or len(df) == 0:\n",
        "        print(\" 데이터를 로드할 수 없습니다.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\n 로드된 데이터 정보:\")\n",
        "    print(f\"  - 총 곡 수: {len(df):,}곡\")\n",
        "    print(f\"  - 컬럼: {list(df.columns)}\")\n",
        "    print(f\"  - 기간: {df['year'].min()}년 ~ {df['year'].max()}년\")\n",
        "\n",
        "    if 'genre' in df.columns:\n",
        "        print(f\"\\n 장르별 분포:\")\n",
        "        genre_dist = df['genre'].value_counts()\n",
        "        for genre, count in genre_dist.head(10).items():\n",
        "            print(f\"  - {genre}: {count:,}곡\")\n",
        "\n",
        "    # 2. 분류기 초기화 및 데이터 전처리\n",
        "    print(f\"\\n 분류기 초기화...\")\n",
        "    classifier = KoreanLyricsClassifier(\n",
        "        max_features=25000,  # K-Pop 데이터에 맞게 증가\n",
        "        max_length=500,      # 가사 길이에 맞게 조정\n",
        "        embedding_dim=200    # 임베딩 차원 증가\n",
        "    )\n",
        "\n",
        "    # 3. 데이터 전처리\n",
        "    print(f\"\\n  데이터 전처리...\")\n",
        "    X, y = classifier.prepare_data(df)\n",
        "\n",
        "    if X is None or y is None:\n",
        "        print(\" 데이터 전처리 실패\")\n",
        "        return None\n",
        "\n",
        "    print(f\"전처리 완료:\")\n",
        "    print(f\"  - 입력 데이터 형태: {X.shape}\")\n",
        "    print(f\"  - 클래스 수: {len(np.unique(y))}\")\n",
        "    print(f\"  - 클래스: {list(classifier.label_encoder.classes_)}\")\n",
        "\n",
        "    # 4. 훈련/테스트 데이터 분할\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"\\n 데이터 분할:\")\n",
        "    print(f\"  - 훈련 데이터: {X_train.shape[0]:,}곡\")\n",
        "    print(f\"  - 테스트 데이터: {X_test.shape[0]:,}곡\")\n",
        "\n",
        "    # 5. 모델 구축 (4단계 백업 시스템)\n",
        "    print(f\"\\n  GRU 모델 구축...\")\n",
        "    num_classes = len(np.unique(y))\n",
        "    model = None\n",
        "\n",
        "    # 1단계: 고급 모델 시도\n",
        "    try:\n",
        "        print(\"  고급 GRU 모델 시도...\")\n",
        "        model = classifier.build_advanced_model(num_classes)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  고급 모델 구축 실패: {e}\")\n",
        "\n",
        "        # 2단계: 간단한 모델 시도\n",
        "        try:\n",
        "            print(\"  간단한 GRU 모델 시도...\")\n",
        "            model = classifier.build_simple_model(num_classes)\n",
        "\n",
        "        except Exception as e2:\n",
        "            print(f\"  간단한 모델 구축 실패: {e2}\")\n",
        "\n",
        "            # 3단계: 기본 모델 시도\n",
        "            try:\n",
        "                print(\"  기본 모델 시도...\")\n",
        "                model = classifier.build_basic_model(num_classes)\n",
        "\n",
        "            except Exception as e3:\n",
        "                print(f\"  기본 모델 구축 실패: {e3}\")\n",
        "\n",
        "                # 4단계: 초간단 모델 시도\n",
        "                try:\n",
        "                    print(\"  초간단 모델 시도...\")\n",
        "                    model = classifier.build_ultra_simple_model(num_classes)\n",
        "\n",
        "                except Exception as e4:\n",
        "                    print(f\" 모든 모델 구축 실패: {e4}\")\n",
        "                    print(\"\\n 모든 모델 생성이 실패했습니다.\")\n",
        "                    print(\"환경 문제일 가능성이 높습니다. 다음을 시도해보세요:\")\n",
        "                    print(\"1. 런타임 재시작\")\n",
        "                    print(\"2. TensorFlow 재설치: !pip install --upgrade tensorflow\")\n",
        "                    print(\"3. 가상환경 재설정\")\n",
        "\n",
        "                    # 실패해도 데이터는 반환\n",
        "                    return None, df\n",
        "\n",
        "    # 모델 생성 성공 확인\n",
        "    if model is None or classifier.model is None:\n",
        "        print(\" 모델 생성에 완전히 실패했습니다.\")\n",
        "        print(\"데이터만 반환합니다.\")\n",
        "        return None, df\n",
        "\n",
        "    # 모델 정보 출력\n",
        "    try:\n",
        "        param_count = model.count_params()\n",
        "        print(f\"\\n 모델 구축 성공!\")\n",
        "        print(f\"모델 정보:\")\n",
        "        print(f\"  - 파라미터 수: {param_count:,}\")\n",
        "        print(f\"  - 입력 크기: {classifier.max_length}\")\n",
        "        print(f\"  - 어휘 크기: {classifier.max_features}\")\n",
        "        print(f\"  - 클래스 수: {num_classes}\")\n",
        "        print(f\"  - 클래스: {list(classifier.label_encoder.classes_)}\")\n",
        "\n",
        "        # 모델 레이어 정보만 간단히 출력\n",
        "        print(f\"\\n 모델 레이어:\")\n",
        "        for i, layer in enumerate(model.layers):\n",
        "            print(f\"  {i+1}. {layer.__class__.__name__}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  모델 정보 출력 오류: {e}\")\n",
        "        print(\"모델은 정상적으로 생성되었지만 정보 출력에 문제가 있습니다.\")\n",
        "        print(\"훈련을 계속 진행합니다...\")\n",
        "\n",
        "    # 6. 모델 훈련\n",
        "    if classifier.model is None:\n",
        "        print(\" 모델이 생성되지 않아 훈련을 건너뜁니다.\")\n",
        "        print(\"데이터 분석 결과만 제공합니다.\")\n",
        "        return None, df\n",
        "\n",
        "    print(f\"\\n 모델 훈련 시작...\")\n",
        "\n",
        "    # 데이터 크기에 따른 하이퍼파라미터 조정\n",
        "    data_size = len(X_train)\n",
        "    if data_size < 1000:\n",
        "        epochs, batch_size = 10, 4  # 매우 보수적인 설정\n",
        "        print(f\" 소규모 데이터 설정: {epochs} 에포크, 배치 크기 {batch_size}\")\n",
        "    elif data_size < 5000:\n",
        "        epochs, batch_size = 20, 8\n",
        "        print(f\" 중간 규모 데이터 설정: {epochs} 에포크, 배치 크기 {batch_size}\")\n",
        "    else:\n",
        "        epochs, batch_size = 30, 16  # 더 작은 배치로 안전하게\n",
        "        print(f\" 대규모 데이터 설정: {epochs} 에포크, 배치 크기 {batch_size}\")\n",
        "\n",
        "    history = None\n",
        "\n",
        "    try:\n",
        "        # 첫 번째 시도\n",
        "        print(\" 첫 번째 훈련 시도...\")\n",
        "        history = classifier.train_with_kpop_data(\n",
        "            X_train, y_train,\n",
        "            validation_split=0.15,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  첫 번째 훈련 시도 실패: {e}\")\n",
        "        print(\"더 보수적인 설정으로 재시도합니다...\")\n",
        "\n",
        "        try:\n",
        "            # 두 번째 시도 (더 작은 배치, 적은 에포크)\n",
        "            print(\" 두 번째 훈련 시도...\")\n",
        "            history = classifier.train_with_kpop_data(\n",
        "                X_train, y_train,\n",
        "                validation_split=0.1,\n",
        "                epochs=max(5, epochs//2),\n",
        "                batch_size=max(2, batch_size//2)\n",
        "            )\n",
        "\n",
        "        except Exception as e2:\n",
        "            print(f\" 두 번째 훈련 시도도 실패: {e2}\")\n",
        "            print(\"훈련 없이 모델만 반환합니다.\")\n",
        "            return classifier, df\n",
        "\n",
        "    if history is None:\n",
        "        print(\" 훈련에 완전히 실패했습니다.\")\n",
        "        print(\"훈련되지 않은 모델을 반환합니다.\")\n",
        "        return classifier, df\n",
        "\n",
        "    # 7. 훈련 과정 시각화\n",
        "    if history is not None:\n",
        "        print(f\"\\n 훈련 과정 시각화...\")\n",
        "        try:\n",
        "            classifier.plot_training_history_advanced()\n",
        "        except Exception as e:\n",
        "            print(f\"  시각화 오류: {e}\")\n",
        "    else:\n",
        "        print(\"  훈련 기록이 없어 시각화를 건너뜁니다.\")\n",
        "\n",
        "    # 8. 모델 평가\n",
        "    if classifier.model is not None and history is not None:\n",
        "        print(f\"\\n 모델 평가...\")\n",
        "        try:\n",
        "            test_accuracy, y_pred = classifier.analyze_model_performance(X_test, y_test)\n",
        "        except Exception as e:\n",
        "            print(f\"  모델 평가 오류: {e}\")\n",
        "            test_accuracy = 0.0\n",
        "    else:\n",
        "        print(\"  모델이나 훈련 기록이 없어 평가를 건너뜁니다.\")\n",
        "        test_accuracy = 0.0\n",
        "\n",
        "    # 9. 실제 예측 테스트\n",
        "    if classifier.model is not None:\n",
        "        print(f\"\\n 가사 장르 예측 테스트:\")\n",
        "\n",
        "        test_cases = [\n",
        "            \"사랑하는 사람아 내 마음을 받아줘 영원히 함께하자\",\n",
        "            \"모두 일어나 손을 들어 신나게 춤을 춰봐 파티타임\",\n",
        "            \"힘든 세상 속에서 꿈을 잃지 말고 끝까지 버텨내자\",\n",
        "            \"조용한 밤 기타 소리에 맞춰 부르는 노래\",\n",
        "            \"마이크 잡고 무대 위에서 랩으로 외치는 진실\"\n",
        "        ]\n",
        "\n",
        "        for i, lyrics in enumerate(test_cases, 1):\n",
        "            try:\n",
        "                genre, confidence, top_3 = classifier.predict_song_genre(lyrics)\n",
        "                print(f\"\\n{i}. 테스트 가사: '{lyrics[:30]}...'\")\n",
        "                print(f\"    예측 장르: {genre}\")\n",
        "                print(f\"    신뢰도: {confidence:.3f}\")\n",
        "                print(f\"    Top 3: {[(g, f'{c:.3f}') for g, c in top_3]}\")\n",
        "            except Exception as e:\n",
        "                print(f\"{i}. 예측 오류: {e}\")\n",
        "    else:\n",
        "        print(\"  모델이 없어 예측 테스트를 건너뜁니다.\")\n",
        "\n",
        "    # 10. 모델 저장\n",
        "    if classifier.model is not None:\n",
        "        try:\n",
        "            classifier.model.save('kpop_lyrics_classifier_advanced.h5')\n",
        "            print(f\"\\n 모델 저장 완료: 'kpop_lyrics_classifier_advanced.h5'\")\n",
        "        except Exception as e:\n",
        "            print(f\"  모델 저장 오류: {e}\")\n",
        "    else:\n",
        "        print(\"  저장할 모델이 없습니다.\")\n",
        "\n",
        "    # 11. 최종 결과\n",
        "    if classifier.model is not None and history is not None:\n",
        "        print(f\"\\n K-Pop 가사 분류기 학습 완료!\")\n",
        "        print(f\"최종 테스트 정확도: {test_accuracy:.4f}\")\n",
        "        return classifier, df\n",
        "    else:\n",
        "        print(f\"\\n K-Pop 가사 분류기 학습 실패\")\n",
        "        print(\"모델이나 훈련이 완료되지 않았습니다.\")\n",
        "        return None, df\n",
        "\n",
        "# 추가 유틸리티 함수들\n",
        "def analyze_kpop_trends(df):\n",
        "    \"\"\"K-Pop 트렌드 분석\"\"\"\n",
        "    if 'year' not in df.columns:\n",
        "        return\n",
        "\n",
        "    print(\" K-Pop 장르 트렌드 분석\")\n",
        "\n",
        "    # 연도별 장르 분포\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    pivot_data = df.groupby(['year', 'genre']).size().unstack(fill_value=0)\n",
        "    pivot_data.plot(kind='area', stacked=True, alpha=0.7)\n",
        "\n",
        "    plt.title('연도별 K-Pop 장르 분포 변화')\n",
        "    plt.xlabel('연도')\n",
        "    plt.ylabel('곡 수')\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def export_predictions(classifier, df, output_file='kpop_predictions.csv'):\n",
        "    \"\"\"예측 결과를 CSV로 내보내기\"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        genre, confidence, top_3 = classifier.predict_song_genre(row['lyrics'])\n",
        "        predictions.append({\n",
        "            'title': row['title'],\n",
        "            'artist': row['artist'],\n",
        "            'actual_genre': row['genre'],\n",
        "            'predicted_genre': genre,\n",
        "            'confidence': confidence,\n",
        "            'top_3_predictions': str(top_3)\n",
        "        })\n",
        "\n",
        "    pred_df = pd.DataFrame(predictions)\n",
        "    pred_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
        "    print(f\" 예측 결과 저장: {output_file}\")\n",
        "\n",
        "# 간단한 테스트 함수 추가\n",
        "def quick_test():\n",
        "    \"\"\"빠른 테스트용 함수 (최소 설정)\"\"\"\n",
        "    print(\" 빠른 테스트 모드 시작...\")\n",
        "\n",
        "    try:\n",
        "        # 최소 설정으로 분류기 생성\n",
        "        classifier = KoreanLyricsClassifier(\n",
        "            max_features=1000,\n",
        "            max_length=50,\n",
        "            embedding_dim=32\n",
        "        )\n",
        "\n",
        "        # 샘플 데이터 생성\n",
        "        data_loader = KPopLyricsDataLoader()\n",
        "        df = data_loader.create_sample_data()\n",
        "\n",
        "        print(f\"샘플 데이터: {len(df)}곡\")\n",
        "\n",
        "        # 데이터 전처리\n",
        "        X, y = classifier.prepare_data(df)\n",
        "        if X is None:\n",
        "            print(\" 데이터 전처리 실패\")\n",
        "            return None, df\n",
        "\n",
        "        # 초간단 모델 생성\n",
        "        num_classes = len(np.unique(y))\n",
        "        model = classifier.build_ultra_simple_model(num_classes)\n",
        "\n",
        "        if model is None:\n",
        "            print(\" 모델 생성 실패\")\n",
        "            return None, df\n",
        "\n",
        "        print(\" 빠른 테스트 성공!\")\n",
        "        print(\"실제 데이터로 main() 함수를 실행해보세요.\")\n",
        "\n",
        "        return classifier, df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" 빠른 테스트 실패: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def safe_main():\n",
        "    \"\"\"안전한 main 함수 (오류 발생시 None, None 반환)\"\"\"\n",
        "    try:\n",
        "        return main()\n",
        "    except Exception as e:\n",
        "        print(f\" main() 함수 실행 실패: {e}\")\n",
        "        print(\"\\n 해결 방법:\")\n",
        "        print(\"1. quick_test() 함수로 먼저 테스트\")\n",
        "        print(\"2. 런타임 재시작 후 재실행\")\n",
        "        print(\"3. 라이브러리 재설치\")\n",
        "        return None, None\n",
        "\n",
        "# 실행 코드\n",
        "if __name__ == \"__main__\":\n",
        "    # TensorFlow GPU 설정 (선택사항)\n",
        "    try:\n",
        "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "            print(\" GPU 사용 가능\")\n",
        "        else:\n",
        "            print(\" CPU 모드로 실행\")\n",
        "    except Exception as e:\n",
        "        print(f\"  GPU 설정 오류: {e}\")\n",
        "        print(\" CPU 모드로 실행\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        # 안전한 메인 함수 실행\n",
        "        result = safe_main()\n",
        "\n",
        "        if result is not None and result[0] is not None and result[1] is not None:\n",
        "            classifier, dataset = result\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\" 추가 분석 실행...\")\n",
        "\n",
        "            # K-Pop 트렌드 분석\n",
        "            try:\n",
        "                if 'year' in dataset.columns:\n",
        "                    analyze_kpop_trends(dataset)\n",
        "                else:\n",
        "                    print(\"  연도 정보가 없어 트렌드 분석을 건너뜁니다.\")\n",
        "            except Exception as e:\n",
        "                print(f\"  트렌드 분석 오류: {e}\")\n",
        "\n",
        "            print(\"\\n 실행 완료!\")\n",
        "\n",
        "        else:\n",
        "            print(\"\\n 실행 실패\")\n",
        "            print(\" 빠른 테스트를 시도해보세요:\")\n",
        "            print(\"quick_test()\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n  사용자에 의해 중단되었습니다.\")\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\" 라이브러리 import 오류: {e}\")\n",
        "        print(\"\\n 해결 방법:\")\n",
        "        print(\"1. 필수 라이브러리 설치:\")\n",
        "        print(\"   !pip install konlpy tensorflow scikit-learn pandas numpy matplotlib seaborn\")\n",
        "        print(\"2. KoNLPy 추가 설정 (Ubuntu/Colab):\")\n",
        "        print(\"   !apt-get install g++ openjdk-8-jdk-headless -qq\")\n",
        "        print(\"3. 런타임 재시작 후 다시 실행\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" 예상치 못한 오류 발생: {e}\")\n",
        "        print(f\"오류 타입: {type(e).__name__}\")\n",
        "\n",
        "        # 구체적인 해결 방법 제시\n",
        "        if \"unpack\" in str(e) or \"NoneType\" in str(e):\n",
        "            print(\"\\n 언패킹 오류 해결 방법:\")\n",
        "            print(\"1. 다음과 같이 안전하게 실행:\")\n",
        "            print(\"   result = safe_main()\")\n",
        "            print(\"   if result[0] is not None:\")\n",
        "            print(\"       classifier, dataset = result\")\n",
        "            print(\"2. 또는 빠른 테스트:\")\n",
        "            print(\"   quick_test()\")\n",
        "\n",
        "        elif \"top_k\" in str(e).lower():\n",
        "            print(\"\\n Top-K 메트릭 오류 해결 방법:\")\n",
        "            print(\"1. 클래스 수가 너무 적을 수 있습니다.\")\n",
        "            print(\"2. 더 많은 장르의 데이터를 추가해보세요.\")\n",
        "            print(\"3. 또는 단순한 정확도 메트릭만 사용해보세요.\")\n",
        "\n",
        "        elif \"memory\" in str(e).lower() or \"resource\" in str(e).lower():\n",
        "            print(\"\\n 메모리 부족 해결 방법:\")\n",
        "            print(\"1. 배치 크기를 줄여보세요: batch_size=2\")\n",
        "            print(\"2. max_features를 줄여보세요: max_features=1000\")\n",
        "            print(\"3. max_length를 줄여보세요: max_length=50\")\n",
        "            print(\"4. 런타임을 GPU로 변경해보세요.\")\n",
        "\n",
        "        else:\n",
        "            print(\"\\n 일반적인 문제 해결 방법:\")\n",
        "            print(\"1. 런타임 재시작: 런타임 → 런타임 재시작\")\n",
        "            print(\"2. 필수 라이브러리 재설치:\")\n",
        "            print(\"   !pip install --upgrade tensorflow keras scikit-learn\")\n",
        "            print(\"3. 빠른 테스트 실행:\")\n",
        "            print(\"   quick_test()\")\n",
        "\n",
        "        # 에러 추적 정보 출력 (디버깅용)\n",
        "        import traceback\n",
        "        print(f\"\\n 상세 오류 정보:\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n"
      ]
    }
  ]
}