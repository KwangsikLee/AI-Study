{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ba117289213145658465990e9a21a79f",
            "952b5d34082e4313a42fe7429e3b1a1e",
            "3776246062fd4788b26ed14b9055d5a6",
            "effb384b71f14bedb86516157fa6f801",
            "6df62d02490c42dcbe3c4533eb50e08a",
            "2f6b6ef128874d7b85a26b62fffb6008",
            "f91508bf5a654dadb9d45e86d2ae746a",
            "db0eb96250ce44609b06a9d95d1f7255",
            "b811a4325b584db8bd0cd2171e7c06d6",
            "2d91d9acc6764c0b8c08282b409db3b3",
            "dbbcde36c6cf4d5eb178a06b8bc92353"
          ]
        },
        "id": "T44j3Nt6WY5v",
        "outputId": "be6960f6-147e-461d-8537-027bc384d000"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Tavily Search (langchain-teddynote) 로드 완료\n",
            "✅ CLIP 라이브러리 로드 완료\n",
            "\n",
            "======================================================================\n",
            "🚀 GPT & LangChain & Tavily 연동 멀티모달 AI 시스템\n",
            "======================================================================\n",
            "\n",
            "✅ API 키 설정 완료:\n",
            "   • OpenAI API: sk-proj-ZFU7TcfmbMzg...\n",
            "   • LangSmith 프로젝트: langgraph_5\n",
            "   • Tavily API: tvly-dev-r3bStTqv6qi...\n",
            "🚀 GPT API 연동 AI 시스템 초기화 중...\n",
            "🔑 API 서비스 설정 중...\n",
            "✅ OpenAI API: 설정됨\n",
            "✅ LangSmith: 설정됨 (프로젝트: langgraph_6)\n",
            "✅ Tavily API: 설정됨\n",
            "🖥️ 사용 디바이스: cpu\n",
            "📦 이미지 분류 모델 로딩...\n",
            "✅ ResNet 분류 모델 로드 완료\n",
            "🔍 비전-언어 모델 로딩...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ CLIP 모델 로드 완료\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba117289213145658465990e9a21a79f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ BLIP 모델 로드 완료\n",
            "🧠 GPT 및 LangChain 초기화...\n",
            "✅ LangSmith 트레이싱 활성화 (프로젝트: langgraph_6)\n",
            "✅ GPT 및 LangChain 초기화 완료\n",
            "🔍 검색 도구 초기화...\n",
            "✅ Tavily Search 도구 로드 완료\n",
            "✅ AI 시스템 초기화 완료!\n",
            "\n",
            "🌐 웹 인터페이스를 시작합니다...\n",
            "💡 브라우저에서 자동으로 열립니다!\n",
            "🔗 수동 접속: http://localhost:17861\n",
            "\n",
            "======================================================================\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://46eb9af34a9987d580.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://46eb9af34a9987d580.gradio.live\" width=\"100%\" height=\"800\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# GPT API & LangChain 연동 멀티모달 AI 시스템 (API 키 직접 설정)\n",
        "# 이미지 업로드, 분석, 질문답변을 모두 웹에서 할 수 있는 통합 시스템\n",
        "\n",
        "# 필요한 라이브러리 설치\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_packages():\n",
        "    \"\"\"필요한 패키지들을 설치\"\"\"\n",
        "    packages = [\n",
        "        \"gradio\",\n",
        "        \"torch\",\n",
        "        \"torchvision\",\n",
        "        \"transformers\",\n",
        "        \"pillow\",\n",
        "        \"requests\",\n",
        "        \"matplotlib\",\n",
        "        \"openai\",\n",
        "        \"langchain\",\n",
        "        \"langchain-openai\",\n",
        "        \"langsmith\",\n",
        "        \"langchain-community\",\n",
        "        \"tavily-python\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(f\"Warning: Failed to install {package}\")\n",
        "\n",
        "# langchain-teddynote 설치 (TavilySearch용)\n",
        "def install_teddynote():\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"langchain-teddynote\"])\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(\"Warning: Failed to install langchain-teddynote\")\n",
        "\n",
        "# 패키지 설치 실행\n",
        "install_packages()\n",
        "install_teddynote()\n",
        "\n",
        "# CLIP 설치 (별도로)\n",
        "try:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"git+https://github.com/openai/CLIP.git\"])\n",
        "except:\n",
        "    print(\"Warning: CLIP installation failed, some features may be limited\")\n",
        "\n",
        "import os\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import requests\n",
        "import io\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "# =============================================================================\n",
        "# API 키 설정 (사용자가 직접 입력)\n",
        "# =============================================================================\n",
        "\n",
        "# OpenAI API 키 설정 (사용자가 입력해야 함)\n",
        "# from google.colab import userdata\n",
        "\n",
        "# api_key=userdata.get('api_key')\n",
        "# api_key2=userdata.get('api_key2')\n",
        "# api_key3=userdata.get('api_key3')\n",
        "# os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = api_key2\n",
        "# os.environ[\"TAVILY_API_KEY\"] = api_key3\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "# OpenAI API 클라이언트 생성\n",
        "OPENAPI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "LangSmith_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
        "\n",
        "# 2) LangSmith 연동 설정\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"langgraph_6\"\n",
        "\n",
        "# =============================================================================\n",
        "# 라이브러리 Import 및 모델 가용성 확인\n",
        "# =============================================================================\n",
        "\n",
        "# OpenAI & LangChain imports\n",
        "from openai import OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.callbacks import LangChainTracer\n",
        "from langsmith import Client\n",
        "\n",
        "# Tavily Search import\n",
        "try:\n",
        "    from langchain_teddynote.tools.tavily import TavilySearch\n",
        "    TAVILY_AVAILABLE = True\n",
        "    print(\"✅ Tavily Search (langchain-teddynote) 로드 완료\")\n",
        "except ImportError:\n",
        "    try:\n",
        "        from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "        TAVILY_AVAILABLE = True\n",
        "        print(\"✅ Tavily Search (community) 로드 완료\")\n",
        "    except ImportError:\n",
        "        TAVILY_AVAILABLE = False\n",
        "        print(\"❌ Tavily Search not available\")\n",
        "\n",
        "# CLIP import\n",
        "try:\n",
        "    import clip\n",
        "    CLIP_AVAILABLE = True\n",
        "    print(\"✅ CLIP 라이브러리 로드 완료\")\n",
        "except ImportError:\n",
        "    CLIP_AVAILABLE = False\n",
        "    print(\"❌ CLIP not available, using alternative methods\")\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class GPTMultimodalAI:\n",
        "    \"\"\"GPT API 연동 멀티모달 AI 시스템 (API 키 직접 설정)\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"🚀 GPT API 연동 AI 시스템 초기화 중...\")\n",
        "\n",
        "        # 모델 가용성 상태 초기화\n",
        "        self.clip_available = False\n",
        "        self.blip_available = False\n",
        "        self.search_available = False\n",
        "\n",
        "        # API 키 확인 및 설정\n",
        "        self.setup_api_services()\n",
        "\n",
        "        # 디바이스 설정\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"🖥️ 사용 디바이스: {self.device}\")\n",
        "\n",
        "        # 모델들 초기화\n",
        "        self.init_classification_model()\n",
        "        self.init_vision_language_models()\n",
        "        self.init_gpt_and_langchain()\n",
        "        self.init_search_tools()\n",
        "\n",
        "        self.current_image = None\n",
        "        self.analysis_cache = {}\n",
        "\n",
        "        print(\"✅ AI 시스템 초기화 완료!\")\n",
        "\n",
        "    def setup_api_services(self):\n",
        "        \"\"\"API 서비스 설정\"\"\"\n",
        "        print(\"🔑 API 서비스 설정 중...\")\n",
        "\n",
        "        # OpenAI 클라이언트 초기화\n",
        "        self.openai_api_key = OPENAI_API_KEY\n",
        "        self.openai_client = OpenAI(api_key=self.openai_api_key)\n",
        "\n",
        "        # LangSmith 설정 확인\n",
        "        self.langsmith_api_key = os.environ.get(\"LANGCHAIN_API_KEY\")\n",
        "        self.langsmith_project = os.environ.get(\"LANGSMITH_PROJECT\", \"langgraph_5\")\n",
        "\n",
        "        print(f\"✅ OpenAI API: 설정됨\")\n",
        "        print(f\"✅ LangSmith: 설정됨 (프로젝트: {self.langsmith_project})\")\n",
        "        print(f\"✅ Tavily API: 설정됨\")\n",
        "\n",
        "    def init_classification_model(self):\n",
        "        \"\"\"이미지 분류 모델 초기화\"\"\"\n",
        "        print(\"📦 이미지 분류 모델 로딩...\")\n",
        "        try:\n",
        "            self.classification_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "            self.classification_model.eval()\n",
        "\n",
        "            self.classification_transform = transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "\n",
        "            # ImageNet 클래스 로드\n",
        "            try:\n",
        "                url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
        "                response = requests.get(url, timeout=10)\n",
        "                self.imagenet_classes = response.text.strip().split('\\n')\n",
        "            except:\n",
        "                self.imagenet_classes = [f\"클래스_{i}\" for i in range(1000)]\n",
        "\n",
        "            print(\"✅ ResNet 분류 모델 로드 완료\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 분류 모델 로드 실패: {e}\")\n",
        "\n",
        "    def init_vision_language_models(self):\n",
        "        \"\"\"비전-언어 모델 초기화\"\"\"\n",
        "        print(\"🔍 비전-언어 모델 로딩...\")\n",
        "\n",
        "        # CLIP 모델 초기화\n",
        "        if CLIP_AVAILABLE:\n",
        "            try:\n",
        "                self.clip_model, self.clip_preprocess = clip.load(\"ViT-B/32\", device=self.device)\n",
        "                self.clip_available = True\n",
        "                print(\"✅ CLIP 모델 로드 완료\")\n",
        "            except Exception as e:\n",
        "                self.clip_available = False\n",
        "                print(f\"❌ CLIP 모델 로드 실패: {e}\")\n",
        "        else:\n",
        "            self.clip_available = False\n",
        "            print(\"ℹ️ CLIP 라이브러리를 사용할 수 없습니다\")\n",
        "\n",
        "        # BLIP 모델 초기화\n",
        "        try:\n",
        "            self.blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "            self.blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "            self.blip_available = True\n",
        "            print(\"✅ BLIP 모델 로드 완료\")\n",
        "        except Exception as e:\n",
        "            self.blip_available = False\n",
        "            print(f\"❌ BLIP 모델 로드 실패: {e}\")\n",
        "\n",
        "    def init_gpt_and_langchain(self):\n",
        "        \"\"\"GPT 및 LangChain 초기화\"\"\"\n",
        "        print(\"🧠 GPT 및 LangChain 초기화...\")\n",
        "\n",
        "        try:\n",
        "            # LangChain ChatOpenAI 설정\n",
        "            self.llm = ChatOpenAI(\n",
        "                openai_api_key=self.openai_api_key,\n",
        "                model_name=\"gpt-3.5-turbo\",\n",
        "                temperature=0.7,\n",
        "                max_tokens=1000\n",
        "            )\n",
        "\n",
        "            # 메모리 설정\n",
        "            self.memory = ConversationBufferMemory(\n",
        "                return_messages=True,\n",
        "                memory_key=\"chat_history\"\n",
        "            )\n",
        "\n",
        "            # LangSmith 트레이서 설정\n",
        "            try:\n",
        "                self.tracer = LangChainTracer(\n",
        "                    project_name=self.langsmith_project\n",
        "                )\n",
        "                self.callbacks = [self.tracer]\n",
        "                print(f\"✅ LangSmith 트레이싱 활성화 (프로젝트: {self.langsmith_project})\")\n",
        "            except:\n",
        "                self.callbacks = []\n",
        "                print(\"⚠️ LangSmith 트레이싱 설정 실패\")\n",
        "\n",
        "            print(\"✅ GPT 및 LangChain 초기화 완료\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ GPT/LangChain 초기화 실패: {e}\")\n",
        "            self.llm = None\n",
        "\n",
        "    def init_search_tools(self):\n",
        "        \"\"\"검색 도구 초기화\"\"\"\n",
        "        print(\"🔍 검색 도구 초기화...\")\n",
        "\n",
        "        if TAVILY_AVAILABLE:\n",
        "            try:\n",
        "                # TavilySearch 초기화 시도\n",
        "                if 'TavilySearch' in globals():\n",
        "                    self.tavily_search = TavilySearch()\n",
        "                else:\n",
        "                    # 대안으로 TavilySearchResults 사용\n",
        "                    self.tavily_search = TavilySearchResults(max_results=3)\n",
        "                self.search_available = True\n",
        "                print(\"✅ Tavily Search 도구 로드 완료\")\n",
        "            except Exception as e:\n",
        "                self.search_available = False\n",
        "                print(f\"⚠️ Tavily Search 초기화 실패: {e}\")\n",
        "        else:\n",
        "            self.search_available = False\n",
        "            print(\"❌ Tavily Search 사용 불가\")\n",
        "\n",
        "    def search_web(self, query):\n",
        "        \"\"\"웹 검색 수행\"\"\"\n",
        "        if not self.search_available:\n",
        "            return \"웹 검색 기능이 사용 불가능합니다.\"\n",
        "\n",
        "        try:\n",
        "            # TavilySearch 실행\n",
        "            if hasattr(self, 'tavily_search'):\n",
        "                if hasattr(self.tavily_search, 'invoke'):\n",
        "                    results = self.tavily_search.invoke(query)\n",
        "                else:\n",
        "                    results = self.tavily_search.run(query)\n",
        "\n",
        "                # 결과 정리\n",
        "                if isinstance(results, list):\n",
        "                    search_summary = \"\\n\".join([\n",
        "                        f\"• {result.get('title', 'No title')}: {result.get('content', result.get('snippet', 'No content'))[:200]}...\"\n",
        "                        for result in results[:3]\n",
        "                    ])\n",
        "                elif isinstance(results, str):\n",
        "                    search_summary = results[:500] + \"...\" if len(results) > 500 else results\n",
        "                else:\n",
        "                    search_summary = str(results)[:500]\n",
        "\n",
        "                return search_summary\n",
        "            else:\n",
        "                return \"검색 도구가 초기화되지 않았습니다.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"검색 중 오류 발생: {str(e)}\"\n",
        "\n",
        "    def classify_image(self, image):\n",
        "        \"\"\"이미지 분류 수행\"\"\"\n",
        "        if image is None:\n",
        "            return \"이미지를 업로드해주세요.\"\n",
        "\n",
        "        try:\n",
        "            # PIL Image로 변환\n",
        "            if isinstance(image, str):\n",
        "                image = Image.open(image).convert('RGB')\n",
        "            elif not isinstance(image, Image.Image):\n",
        "                image = Image.fromarray(image).convert('RGB')\n",
        "\n",
        "            # 전처리 및 예측\n",
        "            input_tensor = self.classification_transform(image).unsqueeze(0)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.classification_model(input_tensor)\n",
        "                probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
        "\n",
        "            # 상위 5개 결과\n",
        "            top_probs, top_indices = torch.topk(probabilities, 5)\n",
        "\n",
        "            results = \"🎯 **이미지 분류 결과**\\n\\n\"\n",
        "            for i in range(5):\n",
        "                class_idx = top_indices[i].item()\n",
        "                prob = top_probs[i].item()\n",
        "                class_name = self.imagenet_classes[class_idx]\n",
        "                results += f\"**{i+1}.** {class_name}\\n\"\n",
        "                results += f\"   📊 신뢰도: {prob*100:.2f}%\\n\\n\"\n",
        "\n",
        "            self.current_image = image\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ 분류 중 오류 발생: {str(e)}\"\n",
        "\n",
        "    def generate_description_with_gpt(self, image):\n",
        "        \"\"\"GPT API를 사용한 이미지 설명 생성\"\"\"\n",
        "        if image is None:\n",
        "            return \"이미지를 업로드해주세요.\"\n",
        "\n",
        "        try:\n",
        "            # PIL Image로 변환\n",
        "            if isinstance(image, str):\n",
        "                image = Image.open(image).convert('RGB')\n",
        "            elif not isinstance(image, Image.Image):\n",
        "                image = Image.fromarray(image).convert('RGB')\n",
        "\n",
        "            # BLIP으로 기본 캡션 생성\n",
        "            basic_caption = self.get_basic_caption(image)\n",
        "\n",
        "            # 이미지 분석 정보 수집\n",
        "            analysis_info = self.get_image_analysis(image)\n",
        "\n",
        "            # GPT로 향상된 설명 생성\n",
        "            enhanced_description = self.enhance_description_with_gpt(basic_caption, analysis_info)\n",
        "\n",
        "            result = \"📖 **GPT 향상 이미지 설명**\\n\\n\"\n",
        "            result += f\"🔍 **기본 분석:** {basic_caption}\\n\\n\"\n",
        "            result += f\"🧠 **GPT 향상 설명:**\\n{enhanced_description}\\n\\n\"\n",
        "            result += f\"📊 **분석 정보:** {analysis_info}\"\n",
        "\n",
        "            self.current_image = image\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ 설명 생성 중 오류 발생: {str(e)}\"\n",
        "\n",
        "    def get_basic_caption(self, image):\n",
        "        \"\"\"BLIP을 사용한 기본 캡션 생성\"\"\"\n",
        "        if not self.blip_available:\n",
        "            return \"이미지 분석 중 (BLIP 모델 사용 불가)\"\n",
        "\n",
        "        try:\n",
        "            inputs = self.blip_processor(image, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                out = self.blip_model.generate(**inputs, max_length=50)\n",
        "            return self.blip_processor.decode(out[0], skip_special_tokens=True)\n",
        "        except Exception as e:\n",
        "            return f\"이미지 분석 중 (오류: {str(e)[:50]})\"\n",
        "\n",
        "    def get_image_analysis(self, image):\n",
        "        \"\"\"이미지 추가 분석 정보\"\"\"\n",
        "        analysis = []\n",
        "\n",
        "        # 이미지 크기\n",
        "        width, height = image.size\n",
        "        analysis.append(f\"크기: {width}x{height}\")\n",
        "\n",
        "        # 색상 분석\n",
        "        try:\n",
        "            colors = image.getcolors(maxcolors=256*256*256)\n",
        "            if colors:\n",
        "                analysis.append(f\"색상 분석 완료\")\n",
        "        except:\n",
        "            analysis.append(\"색상 분석 실패\")\n",
        "\n",
        "        # CLIP 분석 (가능한 경우)\n",
        "        if self.clip_available and hasattr(self, 'clip_model'):\n",
        "            try:\n",
        "                categories = [\"사람\", \"동물\", \"자연\", \"건물\", \"음식\", \"차량\", \"스포츠\", \"예술\"]\n",
        "                category = self.analyze_with_clip(image, categories)\n",
        "                analysis.append(f\"카테고리: {category}\")\n",
        "            except:\n",
        "                analysis.append(\"CLIP 분석 실패\")\n",
        "\n",
        "        return \" | \".join(analysis)\n",
        "\n",
        "    def analyze_with_clip(self, image, categories):\n",
        "        \"\"\"CLIP을 사용한 카테고리 분석\"\"\"\n",
        "        if not self.clip_available:\n",
        "            return \"CLIP 사용 불가\"\n",
        "\n",
        "        try:\n",
        "            text_queries = [f\"a photo of {cat}\" for cat in categories]\n",
        "            text_tokens = clip.tokenize(text_queries).to(self.device)\n",
        "            image_tensor = self.clip_preprocess(image).unsqueeze(0).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                image_features = self.clip_model.encode_image(image_tensor)\n",
        "                text_features = self.clip_model.encode_text(text_tokens)\n",
        "                similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "            best_idx = similarity[0].argmax().item()\n",
        "            return categories[best_idx]\n",
        "        except Exception as e:\n",
        "            return f\"분석 오류: {str(e)[:20]}\"\n",
        "\n",
        "    def enhance_description_with_gpt(self, basic_caption, analysis_info):\n",
        "        \"\"\"GPT로 설명 향상\"\"\"\n",
        "        if not self.llm:\n",
        "            return \"GPT API가 설정되지 않았습니다.\"\n",
        "\n",
        "        try:\n",
        "            prompt = f\"\"\"\n",
        "            이미지 기본 분석: {basic_caption}\n",
        "            추가 정보: {analysis_info}\n",
        "\n",
        "            위 정보를 바탕으로 이미지에 대한 상세하고 흥미로운 한국어 설명을 작성해주세요.\n",
        "            다음 요소들을 포함해주세요:\n",
        "            1. 시각적 요소들의 구체적 설명\n",
        "            2. 분위기나 느낌\n",
        "            3. 주목할만한 특징들\n",
        "            4. 추정되는 상황이나 맥락\n",
        "\n",
        "            자연스러운 한국어로 2-3 문단으로 작성해주세요.\n",
        "            \"\"\"\n",
        "\n",
        "            messages = [\n",
        "                SystemMessage(content=\"당신은 이미지를 분석하고 창의적이고 상세한 설명을 제공하는 전문가입니다.\"),\n",
        "                HumanMessage(content=prompt)\n",
        "            ]\n",
        "\n",
        "            response = self.llm.invoke(messages, callbacks=self.callbacks)\n",
        "            return response.content\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"GPT 설명 생성 중 오류: {str(e)}\"\n",
        "\n",
        "    def answer_question_with_gpt_and_search(self, image, question):\n",
        "        \"\"\"GPT API와 웹 검색을 결합한 이미지 질문답변\"\"\"\n",
        "        if image is None:\n",
        "            return \"먼저 이미지를 업로드해주세요.\"\n",
        "\n",
        "        if not question or question.strip() == \"\":\n",
        "            return \"질문을 입력해주세요.\"\n",
        "\n",
        "        if not self.llm:\n",
        "            return \"GPT API가 설정되지 않았습니다.\"\n",
        "\n",
        "        try:\n",
        "            # PIL Image로 변환\n",
        "            if isinstance(image, str):\n",
        "                image = Image.open(image).convert('RGB')\n",
        "            elif not isinstance(image, Image.Image):\n",
        "                image = Image.fromarray(image).convert('RGB')\n",
        "\n",
        "            # 이미지 컨텍스트 생성\n",
        "            context = self.create_image_context_for_gpt(image)\n",
        "\n",
        "            # 필요시 웹 검색 수행\n",
        "            search_results = \"\"\n",
        "            if any(keyword in question.lower() for keyword in ['최신', '현재', '뉴스', '정보', '찾아', '검색']):\n",
        "                search_query = f\"{context['caption']} {question}\"\n",
        "                search_results = self.search_web(search_query)\n",
        "\n",
        "            # GPT로 질문답변\n",
        "            answer = self.generate_gpt_answer_with_search(context, question, search_results)\n",
        "\n",
        "            result = f\"❓ **질문:** {question}\\n\\n\"\n",
        "            result += f\"🤖 **GPT 답변:** {answer}\"\n",
        "\n",
        "            if search_results and search_results != \"웹 검색 기능이 사용 불가능합니다.\":\n",
        "                result += f\"\\n\\n🔍 **참고 검색 결과:**\\n{search_results[:300]}...\"\n",
        "\n",
        "            self.current_image = image\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ 답변 생성 중 오류 발생: {str(e)}\"\n",
        "\n",
        "    def create_image_context_for_gpt(self, image):\n",
        "        \"\"\"GPT용 이미지 컨텍스트 생성\"\"\"\n",
        "        context = {}\n",
        "\n",
        "        # 기본 캡션\n",
        "        context['caption'] = self.get_basic_caption(image)\n",
        "\n",
        "        # 분류 정보\n",
        "        try:\n",
        "            input_tensor = self.classification_transform(image).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.classification_model(input_tensor)\n",
        "                probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
        "            top_prob, top_idx = torch.topk(probabilities, 3)\n",
        "\n",
        "            top_classes = []\n",
        "            for i in range(3):\n",
        "                class_name = self.imagenet_classes[top_idx[i].item()]\n",
        "                prob = top_prob[i].item()\n",
        "                top_classes.append(f\"{class_name} ({prob*100:.1f}%)\")\n",
        "\n",
        "            context['classification'] = \", \".join(top_classes)\n",
        "        except:\n",
        "            context['classification'] = \"분류 정보 없음\"\n",
        "\n",
        "        # 추가 분석\n",
        "        context['analysis'] = self.get_image_analysis(image)\n",
        "\n",
        "        return context\n",
        "\n",
        "    def generate_gpt_answer_with_search(self, context, question, search_results=\"\"):\n",
        "        \"\"\"GPT로 검색 결과를 포함한 질문 답변 생성\"\"\"\n",
        "        try:\n",
        "            search_context = f\"\\n\\n웹 검색 결과:\\n{search_results}\" if search_results else \"\"\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "            이미지 분석 정보:\n",
        "            - 기본 설명: {context['caption']}\n",
        "            - 분류 결과: {context['classification']}\n",
        "            - 추가 분석: {context['analysis']}{search_context}\n",
        "\n",
        "            사용자 질문: {question}\n",
        "\n",
        "            위 이미지 분석 정보와 검색 결과를 바탕으로 사용자의 질문에 정확하고 도움이 되는 한국어 답변을 제공해주세요.\n",
        "            만약 이미지 정보만으로는 확실한 답변이 어려운 경우, 그렇다고 명시하고 가능한 추정이나 일반적인 정보를 제공해주세요.\n",
        "            검색 결과가 있다면 이를 적절히 활용해주세요.\n",
        "            \"\"\"\n",
        "\n",
        "            # 메모리에서 이전 대화 가져오기\n",
        "            chat_history = self.memory.chat_memory.messages if self.memory.chat_memory.messages else []\n",
        "\n",
        "            messages = [\n",
        "                SystemMessage(content=\"당신은 이미지를 분석하고 웹 검색 결과를 활용하여 사용자의 질문에 정확하게 답변하는 AI 어시스턴트입니다.\"),\n",
        "                *chat_history[-6:],  # 최근 3번의 대화만 유지\n",
        "                HumanMessage(content=prompt)\n",
        "            ]\n",
        "\n",
        "            # callbacks는 이미 초기화 시 설정되어 있으므로 제거\n",
        "            response = self.llm.invoke(messages)\n",
        "            answer = response.content\n",
        "\n",
        "            # 메모리에 대화 저장\n",
        "            self.memory.chat_memory.add_user_message(question)\n",
        "            self.memory.chat_memory.add_ai_message(answer)\n",
        "\n",
        "            return answer\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"GPT 답변 생성 중 오류: {str(e)}\"\n",
        "\n",
        "    def get_api_status(self):\n",
        "        \"\"\"API 상태 확인\"\"\"\n",
        "        status = \"🔧 **API 상태**\\n\\n\"\n",
        "\n",
        "        # OpenAI API 상태\n",
        "        try:\n",
        "            test_response = self.openai_client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": \"test\"}],\n",
        "                max_tokens=5\n",
        "            )\n",
        "            status += \"✅ OpenAI API: 연결됨\\n\"\n",
        "        except:\n",
        "            status += \"❌ OpenAI API: 연결 실패\\n\"\n",
        "\n",
        "        # LangSmith 상태\n",
        "        status += f\"✅ LangSmith: 활성화됨 (프로젝트: {self.langsmith_project})\\n\"\n",
        "\n",
        "        # Tavily Search 상태\n",
        "        status += f\"{'✅' if self.search_available else '❌'} Tavily Search: {'활성화됨' if self.search_available else '비활성화됨'}\\n\"\n",
        "\n",
        "        # 모델 상태\n",
        "        status += f\"✅ 이미지 분류: 활성화됨\\n\"\n",
        "        status += f\"{'✅' if self.blip_available else '❌'} BLIP 캡션: {'활성화됨' if self.blip_available else '비활성화됨'}\\n\"\n",
        "        status += f\"{'✅' if self.clip_available else '❌'} CLIP: {'활성화됨' if self.clip_available else '비활성화됨'}\\n\"\n",
        "\n",
        "        return status\n",
        "\n",
        "def create_gradio_interface():\n",
        "    \"\"\"GPT 연동 Gradio 웹 인터페이스 생성\"\"\"\n",
        "\n",
        "    # AI 시스템 초기화\n",
        "    ai_system = GPTMultimodalAI()\n",
        "\n",
        "    # CSS 스타일\n",
        "    css = \"\"\"\n",
        "    .gradio-container {\n",
        "        max-width: 1400px !important;\n",
        "    }\n",
        "    .image-container {\n",
        "        max-height: 400px;\n",
        "    }\n",
        "    .output-text {\n",
        "        font-family: 'Malgun Gothic', Arial, sans-serif;\n",
        "        line-height: 1.6;\n",
        "    }\n",
        "    .api-status {\n",
        "        background-color: #f0f0f0;\n",
        "        padding: 10px;\n",
        "        border-radius: 5px;\n",
        "        margin: 10px 0;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    with gr.Blocks(css=css, title=\"🤖 GPT & LangChain & Tavily 연동 AI\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(f\"\"\"\n",
        "        # 🤖 GPT & LangChain & Tavily 연동 멀티모달 AI 시스템\n",
        "        ### OpenAI GPT, LangChain, LangSmith, Tavily Search가 연동된 최고급 이미지 분석 시스템\n",
        "\n",
        "        🚀 **주요 기능:**\n",
        "        - 📊 **이미지 분류**: ResNet 기반 정확한 객체 분류\n",
        "        - 🧠 **GPT 향상 설명**: BLIP + GPT API로 상세하고 창의적인 설명 생성\n",
        "        - 💬 **지능형 질문답변**: 이미지 + 웹 검색 결합 AI 대화\n",
        "        - 🔍 **Tavily 웹 검색**: 실시간 정보 검색 및 통합\n",
        "        - 📈 **LangSmith 추적**: 대화 품질 모니터링 (프로젝트: {ai_system.langsmith_project})\n",
        "\n",
        "        ⚠️ **보안 공지**: API 키가 코드에 직접 설정되어 있습니다. 운영 환경에서는 환경변수 사용을 권장합니다.\n",
        "        \"\"\")\n",
        "\n",
        "        # API 상태 표시\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                api_status = gr.Markdown(\n",
        "                    value=ai_system.get_api_status(),\n",
        "                    elem_classes=[\"api-status\"]\n",
        "                )\n",
        "                refresh_status_btn = gr.Button(\"🔄 상태 새로고침\", size=\"sm\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                # 이미지 업로드\n",
        "                image_input = gr.Image(\n",
        "                    label=\"📷 이미지 업로드\",\n",
        "                    type=\"pil\",\n",
        "                    height=400\n",
        "                )\n",
        "\n",
        "                # 질문 입력\n",
        "                question_input = gr.Textbox(\n",
        "                    label=\"❓ 질문 입력 (GPT + Tavily 검색 결합 답변)\",\n",
        "                    placeholder=\"예: 이 이미지의 최신 정보를 찾아서 알려주세요\",\n",
        "                    lines=3\n",
        "                )\n",
        "\n",
        "                # 검색 테스트\n",
        "                with gr.Accordion(\"🔍 Tavily 검색 테스트\", open=False):\n",
        "                    search_query_input = gr.Textbox(\n",
        "                        label=\"검색어 입력\",\n",
        "                        placeholder=\"검색하고 싶은 내용을 입력하세요\"\n",
        "                    )\n",
        "                    search_btn = gr.Button(\"🔍 웹 검색 실행\")\n",
        "                    search_output = gr.Textbox(\n",
        "                        label=\"검색 결과\",\n",
        "                        lines=5\n",
        "                    )\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                # 탭 구성\n",
        "                with gr.Tabs():\n",
        "                    with gr.Tab(\"📊 이미지 분류\"):\n",
        "                        classify_btn = gr.Button(\"🔍 분류 시작\", variant=\"primary\", size=\"lg\")\n",
        "                        classification_output = gr.Markdown(\n",
        "                            label=\"분류 결과\",\n",
        "                            elem_classes=[\"output-text\"]\n",
        "                        )\n",
        "\n",
        "                    with gr.Tab(\"🧠 GPT 향상 설명\"):\n",
        "                        describe_btn = gr.Button(\"✨ GPT 설명 생성\", variant=\"primary\", size=\"lg\")\n",
        "                        description_output = gr.Markdown(\n",
        "                            label=\"GPT 향상 설명\",\n",
        "                            elem_classes=[\"output-text\"]\n",
        "                        )\n",
        "\n",
        "                    with gr.Tab(\"💬 GPT + Tavily 질문답변\"):\n",
        "                        answer_btn = gr.Button(\"🤖 GPT + 검색 답변\", variant=\"primary\", size=\"lg\")\n",
        "                        qa_output = gr.Markdown(\n",
        "                            label=\"GPT + Tavily 답변\",\n",
        "                            elem_classes=[\"output-text\"]\n",
        "                        )\n",
        "\n",
        "                    with gr.Tab(\"📈 대화 히스토리\"):\n",
        "                        clear_memory_btn = gr.Button(\"🗑️ 대화 기록 초기화\", variant=\"secondary\")\n",
        "                        memory_output = gr.Markdown(\n",
        "                            label=\"대화 기록\",\n",
        "                            elem_classes=[\"output-text\"]\n",
        "                        )\n",
        "\n",
        "        # 샘플 이미지들\n",
        "        gr.Markdown(\"### 🖼️ 샘플 이미지로 테스트해보세요!\")\n",
        "\n",
        "        sample_images = [\n",
        "            \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/American_Eskimo_Dog.jpg/440px-American_Eskimo_Dog.jpg\",\n",
        "            \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Collage_of_Nine_Dogs.jpg/440px-Collage_of_Nine_Dogs.jpg\",\n",
        "            \"https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg/440px-Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg\"\n",
        "        ]\n",
        "\n",
        "        with gr.Row():\n",
        "            sample_btn1 = gr.Button(\"🐕 강아지 샘플\", size=\"sm\")\n",
        "            sample_btn2 = gr.Button(\"🐕‍🦺 여러 강아지\", size=\"sm\")\n",
        "            sample_btn3 = gr.Button(\"🐱 고양이 샘플\", size=\"sm\")\n",
        "\n",
        "        # 함수들 정의\n",
        "        def show_memory():\n",
        "            if ai_system.memory.chat_memory.messages:\n",
        "                history = \"📋 **최근 대화 기록:**\\n\\n\"\n",
        "                for i, msg in enumerate(ai_system.memory.chat_memory.messages[-10:]):  # 최근 10개만\n",
        "                    if hasattr(msg, 'content'):\n",
        "                        role = \"🧑 사용자\" if \"Human\" in str(type(msg)) else \"🤖 AI\"\n",
        "                        history += f\"**{role}:** {msg.content[:200]}{'...' if len(msg.content) > 200 else ''}\\n\\n\"\n",
        "                return history\n",
        "            else:\n",
        "                return \"아직 대화 기록이 없습니다.\"\n",
        "\n",
        "        def clear_memory():\n",
        "            ai_system.memory.clear()\n",
        "            return \"🗑️ 대화 기록이 초기화되었습니다.\"\n",
        "\n",
        "        # 이벤트 바인딩\n",
        "        classify_btn.click(\n",
        "            fn=ai_system.classify_image,\n",
        "            inputs=[image_input],\n",
        "            outputs=[classification_output]\n",
        "        )\n",
        "\n",
        "        describe_btn.click(\n",
        "            fn=ai_system.generate_description_with_gpt,\n",
        "            inputs=[image_input],\n",
        "            outputs=[description_output]\n",
        "        )\n",
        "\n",
        "        answer_btn.click(\n",
        "            fn=ai_system.answer_question_with_gpt_and_search,\n",
        "            inputs=[image_input, question_input],\n",
        "            outputs=[qa_output]\n",
        "        )\n",
        "\n",
        "        refresh_status_btn.click(\n",
        "            fn=ai_system.get_api_status,\n",
        "            inputs=[],\n",
        "            outputs=[api_status]\n",
        "        )\n",
        "\n",
        "        search_btn.click(\n",
        "            fn=ai_system.search_web,\n",
        "            inputs=[search_query_input],\n",
        "            outputs=[search_output]\n",
        "        )\n",
        "\n",
        "        clear_memory_btn.click(\n",
        "            fn=clear_memory,\n",
        "            inputs=[],\n",
        "            outputs=[memory_output]\n",
        "        )\n",
        "\n",
        "        # 대화 기록 표시\n",
        "        answer_btn.click(\n",
        "            fn=show_memory,\n",
        "            inputs=[],\n",
        "            outputs=[memory_output]\n",
        "        )\n",
        "\n",
        "        # 샘플 이미지 바인딩\n",
        "        sample_btn1.click(fn=lambda: sample_images[0], outputs=[image_input])\n",
        "        sample_btn2.click(fn=lambda: sample_images[1], outputs=[image_input])\n",
        "        sample_btn3.click(fn=lambda: sample_images[2], outputs=[image_input])\n",
        "\n",
        "        # 사용법 안내\n",
        "        gr.Markdown(f\"\"\"\n",
        "        ---\n",
        "        ### 📋 시스템 정보 및 사용법\n",
        "\n",
        "        #### 🔧 **현재 설정**\n",
        "        - **OpenAI API**: 직접 설정됨\n",
        "        - **LangSmith 프로젝트**: {ai_system.langsmith_project}\n",
        "        - **Tavily Search**: {'활성화됨' if ai_system.search_available else '비활성화됨'}\n",
        "        - **추적 모드**: LangSmith 트레이싱 활성화\n",
        "\n",
        "        #### 🎯 **고급 기능**\n",
        "        1. **이미지 분류**: ResNet 기반 1000개 클래스 분류\n",
        "        2. **GPT 향상 설명**: BLIP 기본 분석 + GPT의 창의적 해석\n",
        "        3. **GPT + Tavily 질문답변**: 이미지 + 실시간 웹 검색 결합\n",
        "        4. **Tavily 검색**: 독립적인 웹 검색 기능\n",
        "        5. **대화 히스토리**: 연속적 컨텍스트 유지\n",
        "\n",
        "        #### 💡 **고급 질문 예시**\n",
        "        - \"이 동물의 최신 연구 결과를 찾아서 알려주세요\"\n",
        "        - \"이 장소의 현재 관광 정보를 검색해주세요\"\n",
        "        - \"이 제품의 최신 리뷰나 뉴스를 찾아주세요\"\n",
        "        - \"이 기술의 최근 발전 동향을 분석해주세요\"\n",
        "\n",
        "        #### 🔍 **LangSmith 모니터링**\n",
        "        - 프로젝트: {ai_system.langsmith_project}\n",
        "        - 모든 대화와 검색이 추적됩니다\n",
        "        - 성능, 비용, 품질 메트릭 수집\n",
        "\n",
        "        #### ⚡ **성능 최적화**\n",
        "        - 이미지는 자동으로 최적 크기로 조정\n",
        "        - 대화 기록은 효율성을 위해 최근 6개만 유지\n",
        "        - 검색 결과는 관련성 높은 상위 3개만 사용\n",
        "\n",
        "        ⚠️ **보안 참고**: 현재 API 키가 코드에 하드코딩되어 있습니다.\n",
        "        운영 환경에서는 반드시 환경변수나 보안 키 관리 시스템을 사용하세요.\n",
        "        \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "# 실행 함수\n",
        "def main():\n",
        "    \"\"\"메인 실행 함수\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"🚀 GPT & LangChain & Tavily 연동 멀티모달 AI 시스템\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(f\"\\n✅ API 키 설정 완료:\")\n",
        "    print(f\"   • OpenAI API: {OPENAI_API_KEY[:20]}...\")\n",
        "    print(f\"   • LangSmith 프로젝트: langgraph_5\")\n",
        "    print(f\"   • Tavily API: {TAVILY_API_KEY[:20]}...\")\n",
        "\n",
        "    # 인터페이스 생성 및 실행\n",
        "    demo = create_gradio_interface()\n",
        "\n",
        "    print(\"\\n🌐 웹 인터페이스를 시작합니다...\")\n",
        "    print(\"💡 브라우저에서 자동으로 열립니다!\")\n",
        "    print(\"🔗 수동 접속: http://localhost:17861\")\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    # 공개 링크로 실행\n",
        "    demo.launch(\n",
        "        share=True,\n",
        "        inbrowser=True,\n",
        "        show_error=True,\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7860,\n",
        "        height=800,\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2d91d9acc6764c0b8c08282b409db3b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f6b6ef128874d7b85a26b62fffb6008": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3776246062fd4788b26ed14b9055d5a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db0eb96250ce44609b06a9d95d1f7255",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b811a4325b584db8bd0cd2171e7c06d6",
            "value": 1
          }
        },
        "6df62d02490c42dcbe3c4533eb50e08a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "952b5d34082e4313a42fe7429e3b1a1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f6b6ef128874d7b85a26b62fffb6008",
            "placeholder": "​",
            "style": "IPY_MODEL_f91508bf5a654dadb9d45e86d2ae746a",
            "value": "Fetching 1 files: 100%"
          }
        },
        "b811a4325b584db8bd0cd2171e7c06d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba117289213145658465990e9a21a79f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_952b5d34082e4313a42fe7429e3b1a1e",
              "IPY_MODEL_3776246062fd4788b26ed14b9055d5a6",
              "IPY_MODEL_effb384b71f14bedb86516157fa6f801"
            ],
            "layout": "IPY_MODEL_6df62d02490c42dcbe3c4533eb50e08a"
          }
        },
        "db0eb96250ce44609b06a9d95d1f7255": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbbcde36c6cf4d5eb178a06b8bc92353": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "effb384b71f14bedb86516157fa6f801": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d91d9acc6764c0b8c08282b409db3b3",
            "placeholder": "​",
            "style": "IPY_MODEL_dbbcde36c6cf4d5eb178a06b8bc92353",
            "value": " 1/1 [00:00&lt;00:00, 66.66it/s]"
          }
        },
        "f91508bf5a654dadb9d45e86d2ae746a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
