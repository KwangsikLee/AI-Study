{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI_bE6p59D5n",
        "outputId": "7fd3f9ae-fb1b-488d-8b36-2bbe2016dbfd"
      },
      "outputs": [],
      "source": [
        "# ì˜ˆì œ 3: Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤ ë©€í‹°ëª¨ë‹¬ AI ì‹œìŠ¤í…œ\n",
        "# ì´ë¯¸ì§€ ì—…ë¡œë“œ, ë¶„ì„, ì§ˆë¬¸ë‹µë³€ì„ ëª¨ë‘ ì›¹ì—ì„œ í•  ìˆ˜ ìˆëŠ” í†µí•© ì‹œìŠ¤í…œ\n",
        "\n",
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install gradio transformers torch torchvision pillow requests matplotlib clip-by-openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDvCIAopA3aV",
        "outputId": "aff1a8ce-c981-4e5b-f342-24331195aba8"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdPgVmMB9GWO",
        "outputId": "b08c33ab-0759-4b18-c8ad-9ae79423449e"
      },
      "outputs": [],
      "source": [
        "# ë‚˜ëˆ”ê³ ë”• í°íŠ¸ ì„¤ì¹˜ ë° ì„¤ì •\n",
        "!apt-get update -qq\n",
        "!apt-get install fonts-nanum -qq\n",
        "!fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from PIL import Image\n",
        "import requests\n",
        "import io\n",
        "import clip\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# í°íŠ¸ ì„¤ì •\n",
        "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path, size=10)\n",
        "plt.rcParams['font.family'] = 'NanumGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7f57aecd666847b7906105abc908cb15",
            "a916a6adec0043caba5fab4d22b3a534",
            "c66902cf3a124b5c80fec335b6e1a432",
            "878736fc51264c08a63022bfb695b907",
            "1857e16ae1e14f3eabaac7b585b9dae6",
            "d846aec4efae4bf3a2ffb11b0a19fdcb",
            "54fb760b6ae34ea4ba8f9cb50fb91f6a",
            "761abe9c8f1c4affa3f51a76f43ce234",
            "f62e445245e54ff1ae088efc47ed7a55",
            "509f471916f144c39835bf72710ced48",
            "3748f5eef43c4664b331d0ebcbff1ef7"
          ]
        },
        "id": "ePzSPsdV85f_",
        "outputId": "df1b5a98-ec07-47f7-93e6-b67e1f3fb96a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.44.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting clip-by-openai\n",
            "  Using cached clip_by_openai-1.1-py3-none-any.whl.metadata (369 bytes)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.12.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.12.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.34.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.12.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.47.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.17.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from clip-by-openai) (6.3.1)\n",
            "INFO: pip is looking at multiple versions of clip-by-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached clip_by_openai-1.0.1-py3-none-any.whl.metadata (407 bytes)\n",
            "  Using cached clip_by_openai-0.1.1.5-py3-none-any.whl.metadata (8.6 kB)\n",
            "  Using cached clip_by_openai-0.1.1.4-py3-none-any.whl.metadata (8.6 kB)\n",
            "  Using cached clip_by_openai-0.1.1.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "  Using cached clip_by_openai-0.1.1.2-py3-none-any.whl.metadata (9.0 kB)\n",
            "  Using cached clip_by_openai-0.1.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "  Using cached clip_by_openai-0.1.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "INFO: pip is still looking at multiple versions of clip-by-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
            "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting triton==3.3.1 (from torch)\n",
            "  Using cached triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting triton==3.3.0 (from torch)\n",
            "  Using cached triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.10.2.21->torch)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.7.1.2->torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
            "  Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cufft-cu12==11.3.0.4->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.2.0 (from torch)\n",
            "  Using cached triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting sympy==1.13.1 (from torch)\n",
            "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting triton==3.1.0 (from torch)\n",
            "  Using cached triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.20.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.5.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.19.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.10.2.21->torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.7.1.2->torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch)\n",
            "  Using cached triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.19.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.18.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.3.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.18.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.17.2-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.17.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.2.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.17.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.2.0-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting triton==2.2.0 (from torch)\n",
            "  Using cached triton-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "\u001b[31mERROR: Cannot install clip-by-openai==0.1.0, clip-by-openai==0.1.1, clip-by-openai==0.1.1.2, clip-by-openai==0.1.1.3, clip-by-openai==0.1.1.4, clip-by-openai==0.1.1.5, clip-by-openai==1.0.1, clip-by-openai==1.1, torch and torchvision==0.17.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested torch\n",
            "    torchvision 0.17.0 depends on torch==2.2.0\n",
            "    clip-by-openai 1.1 depends on torch<1.7.2 and >=1.7.1\n",
            "    The user requested torch\n",
            "    torchvision 0.17.0 depends on torch==2.2.0\n",
            "    clip-by-openai 1.0.1 depends on torch<1.7.2 and >=1.7.1\n",
            "    The user requested torch\n",
            "    torchvision 0.17.0 depends on torch==2.2.0\n",
            "    clip-by-openai 0.1.1.5 depends on torch==1.7.1\n",
            "    The user requested torch\n",
            "    torchvision 0.17.0 depends on torch==2.2.0\n",
            "    clip-by-openai 0.1.1.4 depends on torch==1.7.1\n",
            "    The user requested torch\n",
            "    torchvision 0.17.0 depends on torch==2.2.0\n",
            "    clip-by-openai 0.1.1.3 depends on torch==1.7.1\n",
            "    The user requested torch\n",
            "    torchvision 0.17.0 depends on torch==2.2.0\n",
            "    clip-by-openai 0.1.1.2 depends on torch==1.7.1\n",
            "    The user requested torch\n",
            "    torchvision 0.17.0 depends on torch==2.2.0\n",
            "    clip-by-openai 0.1.1 depends on torch==1.7.1\n",
            "    The user requested torch\n",
            "    torchvision 0.17.0 depends on torch==2.2.0\n",
            "    clip-by-openai 0.1.0 depends on torch==1.7.1\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0mğŸŒŸ Gradio ë©€í‹°ëª¨ë‹¬ AI ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...\n",
            "ğŸš€ AI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...\n",
            "ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: cpu\n",
            "ğŸ“¦ ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ ë¡œë”©...\n",
            "ğŸ” ë¹„ì „-ì–¸ì–´ ëª¨ë¸ ë¡œë”©...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f57aecd666847b7906105abc908cb15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ ë¡œë”©...\n",
            "âœ… AI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!\n",
            "\n",
            "============================================================\n",
            "ğŸ‰ Gradio ë©€í‹°ëª¨ë‹¬ AI ì›¹ ì‹œìŠ¤í…œ\n",
            "============================================================\n",
            "\n",
            "ğŸŒ ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\n",
            "ğŸ’¡ ë¸Œë¼ìš°ì €ì—ì„œ ìë™ìœ¼ë¡œ ì—´ë¦½ë‹ˆë‹¤!\n",
            "ğŸ”— ìˆ˜ë™ ì ‘ì†: http://localhost:7860\n",
            "\n",
            "============================================================\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a4c5da48681a1377d6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://a4c5da48681a1377d6.gradio.live\" width=\"100%\" height=\"800\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¯ Gradio ë©€í‹°ëª¨ë‹¬ AI ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\n",
            "ğŸ’» Colabì—ì„œ ì‹¤í–‰ í›„ ìƒì„±ë˜ëŠ” ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”!\n",
            "\n",
            "==================================================\n",
            "â— ì„¤ì¹˜ ì˜¤ë¥˜ í•´ê²° ê°€ì´ë“œ\n",
            "==================================================\n",
            "\n",
            "ğŸ”§ ì˜ì¡´ì„± ì¶©ëŒì´ ë°œìƒí•œ ê²½ìš°:\n",
            "\n",
            "1ï¸âƒ£ **ëŸ°íƒ€ì„ ì¬ì‹œì‘**\n",
            "   - Runtime > Restart runtime í´ë¦­\n",
            "   - ìœ„ ì½”ë“œë¥¼ ë‹¤ì‹œ ì‹¤í–‰\n",
            "\n",
            "2ï¸âƒ£ **ê°œë³„ ì„¤ì¹˜ ë°©ë²•**\n",
            "   !pip install --upgrade pip\n",
            "   !pip install pillow requests matplotlib\n",
            "   !pip install torch torchvision\n",
            "   !pip install transformers\n",
            "   !pip install gradio\n",
            "\n",
            "3ï¸âƒ£ **CLIP ì„¤ì¹˜ ì‹¤íŒ¨ ì‹œ**\n",
            "   - CLIP ì—†ì´ë„ ê¸°ë³¸ ê¸°ëŠ¥ì€ ì‘ë™í•©ë‹ˆë‹¤\n",
            "   - ìº¡ì…˜ ìƒì„±ê³¼ ë¶„ë¥˜ëŠ” BLIPê³¼ ResNetìœ¼ë¡œ ì²˜ë¦¬\n",
            "\n",
            "4ï¸âƒ£ **ì™„ì „ ì´ˆê¸°í™”**\n",
            "   - Runtime > Factory reset runtime\n",
            "   - ìƒˆë¡œìš´ í™˜ê²½ì—ì„œ ë‹¤ì‹œ ì‹œì‘\n",
            "\n",
            "âœ… ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™í•˜ë©´ ìœ„ ë©”ì‹œì§€ë“¤ì„ ë¬´ì‹œí•˜ì„¸ìš”!\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# ì˜ˆì œ 3: Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤ ë©€í‹°ëª¨ë‹¬ AI ì‹œìŠ¤í…œ\n",
        "# ì´ë¯¸ì§€ ì—…ë¡œë“œ, ë¶„ì„, ì§ˆë¬¸ë‹µë³€ì„ ëª¨ë‘ ì›¹ì—ì„œ í•  ìˆ˜ ìˆëŠ” í†µí•© ì‹œìŠ¤í…œ\n",
        "\n",
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install gradio transformers torch torchvision pillow requests matplotlib clip-by-openai\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from PIL import Image\n",
        "import requests\n",
        "import io\n",
        "import clip\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class MultimodalAI:\n",
        "    \"\"\"í†µí•© ë©€í‹°ëª¨ë‹¬ AI ì‹œìŠ¤í…œ\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"ğŸš€ AI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...\")\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}\")\n",
        "\n",
        "        # ëª¨ë¸ë“¤ ì´ˆê¸°í™”\n",
        "        self.init_classification_model()\n",
        "        self.init_vision_language_models()\n",
        "        self.init_text_generation_model()\n",
        "\n",
        "        self.current_image = None\n",
        "        self.analysis_cache = {}\n",
        "\n",
        "        print(\"âœ… AI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
        "\n",
        "    def init_classification_model(self):\n",
        "        \"\"\"ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ ì´ˆê¸°í™”\"\"\"\n",
        "        print(\"ğŸ“¦ ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ ë¡œë”©...\")\n",
        "        self.classification_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "        self.classification_model.eval()\n",
        "\n",
        "        self.classification_transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        # ImageNet í´ë˜ìŠ¤ ë¡œë“œ\n",
        "        try:\n",
        "            url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
        "            response = requests.get(url, timeout=10)\n",
        "            self.imagenet_classes = response.text.strip().split('\\n')\n",
        "        except:\n",
        "            self.imagenet_classes = [f\"í´ë˜ìŠ¤_{i}\" for i in range(1000)]\n",
        "\n",
        "    def init_vision_language_models(self):\n",
        "        \"\"\"ë¹„ì „-ì–¸ì–´ ëª¨ë¸ ì´ˆê¸°í™”\"\"\"\n",
        "        print(\"ğŸ” ë¹„ì „-ì–¸ì–´ ëª¨ë¸ ë¡œë”©...\")\n",
        "        # CLIP ëª¨ë¸\n",
        "        self.clip_model, self.clip_preprocess = clip.load(\"ViT-B/32\", device=self.device)\n",
        "\n",
        "        # BLIP ëª¨ë¸ (ìº¡ì…˜ ìƒì„±)\n",
        "        self.blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        self.blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "    def init_text_generation_model(self):\n",
        "        \"\"\"í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ ì´ˆê¸°í™”\"\"\"\n",
        "        print(\"ğŸ“ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ ë¡œë”©...\")\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        self.text_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def classify_image(self, image):\n",
        "        \"\"\"ì´ë¯¸ì§€ ë¶„ë¥˜ ìˆ˜í–‰\"\"\"\n",
        "        if image is None:\n",
        "            return \"ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\"\n",
        "\n",
        "        try:\n",
        "            # PIL Imageë¡œ ë³€í™˜\n",
        "            if isinstance(image, str):\n",
        "                image = Image.open(image).convert('RGB')\n",
        "            elif not isinstance(image, Image.Image):\n",
        "                image = Image.fromarray(image).convert('RGB')\n",
        "\n",
        "            # ì „ì²˜ë¦¬ ë° ì˜ˆì¸¡\n",
        "            input_tensor = self.classification_transform(image).unsqueeze(0)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.classification_model(input_tensor)\n",
        "                probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
        "\n",
        "            # ìƒìœ„ 5ê°œ ê²°ê³¼\n",
        "            top_probs, top_indices = torch.topk(probabilities, 5)\n",
        "\n",
        "            results = \"ğŸ¯ **ì´ë¯¸ì§€ ë¶„ë¥˜ ê²°ê³¼**\\n\\n\"\n",
        "            for i in range(5):\n",
        "                class_idx = top_indices[i].item()\n",
        "                prob = top_probs[i].item()\n",
        "                class_name = self.imagenet_classes[class_idx]\n",
        "                results += f\"**{i+1}.** {class_name}\\n\"\n",
        "                results += f\"   ğŸ“Š ì‹ ë¢°ë„: {prob*100:.2f}%\\n\\n\"\n",
        "\n",
        "            self.current_image = image\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"âŒ ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\"\n",
        "\n",
        "    def generate_description(self, image):\n",
        "        \"\"\"ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±\"\"\"\n",
        "        if image is None:\n",
        "            return \"ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\"\n",
        "\n",
        "        try:\n",
        "            # PIL Imageë¡œ ë³€í™˜\n",
        "            if isinstance(image, str):\n",
        "                image = Image.open(image).convert('RGB')\n",
        "            elif not isinstance(image, Image.Image):\n",
        "                image = Image.fromarray(image).convert('RGB')\n",
        "\n",
        "            # BLIPìœ¼ë¡œ ì˜ì–´ ìº¡ì…˜ ìƒì„±\n",
        "            inputs = self.blip_processor(image, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                out = self.blip_model.generate(**inputs, max_length=50)\n",
        "\n",
        "            english_caption = self.blip_processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "            # í•œê¸€ ì„¤ëª… ìƒì„±\n",
        "            korean_description = self.translate_to_korean(english_caption)\n",
        "\n",
        "            # GPT-2ë¡œ ë” ìì„¸í•œ ì„¤ëª… ìƒì„±\n",
        "            detailed_description = self.generate_detailed_description(english_caption)\n",
        "\n",
        "            result = \"ğŸ“– **ì´ë¯¸ì§€ ì„¤ëª…**\\n\\n\"\n",
        "            result += f\"**ğŸ” ê¸°ë³¸ ì„¤ëª…:** {korean_description}\\n\\n\"\n",
        "            result += f\"**ğŸ“ ìƒì„¸ ì„¤ëª…:**\\n{detailed_description}\\n\\n\"\n",
        "            result += f\"**ğŸŒ ì›ë³¸ ì˜ì–´:** {english_caption}\"\n",
        "\n",
        "            self.current_image = image\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"âŒ ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\"\n",
        "\n",
        "    def translate_to_korean(self, english_text):\n",
        "        \"\"\"ê°„ë‹¨í•œ ì˜ì–´-í•œê¸€ ë²ˆì—­\"\"\"\n",
        "        translation_dict = {\n",
        "            'dog': 'ê°•ì•„ì§€', 'cat': 'ê³ ì–‘ì´', 'person': 'ì‚¬ëŒ', 'man': 'ë‚¨ì', 'woman': 'ì—¬ì',\n",
        "            'people': 'ì‚¬ëŒë“¤', 'child': 'ì•„ì´', 'baby': 'ì•„ê¸°', 'car': 'ìë™ì°¨', 'truck': 'íŠ¸ëŸ­',\n",
        "            'bus': 'ë²„ìŠ¤', 'motorcycle': 'ì˜¤í† ë°”ì´', 'bicycle': 'ìì „ê±°', 'tree': 'ë‚˜ë¬´',\n",
        "            'flower': 'ê½ƒ', 'grass': 'ì”ë””', 'building': 'ê±´ë¬¼', 'house': 'ì§‘', 'road': 'ë„ë¡œ',\n",
        "            'street': 'ê±°ë¦¬', 'food': 'ìŒì‹', 'plate': 'ì ‘ì‹œ', 'table': 'í…Œì´ë¸”', 'chair': 'ì˜ì',\n",
        "            'book': 'ì±…', 'phone': 'íœ´ëŒ€í°', 'computer': 'ì»´í“¨í„°', 'water': 'ë¬¼', 'sky': 'í•˜ëŠ˜',\n",
        "            'cloud': 'êµ¬ë¦„', 'sun': 'íƒœì–‘', 'beach': 'í•´ë³€', 'mountain': 'ì‚°', 'park': 'ê³µì›',\n",
        "            'sitting': 'ì•‰ì•„ìˆëŠ”', 'standing': 'ì„œìˆëŠ”', 'walking': 'ê±·ê³ ìˆëŠ”', 'running': 'ë›°ê³ ìˆëŠ”',\n",
        "            'playing': 'ë†€ê³ ìˆëŠ”', 'eating': 'ë¨¹ê³ ìˆëŠ”', 'drinking': 'ë§ˆì‹œê³ ìˆëŠ”', 'sleeping': 'ìê³ ìˆëŠ”',\n",
        "            'white': 'í°ìƒ‰', 'black': 'ê²€ì€ìƒ‰', 'red': 'ë¹¨ê°„ìƒ‰', 'blue': 'íŒŒë€ìƒ‰', 'green': 'ì´ˆë¡ìƒ‰',\n",
        "            'yellow': 'ë…¸ë€ìƒ‰', 'brown': 'ê°ˆìƒ‰', 'gray': 'íšŒìƒ‰', 'small': 'ì‘ì€', 'large': 'í°',\n",
        "            'big': 'í°', 'little': 'ì‘ì€', 'young': 'ì–´ë¦°', 'old': 'ë‚˜ì´ë“ ', 'beautiful': 'ì•„ë¦„ë‹¤ìš´'\n",
        "        }\n",
        "\n",
        "        korean_text = english_text.lower()\n",
        "        for en_word, ko_word in translation_dict.items():\n",
        "            korean_text = korean_text.replace(en_word, ko_word)\n",
        "\n",
        "        # ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ìœ¼ë¡œ ë³€í™˜\n",
        "        if any(word in english_text.lower() for word in ['dog', 'puppy']):\n",
        "            return \"ê°•ì•„ì§€ê°€ í¬í•¨ëœ ì¥ë©´\"\n",
        "        elif any(word in english_text.lower() for word in ['cat', 'kitten']):\n",
        "            return \"ê³ ì–‘ì´ê°€ í¬í•¨ëœ ì¥ë©´\"\n",
        "        elif any(word in english_text.lower() for word in ['person', 'people', 'man', 'woman']):\n",
        "            return \"ì‚¬ëŒì´ í¬í•¨ëœ ì¥ë©´\"\n",
        "        elif any(word in english_text.lower() for word in ['car', 'vehicle']):\n",
        "            return \"ì°¨ëŸ‰ì´ í¬í•¨ëœ ì¥ë©´\"\n",
        "        elif any(word in english_text.lower() for word in ['food', 'meal']):\n",
        "            return \"ìŒì‹ ê´€ë ¨ ì¥ë©´\"\n",
        "        else:\n",
        "            return f\"ë‹¤ì–‘í•œ ìš”ì†Œê°€ í¬í•¨ëœ ì¥ë©´ ({korean_text})\"\n",
        "\n",
        "    def generate_detailed_description(self, base_caption):\n",
        "        \"\"\"GPT-2ë¡œ ìƒì„¸ ì„¤ëª… ìƒì„±\"\"\"\n",
        "        try:\n",
        "            prompt = f\"ì´ ì´ë¯¸ì§€ëŠ” {base_caption}ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ë” ìì„¸íˆ ì„¤ëª…í•˜ë©´,\"\n",
        "\n",
        "            inputs = self.tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.text_model.generate(\n",
        "                    inputs,\n",
        "                    max_length=len(inputs[0]) + 80,\n",
        "                    num_return_sequences=1,\n",
        "                    temperature=0.7,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id,\n",
        "                    do_sample=True,\n",
        "                    top_k=50\n",
        "                )\n",
        "\n",
        "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            detailed_part = generated_text[len(prompt):].strip()\n",
        "\n",
        "            if len(detailed_part) < 20:\n",
        "                return \"ì´ë¯¸ì§€ì˜ ë‹¤ì–‘í•œ ìš”ì†Œë“¤ì´ ì¡°í™”ë¡­ê²Œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì‹œê°ì ìœ¼ë¡œ í¥ë¯¸ë¡œìš´ ì¥ë©´ì„ ì—°ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "            return detailed_part[:200] + \"...\" if len(detailed_part) > 200 else detailed_part\n",
        "\n",
        "        except:\n",
        "            return \"ì´ë¯¸ì§€ì˜ êµ¬ì„± ìš”ì†Œë“¤ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë°°ì¹˜ë˜ì–´ ìˆìœ¼ë©°, ì „ì²´ì ìœ¼ë¡œ ê· í˜•ì¡íŒ ì‹œê°ì  íš¨ê³¼ë¥¼ ë§Œë“¤ì–´ë‚´ê³  ìˆìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "    def answer_question(self, image, question):\n",
        "        \"\"\"ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸ ë‹µë³€\"\"\"\n",
        "        if image is None:\n",
        "            return \"ë¨¼ì € ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\"\n",
        "\n",
        "        if not question or question.strip() == \"\":\n",
        "            return \"ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\"\n",
        "\n",
        "        try:\n",
        "            # PIL Imageë¡œ ë³€í™˜\n",
        "            if isinstance(image, str):\n",
        "                image = Image.open(image).convert('RGB')\n",
        "            elif not isinstance(image, Image.Image):\n",
        "                image = Image.fromarray(image).convert('RGB')\n",
        "\n",
        "            # ì´ë¯¸ì§€ ì»¨í…ìŠ¤íŠ¸ ìƒì„±\n",
        "            context = self.create_image_context(image)\n",
        "\n",
        "            # ì§ˆë¬¸ ë¶„ì„ ë° ë‹µë³€ ìƒì„±\n",
        "            answer = self.generate_contextual_answer(context, question)\n",
        "\n",
        "            result = f\"â“ **ì§ˆë¬¸:** {question}\\n\\n\"\n",
        "            result += f\"ğŸ’¬ **ë‹µë³€:** {answer}\"\n",
        "\n",
        "            self.current_image = image\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\"\n",
        "\n",
        "    def create_image_context(self, image):\n",
        "        \"\"\"ì´ë¯¸ì§€ ì»¨í…ìŠ¤íŠ¸ ìƒì„±\"\"\"\n",
        "        # BLIP ìº¡ì…˜\n",
        "        inputs = self.blip_processor(image, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            out = self.blip_model.generate(**inputs, max_length=30)\n",
        "        caption = self.blip_processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "        # CLIPìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ì„\n",
        "        categories = [\n",
        "            \"ì‚¬ëŒ\", \"ë™ë¬¼\", \"ê°•ì•„ì§€\", \"ê³ ì–‘ì´\", \"ìë™ì°¨\", \"ìŒì‹\", \"ê±´ë¬¼\", \"ìì—°\",\n",
        "            \"ê½ƒ\", \"ë‚˜ë¬´\", \"ì‹¤ë‚´\", \"ì•¼ì™¸\", \"ì¸ë¬¼ì‚¬ì§„\", \"í’ê²½\", \"í´ë¡œì¦ˆì—…\"\n",
        "        ]\n",
        "\n",
        "        category_texts = [f\"a photo of {cat}\" if cat in [\"person\", \"animal\", \"dog\", \"cat\", \"car\", \"food\", \"building\", \"nature\"]\n",
        "                         else f\"{cat} scene\" for cat in categories]\n",
        "\n",
        "        text_tokens = clip.tokenize(category_texts).to(self.device)\n",
        "        image_tensor = self.clip_preprocess(image).unsqueeze(0).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image_features = self.clip_model.encode_image(image_tensor)\n",
        "            text_features = self.clip_model.encode_text(text_tokens)\n",
        "            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "        top_prob, top_idx = similarity[0].topk(1)\n",
        "        main_category = categories[top_idx[0].item()]\n",
        "\n",
        "        return {\n",
        "            'caption': caption,\n",
        "            'category': main_category,\n",
        "            'korean_caption': self.translate_to_korean(caption)\n",
        "        }\n",
        "\n",
        "    def generate_contextual_answer(self, context, question):\n",
        "        \"\"\"ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±\"\"\"\n",
        "        question_lower = question.lower()\n",
        "        caption = context['korean_caption']\n",
        "        category = context['category']\n",
        "\n",
        "        # ì§ˆë¬¸ ìœ í˜•ë³„ ë‹µë³€\n",
        "        if any(word in question_lower for word in ['ë¬´ì—‡', 'ë­', 'ë­”', 'ì–´ë–¤']):\n",
        "            return f\"ì´ ì´ë¯¸ì§€ì—ëŠ” {caption}ì´ ë³´ì…ë‹ˆë‹¤. ì£¼ë¡œ {category} ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤.\"\n",
        "\n",
        "        elif any(word in question_lower for word in ['ì–´ë””', 'ì¥ì†Œ', 'ìœ„ì¹˜']):\n",
        "            if category in ['ì‹¤ë‚´', 'ì•¼ì™¸']:\n",
        "                return f\"ì´ ì‚¬ì§„ì€ {category}ì—ì„œ ì´¬ì˜ëœ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\"\n",
        "            else:\n",
        "                return \"ì •í™•í•œ ì¥ì†ŒëŠ” ì•Œê¸° ì–´ë µì§€ë§Œ, ë‹¤ì–‘í•œ í™˜ê²½ì  ìš”ì†Œë“¤ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "        elif any(word in question_lower for word in ['ëª‡', 'ê°œìˆ˜', 'ìˆ˜ëŸ‰']):\n",
        "            if 'ë™ë¬¼' in category or 'ê°•ì•„ì§€' in caption or 'ê³ ì–‘ì´' in caption:\n",
        "                return \"ì´ë¯¸ì§€ì—ì„œ ì—¬ëŸ¬ ë§ˆë¦¬ì˜ ë™ë¬¼ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
        "            else:\n",
        "                return \"ì •í™•í•œ ê°œìˆ˜ëŠ” íŒŒì•…í•˜ê¸° ì–´ë µì§€ë§Œ, ì—¬ëŸ¬ ìš”ì†Œë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "        elif any(word in question_lower for word in ['ìƒ‰ê¹”', 'ìƒ‰ìƒ', 'ì»¬ëŸ¬']):\n",
        "            return \"ì´ë¯¸ì§€ì—ëŠ” ë‹¤ì–‘í•˜ê³  ì¡°í™”ë¡œìš´ ìƒ‰ìƒë“¤ì´ ì‚¬ìš©ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "        elif any(word in question_lower for word in ['í¬ê¸°', 'í¬ê¸°', 'ì‚¬ì´ì¦ˆ']):\n",
        "            return f\"ì´ {category}ëŠ” ì ì ˆí•œ í¬ê¸°ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "        elif any(word in question_lower for word in ['ì–¸ì œ', 'ì‹œê°„']):\n",
        "            return \"ì´ë¯¸ì§€ë§Œìœ¼ë¡œëŠ” ì •í™•í•œ ì‹œê°„ì„ ì•Œê¸° ì–´ë µìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "        else:\n",
        "            # GPT-2ë¡œ ì¼ë°˜ì ì¸ ë‹µë³€ ìƒì„±\n",
        "            try:\n",
        "                prompt = f\"ì´ë¯¸ì§€ ì„¤ëª…: {caption}. ì¹´í…Œê³ ë¦¬: {category}. ì§ˆë¬¸: {question} ë‹µë³€:\"\n",
        "                inputs = self.tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.text_model.generate(\n",
        "                        inputs,\n",
        "                        max_length=len(inputs[0]) + 60,\n",
        "                        temperature=0.7,\n",
        "                        pad_token_id=self.tokenizer.eos_token_id,\n",
        "                        do_sample=True\n",
        "                    )\n",
        "\n",
        "                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                answer = generated_text[len(prompt):].strip()\n",
        "\n",
        "                if len(answer) < 10:\n",
        "                    return f\"ì´ ì´ë¯¸ì§€ëŠ” {caption}ë¥¼ ë³´ì—¬ì£¼ë©°, {category} íŠ¹ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "                return answer[:150] + \"...\" if len(answer) > 150 else answer\n",
        "\n",
        "            except:\n",
        "                return f\"ì´ ì´ë¯¸ì§€ëŠ” {caption}ë¥¼ ë³´ì—¬ì£¼ë©°, {category}ì™€ ê´€ë ¨ëœ ë‚´ìš©ì…ë‹ˆë‹¤.\"\n",
        "\n",
        "# AI ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
        "print(\"ğŸŒŸ Gradio ë©€í‹°ëª¨ë‹¬ AI ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...\")\n",
        "ai_system = MultimodalAI()\n",
        "\n",
        "# Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì„±\n",
        "def create_gradio_interface():\n",
        "    \"\"\"Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤ ìƒì„±\"\"\"\n",
        "\n",
        "    # CSS ìŠ¤íƒ€ì¼\n",
        "    css = \"\"\"\n",
        "    .gradio-container {\n",
        "        max-width: 1200px !important;\n",
        "    }\n",
        "    .image-container {\n",
        "        max-height: 400px;\n",
        "    }\n",
        "    .output-text {\n",
        "        font-family: 'Malgun Gothic', Arial, sans-serif;\n",
        "        line-height: 1.6;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    with gr.Blocks(css=css, title=\"ğŸ¤– ë©€í‹°ëª¨ë‹¬ AI ì‹œìŠ¤í…œ\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # ğŸ¤– ë©€í‹°ëª¨ë‹¬ AI ì‹œìŠ¤í…œ\n",
        "        ### ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³  ë‹¤ì–‘í•œ AI ë¶„ì„ì„ ë°›ì•„ë³´ì„¸ìš”!\n",
        "\n",
        "        ğŸ¯ **ê¸°ëŠ¥ë“¤:**\n",
        "        - ğŸ“Š **ì´ë¯¸ì§€ ë¶„ë¥˜**: ì´ë¯¸ì§€ ë‚´ìš©ì„ ìë™ìœ¼ë¡œ ë¶„ë¥˜\n",
        "        - ğŸ“– **ì„¤ëª… ìƒì„±**: ì´ë¯¸ì§€ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª… ìƒì„±\n",
        "        - ğŸ’¬ **ì§ˆë¬¸ ë‹µë³€**: ì´ë¯¸ì§€ì— ëŒ€í•´ ììœ ë¡­ê²Œ ì§ˆë¬¸í•˜ê¸°\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                # ì´ë¯¸ì§€ ì—…ë¡œë“œ\n",
        "                image_input = gr.Image(\n",
        "                    label=\"ğŸ“· ì´ë¯¸ì§€ ì—…ë¡œë“œ\",\n",
        "                    type=\"pil\",\n",
        "                    height=400\n",
        "                )\n",
        "\n",
        "                # ì§ˆë¬¸ ì…ë ¥ (ì§ˆë¬¸ë‹µë³€ìš©)\n",
        "                question_input = gr.Textbox(\n",
        "                    label=\"â“ ì§ˆë¬¸ ì…ë ¥ (ì§ˆë¬¸ë‹µë³€ íƒ­ì—ì„œ ì‚¬ìš©)\",\n",
        "                    placeholder=\"ì˜ˆ: ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ìˆë‚˜ìš”?\",\n",
        "                    lines=2\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                # íƒ­ êµ¬ì„±\n",
        "                with gr.Tabs():\n",
        "                    with gr.Tab(\"ğŸ“Š ì´ë¯¸ì§€ ë¶„ë¥˜\"):\n",
        "                        classify_btn = gr.Button(\"ğŸ” ë¶„ë¥˜ ì‹œì‘\", variant=\"primary\", size=\"lg\")\n",
        "                        classification_output = gr.Markdown(\n",
        "                            label=\"ë¶„ë¥˜ ê²°ê³¼\",\n",
        "                            elem_classes=[\"output-text\"]\n",
        "                        )\n",
        "\n",
        "                    with gr.Tab(\"ğŸ“– ì„¤ëª… ìƒì„±\"):\n",
        "                        describe_btn = gr.Button(\"ğŸ“ ì„¤ëª… ìƒì„±\", variant=\"primary\", size=\"lg\")\n",
        "                        description_output = gr.Markdown(\n",
        "                            label=\"ì„¤ëª… ê²°ê³¼\",\n",
        "                            elem_classes=[\"output-text\"]\n",
        "                        )\n",
        "\n",
        "                    with gr.Tab(\"ğŸ’¬ ì§ˆë¬¸ ë‹µë³€\"):\n",
        "                        answer_btn = gr.Button(\"ğŸ’¡ ë‹µë³€ ë°›ê¸°\", variant=\"primary\", size=\"lg\")\n",
        "                        qa_output = gr.Markdown(\n",
        "                            label=\"ë‹µë³€ ê²°ê³¼\",\n",
        "                            elem_classes=[\"output-text\"]\n",
        "                        )\n",
        "\n",
        "        # ìƒ˜í”Œ ì´ë¯¸ì§€ë“¤\n",
        "        gr.Markdown(\"### ğŸ–¼ï¸ ìƒ˜í”Œ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”!\")\n",
        "\n",
        "        sample_images = [\n",
        "            \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/American_Eskimo_Dog.jpg/440px-American_Eskimo_Dog.jpg\",\n",
        "            \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Collage_of_Nine_Dogs.jpg/440px-Collage_of_Nine_Dogs.jpg\"\n",
        "        ]\n",
        "\n",
        "        with gr.Row():\n",
        "            # Use Buttons to load sample images\n",
        "            sample_btn1 = gr.Button(\"ìƒ˜í”Œ 1 ë¡œë“œ\", size=\"sm\")\n",
        "            sample_btn2 = gr.Button(\"ìƒ˜í”Œ 2 ë¡œë“œ\", size=\"sm\")\n",
        "\n",
        "        # Event binding\n",
        "        classify_btn.click(\n",
        "            fn=ai_system.classify_image,\n",
        "            inputs=[image_input],\n",
        "            outputs=[classification_output]\n",
        "        )\n",
        "\n",
        "        describe_btn.click(\n",
        "            fn=ai_system.generate_description,\n",
        "            inputs=[image_input],\n",
        "            outputs=[description_output]\n",
        "        )\n",
        "\n",
        "        answer_btn.click(\n",
        "            fn=ai_system.answer_question,\n",
        "            inputs=[image_input, question_input],\n",
        "            outputs=[qa_output]\n",
        "        )\n",
        "\n",
        "        # Bind sample buttons to update the image input\n",
        "        sample_btn1.click(\n",
        "            fn=lambda: sample_images[0],\n",
        "            inputs=[],\n",
        "            outputs=[image_input]\n",
        "        )\n",
        "\n",
        "        sample_btn2.click(\n",
        "            fn=lambda: sample_images[1],\n",
        "            inputs=[],\n",
        "            outputs=[image_input]\n",
        "        )\n",
        "\n",
        "\n",
        "        # ì‚¬ìš©ë²• ì•ˆë‚´\n",
        "        gr.Markdown(\"\"\"\n",
        "        ---\n",
        "        ### ğŸ“‹ ì‚¬ìš©ë²•\n",
        "        1. **ì´ë¯¸ì§€ ì—…ë¡œë“œ**: ìœ„ì˜ ì´ë¯¸ì§€ ì—…ë¡œë“œ ì˜ì—­ì— ì´ë¯¸ì§€ë¥¼ ë“œë˜ê·¸í•˜ê±°ë‚˜ í´ë¦­í•´ì„œ ì—…ë¡œë“œ\n",
        "        2. **ê¸°ëŠ¥ ì„ íƒ**: ì›í•˜ëŠ” íƒ­(ë¶„ë¥˜/ì„¤ëª…/ì§ˆë¬¸ë‹µë³€)ì„ ì„ íƒ\n",
        "        3. **ì§ˆë¬¸ë‹µë³€ì˜ ê²½ìš°**: ì§ˆë¬¸ ì…ë ¥ë€ì— ê¶ê¸ˆí•œ ë‚´ìš©ì„ í•œê¸€ë¡œ ì…ë ¥\n",
        "        4. **ì‹¤í–‰**: ê° íƒ­ì˜ ë²„íŠ¼ì„ í´ë¦­í•´ì„œ AI ë¶„ì„ ì‹œì‘!\n",
        "\n",
        "        ### ğŸ’¡ ì§ˆë¬¸ ì˜ˆì‹œ\n",
        "        - \"ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ìˆë‚˜ìš”?\"\n",
        "        - \"ëª‡ ë§ˆë¦¬ì˜ ë™ë¬¼ì´ ë³´ì´ë‚˜ìš”?\"\n",
        "        - \"ì£¼ìš” ìƒ‰ìƒì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
        "        - \"ì–´ë–¤ ì¥ì†Œì—ì„œ ì´¬ì˜ëœ ê²ƒ ê°™ë‚˜ìš”?\"\n",
        "\n",
        "        ### âš¡ íŒ\n",
        "        - ìƒ˜í”Œ ì´ë¯¸ì§€ ì•„ë˜ ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ ìë™ìœ¼ë¡œ ì—…ë¡œë“œë©ë‹ˆë‹¤\n",
        "        - ëª¨ë“  ê¸°ëŠ¥ì€ í•œê¸€ì„ ì§€ì›í•©ë‹ˆë‹¤\n",
        "        - ê³ í•´ìƒë„ ì´ë¯¸ì§€ë„ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤\n",
        "        \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Gradio ì•± ì‹¤í–‰\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸ‰ Gradio ë©€í‹°ëª¨ë‹¬ AI ì›¹ ì‹œìŠ¤í…œ\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì‹¤í–‰\n",
        "    demo = create_gradio_interface()\n",
        "\n",
        "    print(\"\\nğŸŒ ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
        "    print(\"ğŸ’¡ ë¸Œë¼ìš°ì €ì—ì„œ ìë™ìœ¼ë¡œ ì—´ë¦½ë‹ˆë‹¤!\")\n",
        "    print(\"ğŸ”— ìˆ˜ë™ ì ‘ì†: http://localhost:7860\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    # ê³µê°œ ë§í¬ë¡œ ì‹¤í–‰ (Colabì—ì„œ ì ‘ê·¼ ê°€ëŠ¥)\n",
        "    demo.launch(\n",
        "        share=False,          # ê³µê°œ ë§í¬ ìƒì„±\n",
        "        inbrowser=True,      # ìë™ìœ¼ë¡œ ë¸Œë¼ìš°ì € ì—´ê¸°\n",
        "        show_error=True,     # ì˜¤ë¥˜ í‘œì‹œ\n",
        "        server_name=\"0.0.0.0\",  # ì™¸ë¶€ ì ‘ê·¼ í—ˆìš©\n",
        "        server_port=7860,    # í¬íŠ¸ ì„¤ì •\n",
        "        height=800,          # ì¸í„°í˜ì´ìŠ¤ ë†’ì´\n",
        "    )\n",
        "\n",
        "print(\"ğŸ¯ Gradio ë©€í‹°ëª¨ë‹¬ AI ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "print(\"ğŸ’» Colabì—ì„œ ì‹¤í–‰ í›„ ìƒì„±ë˜ëŠ” ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”!\")\n",
        "\n",
        "# ì„¤ì¹˜ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"â— ì„¤ì¹˜ ì˜¤ë¥˜ í•´ê²° ê°€ì´ë“œ\")\n",
        "print(\"=\"*50)\n",
        "print(\"\"\"\n",
        "ğŸ”§ ì˜ì¡´ì„± ì¶©ëŒì´ ë°œìƒí•œ ê²½ìš°:\n",
        "\n",
        "1ï¸âƒ£ **ëŸ°íƒ€ì„ ì¬ì‹œì‘**\n",
        "   - Runtime > Restart runtime í´ë¦­\n",
        "   - ìœ„ ì½”ë“œë¥¼ ë‹¤ì‹œ ì‹¤í–‰\n",
        "\n",
        "2ï¸âƒ£ **ê°œë³„ ì„¤ì¹˜ ë°©ë²•**\n",
        "   !pip install --upgrade pip\n",
        "   !pip install pillow requests matplotlib\n",
        "   !pip install torch torchvision\n",
        "   !pip install transformers\n",
        "   !pip install gradio\n",
        "\n",
        "3ï¸âƒ£ **CLIP ì„¤ì¹˜ ì‹¤íŒ¨ ì‹œ**\n",
        "   - CLIP ì—†ì´ë„ ê¸°ë³¸ ê¸°ëŠ¥ì€ ì‘ë™í•©ë‹ˆë‹¤\n",
        "   - ìº¡ì…˜ ìƒì„±ê³¼ ë¶„ë¥˜ëŠ” BLIPê³¼ ResNetìœ¼ë¡œ ì²˜ë¦¬\n",
        "\n",
        "4ï¸âƒ£ **ì™„ì „ ì´ˆê¸°í™”**\n",
        "   - Runtime > Factory reset runtime\n",
        "   - ìƒˆë¡œìš´ í™˜ê²½ì—ì„œ ë‹¤ì‹œ ì‹œì‘\n",
        "\n",
        "âœ… ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™í•˜ë©´ ìœ„ ë©”ì‹œì§€ë“¤ì„ ë¬´ì‹œí•˜ì„¸ìš”!\n",
        "\"\"\")\n",
        "print(\"=\"*50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1857e16ae1e14f3eabaac7b585b9dae6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3748f5eef43c4664b331d0ebcbff1ef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "509f471916f144c39835bf72710ced48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54fb760b6ae34ea4ba8f9cb50fb91f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "761abe9c8f1c4affa3f51a76f43ce234": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f57aecd666847b7906105abc908cb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a916a6adec0043caba5fab4d22b3a534",
              "IPY_MODEL_c66902cf3a124b5c80fec335b6e1a432",
              "IPY_MODEL_878736fc51264c08a63022bfb695b907"
            ],
            "layout": "IPY_MODEL_1857e16ae1e14f3eabaac7b585b9dae6"
          }
        },
        "878736fc51264c08a63022bfb695b907": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_509f471916f144c39835bf72710ced48",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3748f5eef43c4664b331d0ebcbff1ef7",
            "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡63.82it/s]"
          }
        },
        "a916a6adec0043caba5fab4d22b3a534": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d846aec4efae4bf3a2ffb11b0a19fdcb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_54fb760b6ae34ea4ba8f9cb50fb91f6a",
            "value": "Fetchingâ€‡1â€‡files:â€‡100%"
          }
        },
        "c66902cf3a124b5c80fec335b6e1a432": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_761abe9c8f1c4affa3f51a76f43ce234",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f62e445245e54ff1ae088efc47ed7a55",
            "value": 1
          }
        },
        "d846aec4efae4bf3a2ffb11b0a19fdcb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f62e445245e54ff1ae088efc47ed7a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
