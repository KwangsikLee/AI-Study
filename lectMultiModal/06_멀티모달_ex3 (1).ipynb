{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI_bE6p59D5n",
        "outputId": "7fd3f9ae-fb1b-488d-8b36-2bbe2016dbfd"
      },
      "outputs": [],
      "source": [
        "# 예제 3: Gradio 웹 인터페이스 멀티모달 AI 시스템\n",
        "# 이미지 업로드, 분석, 질문답변을 모두 웹에서 할 수 있는 통합 시스템\n",
        "\n",
        "# 필요한 라이브러리 설치\n",
        "!pip install gradio transformers torch torchvision pillow requests matplotlib clip-by-openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDvCIAopA3aV",
        "outputId": "aff1a8ce-c981-4e5b-f342-24331195aba8"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdPgVmMB9GWO",
        "outputId": "b08c33ab-0759-4b18-c8ad-9ae79423449e"
      },
      "outputs": [],
      "source": [
        "# 나눔고딕 폰트 설치 및 설정\n",
        "!apt-get update -qq\n",
        "!apt-get install fonts-nanum -qq\n",
        "!fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from PIL import Image\n",
        "import requests\n",
        "import io\n",
        "import clip\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# 폰트 설정\n",
        "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path, size=10)\n",
        "plt.rcParams['font.family'] = 'NanumGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7f57aecd666847b7906105abc908cb15",
            "a916a6adec0043caba5fab4d22b3a534",
            "c66902cf3a124b5c80fec335b6e1a432",
            "878736fc51264c08a63022bfb695b907",
            "1857e16ae1e14f3eabaac7b585b9dae6",
            "d846aec4efae4bf3a2ffb11b0a19fdcb",
            "54fb760b6ae34ea4ba8f9cb50fb91f6a",
            "761abe9c8f1c4affa3f51a76f43ce234",
            "f62e445245e54ff1ae088efc47ed7a55",
            "509f471916f144c39835bf72710ced48",
            "3748f5eef43c4664b331d0ebcbff1ef7"
          ]
        },
        "id": "ePzSPsdV85f_",
        "outputId": "df1b5a98-ec07-47f7-93e6-b67e1f3fb96a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.44.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting clip-by-openai\n",
            "  Using cached clip_by_openai-1.1-py3-none-any.whl.metadata (369 bytes)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.12.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.12.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.34.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.12.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.47.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.17.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from clip-by-openai) (6.3.1)\n",
            "INFO: pip is looking at multiple versions of clip-by-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached clip_by_openai-1.0.1-py3-none-any.whl.metadata (407 bytes)\n",
            "  Using cached clip_by_openai-0.1.1.5-py3-none-any.whl.metadata (8.6 kB)\n",
            "  Using cached clip_by_openai-0.1.1.4-py3-none-any.whl.metadata (8.6 kB)\n",
            "  Using cached clip_by_openai-0.1.1.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "  Using cached clip_by_openai-0.1.1.2-py3-none-any.whl.metadata (9.0 kB)\n",
            "  Using cached clip_by_openai-0.1.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "  Using cached clip_by_openai-0.1.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "INFO: pip is still looking at multiple versions of clip-by-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
            "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting triton==3.3.1 (from torch)\n",
            "  Using cached triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting triton==3.3.0 (from torch)\n",
            "  Using cached triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.10.2.21->torch)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.7.1.2->torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
            "  Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cufft-cu12==11.3.0.4->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.2.0 (from torch)\n",
            "  Using cached triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting sympy==1.13.1 (from torch)\n",
            "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting triton==3.1.0 (from torch)\n",
            "  Using cached triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.20.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.5.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.19.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.10.2.21->torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.7.1.2->torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch)\n",
            "  Using cached triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.19.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.18.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.3.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.18.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.17.2-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.17.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.2.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.17.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.2.0-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting triton==2.2.0 (from torch)\n",
            "  Using cached triton-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "\u001b[31mERROR: Cannot install clip-by-openai==0.1.0, clip-by-openai==0.1.1, clip-by-openai==0.1.1.2, clip-by-openai==0.1.1.3, clip-by-openai==0.1.1.4, clip-by-openai==0.1.1.5, clip-by-openai==1.0.1, clip-by-openai==1.1, torch and torchvision==0.17.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested torch\n",
            "    torchvision 0.17.0 depends on torch==2.2.0\n",
            "    clip-by-openai 1.1 depends on torch<1.7.2 and >=1.7.1\n",
            "    The user requested torch\n",
            "    torchvision 0.17.0 depends on torch==2.2.0\n",
            "    clip-by-openai 1.0.1 depends on torch<1.7.2 and >=1.7.1\n",
            "    The user requested torch\n",
            "    torchvision 0.17.0 depends on torch==2.2.0\n",
            "    clip-by-openai 0.1.1.5 depends on torch==1.7.1\n",
            "    The user requested torch\n",
            "    torchvision 0.17.0 depends on torch==2.2.0\n",
            "    clip-by-openai 0.1.1.4 depends on torch==1.7.1\n",
            "    The user requested torch\n",
            "    torchvision 0.17.0 depends on torch==2.2.0\n",
            "    clip-by-openai 0.1.1.3 depends on torch==1.7.1\n",
            "    The user requested torch\n",
            "    torchvision 0.17.0 depends on torch==2.2.0\n",
            "    clip-by-openai 0.1.1.2 depends on torch==1.7.1\n",
            "    The user requested torch\n",
            "    torchvision 0.17.0 depends on torch==2.2.0\n",
            "    clip-by-openai 0.1.1 depends on torch==1.7.1\n",
            "    The user requested torch\n",
            "    torchvision 0.17.0 depends on torch==2.2.0\n",
            "    clip-by-openai 0.1.0 depends on torch==1.7.1\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m🌟 Gradio 멀티모달 AI 시스템을 초기화합니다...\n",
            "🚀 AI 시스템 초기화 중...\n",
            "🖥️ 사용 디바이스: cpu\n",
            "📦 이미지 분류 모델 로딩...\n",
            "🔍 비전-언어 모델 로딩...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f57aecd666847b7906105abc908cb15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📝 텍스트 생성 모델 로딩...\n",
            "✅ AI 시스템 초기화 완료!\n",
            "\n",
            "============================================================\n",
            "🎉 Gradio 멀티모달 AI 웹 시스템\n",
            "============================================================\n",
            "\n",
            "🌐 웹 인터페이스를 시작합니다...\n",
            "💡 브라우저에서 자동으로 열립니다!\n",
            "🔗 수동 접속: http://localhost:7860\n",
            "\n",
            "============================================================\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a4c5da48681a1377d6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://a4c5da48681a1377d6.gradio.live\" width=\"100%\" height=\"800\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 Gradio 멀티모달 AI 시스템 준비 완료!\n",
            "💻 Colab에서 실행 후 생성되는 링크를 클릭하세요!\n",
            "\n",
            "==================================================\n",
            "❗ 설치 오류 해결 가이드\n",
            "==================================================\n",
            "\n",
            "🔧 의존성 충돌이 발생한 경우:\n",
            "\n",
            "1️⃣ **런타임 재시작**\n",
            "   - Runtime > Restart runtime 클릭\n",
            "   - 위 코드를 다시 실행\n",
            "\n",
            "2️⃣ **개별 설치 방법**\n",
            "   !pip install --upgrade pip\n",
            "   !pip install pillow requests matplotlib\n",
            "   !pip install torch torchvision\n",
            "   !pip install transformers\n",
            "   !pip install gradio\n",
            "\n",
            "3️⃣ **CLIP 설치 실패 시**\n",
            "   - CLIP 없이도 기본 기능은 작동합니다\n",
            "   - 캡션 생성과 분류는 BLIP과 ResNet으로 처리\n",
            "\n",
            "4️⃣ **완전 초기화**\n",
            "   - Runtime > Factory reset runtime\n",
            "   - 새로운 환경에서 다시 시작\n",
            "\n",
            "✅ 시스템이 정상 작동하면 위 메시지들을 무시하세요!\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# 예제 3: Gradio 웹 인터페이스 멀티모달 AI 시스템\n",
        "# 이미지 업로드, 분석, 질문답변을 모두 웹에서 할 수 있는 통합 시스템\n",
        "\n",
        "# 필요한 라이브러리 설치\n",
        "!pip install gradio transformers torch torchvision pillow requests matplotlib clip-by-openai\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from PIL import Image\n",
        "import requests\n",
        "import io\n",
        "import clip\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class MultimodalAI:\n",
        "    \"\"\"통합 멀티모달 AI 시스템\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"🚀 AI 시스템 초기화 중...\")\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"🖥️ 사용 디바이스: {self.device}\")\n",
        "\n",
        "        # 모델들 초기화\n",
        "        self.init_classification_model()\n",
        "        self.init_vision_language_models()\n",
        "        self.init_text_generation_model()\n",
        "\n",
        "        self.current_image = None\n",
        "        self.analysis_cache = {}\n",
        "\n",
        "        print(\"✅ AI 시스템 초기화 완료!\")\n",
        "\n",
        "    def init_classification_model(self):\n",
        "        \"\"\"이미지 분류 모델 초기화\"\"\"\n",
        "        print(\"📦 이미지 분류 모델 로딩...\")\n",
        "        self.classification_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "        self.classification_model.eval()\n",
        "\n",
        "        self.classification_transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        # ImageNet 클래스 로드\n",
        "        try:\n",
        "            url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
        "            response = requests.get(url, timeout=10)\n",
        "            self.imagenet_classes = response.text.strip().split('\\n')\n",
        "        except:\n",
        "            self.imagenet_classes = [f\"클래스_{i}\" for i in range(1000)]\n",
        "\n",
        "    def init_vision_language_models(self):\n",
        "        \"\"\"비전-언어 모델 초기화\"\"\"\n",
        "        print(\"🔍 비전-언어 모델 로딩...\")\n",
        "        # CLIP 모델\n",
        "        self.clip_model, self.clip_preprocess = clip.load(\"ViT-B/32\", device=self.device)\n",
        "\n",
        "        # BLIP 모델 (캡션 생성)\n",
        "        self.blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        self.blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "    def init_text_generation_model(self):\n",
        "        \"\"\"텍스트 생성 모델 초기화\"\"\"\n",
        "        print(\"📝 텍스트 생성 모델 로딩...\")\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        self.text_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def classify_image(self, image):\n",
        "        \"\"\"이미지 분류 수행\"\"\"\n",
        "        if image is None:\n",
        "            return \"이미지를 업로드해주세요.\"\n",
        "\n",
        "        try:\n",
        "            # PIL Image로 변환\n",
        "            if isinstance(image, str):\n",
        "                image = Image.open(image).convert('RGB')\n",
        "            elif not isinstance(image, Image.Image):\n",
        "                image = Image.fromarray(image).convert('RGB')\n",
        "\n",
        "            # 전처리 및 예측\n",
        "            input_tensor = self.classification_transform(image).unsqueeze(0)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.classification_model(input_tensor)\n",
        "                probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
        "\n",
        "            # 상위 5개 결과\n",
        "            top_probs, top_indices = torch.topk(probabilities, 5)\n",
        "\n",
        "            results = \"🎯 **이미지 분류 결과**\\n\\n\"\n",
        "            for i in range(5):\n",
        "                class_idx = top_indices[i].item()\n",
        "                prob = top_probs[i].item()\n",
        "                class_name = self.imagenet_classes[class_idx]\n",
        "                results += f\"**{i+1}.** {class_name}\\n\"\n",
        "                results += f\"   📊 신뢰도: {prob*100:.2f}%\\n\\n\"\n",
        "\n",
        "            self.current_image = image\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ 분류 중 오류 발생: {str(e)}\"\n",
        "\n",
        "    def generate_description(self, image):\n",
        "        \"\"\"이미지 설명 생성\"\"\"\n",
        "        if image is None:\n",
        "            return \"이미지를 업로드해주세요.\"\n",
        "\n",
        "        try:\n",
        "            # PIL Image로 변환\n",
        "            if isinstance(image, str):\n",
        "                image = Image.open(image).convert('RGB')\n",
        "            elif not isinstance(image, Image.Image):\n",
        "                image = Image.fromarray(image).convert('RGB')\n",
        "\n",
        "            # BLIP으로 영어 캡션 생성\n",
        "            inputs = self.blip_processor(image, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                out = self.blip_model.generate(**inputs, max_length=50)\n",
        "\n",
        "            english_caption = self.blip_processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "            # 한글 설명 생성\n",
        "            korean_description = self.translate_to_korean(english_caption)\n",
        "\n",
        "            # GPT-2로 더 자세한 설명 생성\n",
        "            detailed_description = self.generate_detailed_description(english_caption)\n",
        "\n",
        "            result = \"📖 **이미지 설명**\\n\\n\"\n",
        "            result += f\"**🔍 기본 설명:** {korean_description}\\n\\n\"\n",
        "            result += f\"**📝 상세 설명:**\\n{detailed_description}\\n\\n\"\n",
        "            result += f\"**🌐 원본 영어:** {english_caption}\"\n",
        "\n",
        "            self.current_image = image\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ 설명 생성 중 오류 발생: {str(e)}\"\n",
        "\n",
        "    def translate_to_korean(self, english_text):\n",
        "        \"\"\"간단한 영어-한글 번역\"\"\"\n",
        "        translation_dict = {\n",
        "            'dog': '강아지', 'cat': '고양이', 'person': '사람', 'man': '남자', 'woman': '여자',\n",
        "            'people': '사람들', 'child': '아이', 'baby': '아기', 'car': '자동차', 'truck': '트럭',\n",
        "            'bus': '버스', 'motorcycle': '오토바이', 'bicycle': '자전거', 'tree': '나무',\n",
        "            'flower': '꽃', 'grass': '잔디', 'building': '건물', 'house': '집', 'road': '도로',\n",
        "            'street': '거리', 'food': '음식', 'plate': '접시', 'table': '테이블', 'chair': '의자',\n",
        "            'book': '책', 'phone': '휴대폰', 'computer': '컴퓨터', 'water': '물', 'sky': '하늘',\n",
        "            'cloud': '구름', 'sun': '태양', 'beach': '해변', 'mountain': '산', 'park': '공원',\n",
        "            'sitting': '앉아있는', 'standing': '서있는', 'walking': '걷고있는', 'running': '뛰고있는',\n",
        "            'playing': '놀고있는', 'eating': '먹고있는', 'drinking': '마시고있는', 'sleeping': '자고있는',\n",
        "            'white': '흰색', 'black': '검은색', 'red': '빨간색', 'blue': '파란색', 'green': '초록색',\n",
        "            'yellow': '노란색', 'brown': '갈색', 'gray': '회색', 'small': '작은', 'large': '큰',\n",
        "            'big': '큰', 'little': '작은', 'young': '어린', 'old': '나이든', 'beautiful': '아름다운'\n",
        "        }\n",
        "\n",
        "        korean_text = english_text.lower()\n",
        "        for en_word, ko_word in translation_dict.items():\n",
        "            korean_text = korean_text.replace(en_word, ko_word)\n",
        "\n",
        "        # 더 자연스러운 표현으로 변환\n",
        "        if any(word in english_text.lower() for word in ['dog', 'puppy']):\n",
        "            return \"강아지가 포함된 장면\"\n",
        "        elif any(word in english_text.lower() for word in ['cat', 'kitten']):\n",
        "            return \"고양이가 포함된 장면\"\n",
        "        elif any(word in english_text.lower() for word in ['person', 'people', 'man', 'woman']):\n",
        "            return \"사람이 포함된 장면\"\n",
        "        elif any(word in english_text.lower() for word in ['car', 'vehicle']):\n",
        "            return \"차량이 포함된 장면\"\n",
        "        elif any(word in english_text.lower() for word in ['food', 'meal']):\n",
        "            return \"음식 관련 장면\"\n",
        "        else:\n",
        "            return f\"다양한 요소가 포함된 장면 ({korean_text})\"\n",
        "\n",
        "    def generate_detailed_description(self, base_caption):\n",
        "        \"\"\"GPT-2로 상세 설명 생성\"\"\"\n",
        "        try:\n",
        "            prompt = f\"이 이미지는 {base_caption}를 보여줍니다. 더 자세히 설명하면,\"\n",
        "\n",
        "            inputs = self.tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.text_model.generate(\n",
        "                    inputs,\n",
        "                    max_length=len(inputs[0]) + 80,\n",
        "                    num_return_sequences=1,\n",
        "                    temperature=0.7,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id,\n",
        "                    do_sample=True,\n",
        "                    top_k=50\n",
        "                )\n",
        "\n",
        "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            detailed_part = generated_text[len(prompt):].strip()\n",
        "\n",
        "            if len(detailed_part) < 20:\n",
        "                return \"이미지의 다양한 요소들이 조화롭게 구성되어 있으며, 시각적으로 흥미로운 장면을 연출하고 있습니다.\"\n",
        "\n",
        "            return detailed_part[:200] + \"...\" if len(detailed_part) > 200 else detailed_part\n",
        "\n",
        "        except:\n",
        "            return \"이미지의 구성 요소들이 자연스럽게 배치되어 있으며, 전체적으로 균형잡힌 시각적 효과를 만들어내고 있습니다.\"\n",
        "\n",
        "    def answer_question(self, image, question):\n",
        "        \"\"\"이미지에 대한 질문 답변\"\"\"\n",
        "        if image is None:\n",
        "            return \"먼저 이미지를 업로드해주세요.\"\n",
        "\n",
        "        if not question or question.strip() == \"\":\n",
        "            return \"질문을 입력해주세요.\"\n",
        "\n",
        "        try:\n",
        "            # PIL Image로 변환\n",
        "            if isinstance(image, str):\n",
        "                image = Image.open(image).convert('RGB')\n",
        "            elif not isinstance(image, Image.Image):\n",
        "                image = Image.fromarray(image).convert('RGB')\n",
        "\n",
        "            # 이미지 컨텍스트 생성\n",
        "            context = self.create_image_context(image)\n",
        "\n",
        "            # 질문 분석 및 답변 생성\n",
        "            answer = self.generate_contextual_answer(context, question)\n",
        "\n",
        "            result = f\"❓ **질문:** {question}\\n\\n\"\n",
        "            result += f\"💬 **답변:** {answer}\"\n",
        "\n",
        "            self.current_image = image\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ 답변 생성 중 오류 발생: {str(e)}\"\n",
        "\n",
        "    def create_image_context(self, image):\n",
        "        \"\"\"이미지 컨텍스트 생성\"\"\"\n",
        "        # BLIP 캡션\n",
        "        inputs = self.blip_processor(image, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            out = self.blip_model.generate(**inputs, max_length=30)\n",
        "        caption = self.blip_processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "        # CLIP으로 카테고리 분석\n",
        "        categories = [\n",
        "            \"사람\", \"동물\", \"강아지\", \"고양이\", \"자동차\", \"음식\", \"건물\", \"자연\",\n",
        "            \"꽃\", \"나무\", \"실내\", \"야외\", \"인물사진\", \"풍경\", \"클로즈업\"\n",
        "        ]\n",
        "\n",
        "        category_texts = [f\"a photo of {cat}\" if cat in [\"person\", \"animal\", \"dog\", \"cat\", \"car\", \"food\", \"building\", \"nature\"]\n",
        "                         else f\"{cat} scene\" for cat in categories]\n",
        "\n",
        "        text_tokens = clip.tokenize(category_texts).to(self.device)\n",
        "        image_tensor = self.clip_preprocess(image).unsqueeze(0).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image_features = self.clip_model.encode_image(image_tensor)\n",
        "            text_features = self.clip_model.encode_text(text_tokens)\n",
        "            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "        top_prob, top_idx = similarity[0].topk(1)\n",
        "        main_category = categories[top_idx[0].item()]\n",
        "\n",
        "        return {\n",
        "            'caption': caption,\n",
        "            'category': main_category,\n",
        "            'korean_caption': self.translate_to_korean(caption)\n",
        "        }\n",
        "\n",
        "    def generate_contextual_answer(self, context, question):\n",
        "        \"\"\"컨텍스트 기반 답변 생성\"\"\"\n",
        "        question_lower = question.lower()\n",
        "        caption = context['korean_caption']\n",
        "        category = context['category']\n",
        "\n",
        "        # 질문 유형별 답변\n",
        "        if any(word in question_lower for word in ['무엇', '뭐', '뭔', '어떤']):\n",
        "            return f\"이 이미지에는 {caption}이 보입니다. 주로 {category} 관련 내용입니다.\"\n",
        "\n",
        "        elif any(word in question_lower for word in ['어디', '장소', '위치']):\n",
        "            if category in ['실내', '야외']:\n",
        "                return f\"이 사진은 {category}에서 촬영된 것으로 보입니다.\"\n",
        "            else:\n",
        "                return \"정확한 장소는 알기 어렵지만, 다양한 환경적 요소들을 볼 수 있습니다.\"\n",
        "\n",
        "        elif any(word in question_lower for word in ['몇', '개수', '수량']):\n",
        "            if '동물' in category or '강아지' in caption or '고양이' in caption:\n",
        "                return \"이미지에서 여러 마리의 동물을 확인할 수 있습니다.\"\n",
        "            else:\n",
        "                return \"정확한 개수는 파악하기 어렵지만, 여러 요소들이 포함되어 있습니다.\"\n",
        "\n",
        "        elif any(word in question_lower for word in ['색깔', '색상', '컬러']):\n",
        "            return \"이미지에는 다양하고 조화로운 색상들이 사용되어 있습니다.\"\n",
        "\n",
        "        elif any(word in question_lower for word in ['크기', '크기', '사이즈']):\n",
        "            return f\"이 {category}는 적절한 크기로 구성되어 있습니다.\"\n",
        "\n",
        "        elif any(word in question_lower for word in ['언제', '시간']):\n",
        "            return \"이미지만으로는 정확한 시간을 알기 어렵습니다.\"\n",
        "\n",
        "        else:\n",
        "            # GPT-2로 일반적인 답변 생성\n",
        "            try:\n",
        "                prompt = f\"이미지 설명: {caption}. 카테고리: {category}. 질문: {question} 답변:\"\n",
        "                inputs = self.tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.text_model.generate(\n",
        "                        inputs,\n",
        "                        max_length=len(inputs[0]) + 60,\n",
        "                        temperature=0.7,\n",
        "                        pad_token_id=self.tokenizer.eos_token_id,\n",
        "                        do_sample=True\n",
        "                    )\n",
        "\n",
        "                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                answer = generated_text[len(prompt):].strip()\n",
        "\n",
        "                if len(answer) < 10:\n",
        "                    return f\"이 이미지는 {caption}를 보여주며, {category} 특성을 가지고 있습니다.\"\n",
        "\n",
        "                return answer[:150] + \"...\" if len(answer) > 150 else answer\n",
        "\n",
        "            except:\n",
        "                return f\"이 이미지는 {caption}를 보여주며, {category}와 관련된 내용입니다.\"\n",
        "\n",
        "# AI 시스템 초기화\n",
        "print(\"🌟 Gradio 멀티모달 AI 시스템을 초기화합니다...\")\n",
        "ai_system = MultimodalAI()\n",
        "\n",
        "# Gradio 인터페이스 구성\n",
        "def create_gradio_interface():\n",
        "    \"\"\"Gradio 웹 인터페이스 생성\"\"\"\n",
        "\n",
        "    # CSS 스타일\n",
        "    css = \"\"\"\n",
        "    .gradio-container {\n",
        "        max-width: 1200px !important;\n",
        "    }\n",
        "    .image-container {\n",
        "        max-height: 400px;\n",
        "    }\n",
        "    .output-text {\n",
        "        font-family: 'Malgun Gothic', Arial, sans-serif;\n",
        "        line-height: 1.6;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    with gr.Blocks(css=css, title=\"🤖 멀티모달 AI 시스템\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # 🤖 멀티모달 AI 시스템\n",
        "        ### 이미지를 업로드하고 다양한 AI 분석을 받아보세요!\n",
        "\n",
        "        🎯 **기능들:**\n",
        "        - 📊 **이미지 분류**: 이미지 내용을 자동으로 분류\n",
        "        - 📖 **설명 생성**: 이미지에 대한 상세한 설명 생성\n",
        "        - 💬 **질문 답변**: 이미지에 대해 자유롭게 질문하기\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                # 이미지 업로드\n",
        "                image_input = gr.Image(\n",
        "                    label=\"📷 이미지 업로드\",\n",
        "                    type=\"pil\",\n",
        "                    height=400\n",
        "                )\n",
        "\n",
        "                # 질문 입력 (질문답변용)\n",
        "                question_input = gr.Textbox(\n",
        "                    label=\"❓ 질문 입력 (질문답변 탭에서 사용)\",\n",
        "                    placeholder=\"예: 이 이미지에 무엇이 있나요?\",\n",
        "                    lines=2\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                # 탭 구성\n",
        "                with gr.Tabs():\n",
        "                    with gr.Tab(\"📊 이미지 분류\"):\n",
        "                        classify_btn = gr.Button(\"🔍 분류 시작\", variant=\"primary\", size=\"lg\")\n",
        "                        classification_output = gr.Markdown(\n",
        "                            label=\"분류 결과\",\n",
        "                            elem_classes=[\"output-text\"]\n",
        "                        )\n",
        "\n",
        "                    with gr.Tab(\"📖 설명 생성\"):\n",
        "                        describe_btn = gr.Button(\"📝 설명 생성\", variant=\"primary\", size=\"lg\")\n",
        "                        description_output = gr.Markdown(\n",
        "                            label=\"설명 결과\",\n",
        "                            elem_classes=[\"output-text\"]\n",
        "                        )\n",
        "\n",
        "                    with gr.Tab(\"💬 질문 답변\"):\n",
        "                        answer_btn = gr.Button(\"💡 답변 받기\", variant=\"primary\", size=\"lg\")\n",
        "                        qa_output = gr.Markdown(\n",
        "                            label=\"답변 결과\",\n",
        "                            elem_classes=[\"output-text\"]\n",
        "                        )\n",
        "\n",
        "        # 샘플 이미지들\n",
        "        gr.Markdown(\"### 🖼️ 샘플 이미지로 테스트해보세요!\")\n",
        "\n",
        "        sample_images = [\n",
        "            \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/American_Eskimo_Dog.jpg/440px-American_Eskimo_Dog.jpg\",\n",
        "            \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Collage_of_Nine_Dogs.jpg/440px-Collage_of_Nine_Dogs.jpg\"\n",
        "        ]\n",
        "\n",
        "        with gr.Row():\n",
        "            # Use Buttons to load sample images\n",
        "            sample_btn1 = gr.Button(\"샘플 1 로드\", size=\"sm\")\n",
        "            sample_btn2 = gr.Button(\"샘플 2 로드\", size=\"sm\")\n",
        "\n",
        "        # Event binding\n",
        "        classify_btn.click(\n",
        "            fn=ai_system.classify_image,\n",
        "            inputs=[image_input],\n",
        "            outputs=[classification_output]\n",
        "        )\n",
        "\n",
        "        describe_btn.click(\n",
        "            fn=ai_system.generate_description,\n",
        "            inputs=[image_input],\n",
        "            outputs=[description_output]\n",
        "        )\n",
        "\n",
        "        answer_btn.click(\n",
        "            fn=ai_system.answer_question,\n",
        "            inputs=[image_input, question_input],\n",
        "            outputs=[qa_output]\n",
        "        )\n",
        "\n",
        "        # Bind sample buttons to update the image input\n",
        "        sample_btn1.click(\n",
        "            fn=lambda: sample_images[0],\n",
        "            inputs=[],\n",
        "            outputs=[image_input]\n",
        "        )\n",
        "\n",
        "        sample_btn2.click(\n",
        "            fn=lambda: sample_images[1],\n",
        "            inputs=[],\n",
        "            outputs=[image_input]\n",
        "        )\n",
        "\n",
        "\n",
        "        # 사용법 안내\n",
        "        gr.Markdown(\"\"\"\n",
        "        ---\n",
        "        ### 📋 사용법\n",
        "        1. **이미지 업로드**: 위의 이미지 업로드 영역에 이미지를 드래그하거나 클릭해서 업로드\n",
        "        2. **기능 선택**: 원하는 탭(분류/설명/질문답변)을 선택\n",
        "        3. **질문답변의 경우**: 질문 입력란에 궁금한 내용을 한글로 입력\n",
        "        4. **실행**: 각 탭의 버튼을 클릭해서 AI 분석 시작!\n",
        "\n",
        "        ### 💡 질문 예시\n",
        "        - \"이 이미지에 무엇이 있나요?\"\n",
        "        - \"몇 마리의 동물이 보이나요?\"\n",
        "        - \"주요 색상은 무엇인가요?\"\n",
        "        - \"어떤 장소에서 촬영된 것 같나요?\"\n",
        "\n",
        "        ### ⚡ 팁\n",
        "        - 샘플 이미지 아래 버튼을 클릭하면 자동으로 업로드됩니다\n",
        "        - 모든 기능은 한글을 지원합니다\n",
        "        - 고해상도 이미지도 처리 가능합니다\n",
        "        \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Gradio 앱 실행\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🎉 Gradio 멀티모달 AI 웹 시스템\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 인터페이스 생성 및 실행\n",
        "    demo = create_gradio_interface()\n",
        "\n",
        "    print(\"\\n🌐 웹 인터페이스를 시작합니다...\")\n",
        "    print(\"💡 브라우저에서 자동으로 열립니다!\")\n",
        "    print(\"🔗 수동 접속: http://localhost:7860\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    # 공개 링크로 실행 (Colab에서 접근 가능)\n",
        "    demo.launch(\n",
        "        share=False,          # 공개 링크 생성\n",
        "        inbrowser=True,      # 자동으로 브라우저 열기\n",
        "        show_error=True,     # 오류 표시\n",
        "        server_name=\"0.0.0.0\",  # 외부 접근 허용\n",
        "        server_port=7860,    # 포트 설정\n",
        "        height=800,          # 인터페이스 높이\n",
        "    )\n",
        "\n",
        "print(\"🎯 Gradio 멀티모달 AI 시스템 준비 완료!\")\n",
        "print(\"💻 Colab에서 실행 후 생성되는 링크를 클릭하세요!\")\n",
        "\n",
        "# 설치 문제 해결 가이드\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"❗ 설치 오류 해결 가이드\")\n",
        "print(\"=\"*50)\n",
        "print(\"\"\"\n",
        "🔧 의존성 충돌이 발생한 경우:\n",
        "\n",
        "1️⃣ **런타임 재시작**\n",
        "   - Runtime > Restart runtime 클릭\n",
        "   - 위 코드를 다시 실행\n",
        "\n",
        "2️⃣ **개별 설치 방법**\n",
        "   !pip install --upgrade pip\n",
        "   !pip install pillow requests matplotlib\n",
        "   !pip install torch torchvision\n",
        "   !pip install transformers\n",
        "   !pip install gradio\n",
        "\n",
        "3️⃣ **CLIP 설치 실패 시**\n",
        "   - CLIP 없이도 기본 기능은 작동합니다\n",
        "   - 캡션 생성과 분류는 BLIP과 ResNet으로 처리\n",
        "\n",
        "4️⃣ **완전 초기화**\n",
        "   - Runtime > Factory reset runtime\n",
        "   - 새로운 환경에서 다시 시작\n",
        "\n",
        "✅ 시스템이 정상 작동하면 위 메시지들을 무시하세요!\n",
        "\"\"\")\n",
        "print(\"=\"*50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1857e16ae1e14f3eabaac7b585b9dae6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3748f5eef43c4664b331d0ebcbff1ef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "509f471916f144c39835bf72710ced48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54fb760b6ae34ea4ba8f9cb50fb91f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "761abe9c8f1c4affa3f51a76f43ce234": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f57aecd666847b7906105abc908cb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a916a6adec0043caba5fab4d22b3a534",
              "IPY_MODEL_c66902cf3a124b5c80fec335b6e1a432",
              "IPY_MODEL_878736fc51264c08a63022bfb695b907"
            ],
            "layout": "IPY_MODEL_1857e16ae1e14f3eabaac7b585b9dae6"
          }
        },
        "878736fc51264c08a63022bfb695b907": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_509f471916f144c39835bf72710ced48",
            "placeholder": "​",
            "style": "IPY_MODEL_3748f5eef43c4664b331d0ebcbff1ef7",
            "value": " 1/1 [00:00&lt;00:00, 63.82it/s]"
          }
        },
        "a916a6adec0043caba5fab4d22b3a534": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d846aec4efae4bf3a2ffb11b0a19fdcb",
            "placeholder": "​",
            "style": "IPY_MODEL_54fb760b6ae34ea4ba8f9cb50fb91f6a",
            "value": "Fetching 1 files: 100%"
          }
        },
        "c66902cf3a124b5c80fec335b6e1a432": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_761abe9c8f1c4affa3f51a76f43ce234",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f62e445245e54ff1ae088efc47ed7a55",
            "value": 1
          }
        },
        "d846aec4efae4bf3a2ffb11b0a19fdcb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f62e445245e54ff1ae088efc47ed7a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
