from typing import List, Dict, Any, Optional
import os
import pickle
# LangChain imports
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.schema import Document
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

from sentence_transformers import SentenceTransformer

# =============================================================================
# ë‹¤ì¤‘ ì„ë² ë”© ë° ë²¡í„° DB ê´€ë¦¬ í´ë˜ìŠ¤ (ì„ë² ë”© & ë²¡í„° DBí™”)
# =============================================================================

class MultiEmbeddingManager:
    def __init__(self, api_token):
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸ë“¤
        self.embedding_models = {
            "embedding-gemma": {
                "name": "google/embeddinggemma-300m",
                "description": "ë‹¤êµ­ì–´ ì§€ì› SentenceTransformer ëª¨ë¸",
                "language": "Multilingual",
                "size": "~1.2G"
            },
            "ko-sroberta-multitask": {
                "name": "jhgan/ko-sroberta-multitask",
                "description": "í•œêµ­ì–´ íŠ¹í™” SentenceTransformer ëª¨ë¸",
                "language": "Korean",
                "size": "~300MB"
            },
            "paraphrase-multilingual": {
                "name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                "description": "ë‹¤êµ­ì–´ ì§€ì› ê²½ëŸ‰ ëª¨ë¸",
                "language": "Multilingual",
                "size": "~420MB"
            },
            "ko-sentence-transformer": {
                "name": "sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens",
                "description": "100ê°œ ì–¸ì–´ ì§€ì› BERT ê¸°ë°˜ ëª¨ë¸",
                "language": "Multilingual",
                "size": "~1.1GB"
            },
            "distiluse-multilingual": {
                "name": "sentence-transformers/distiluse-base-multilingual-cased-v2",
                "description": "ë‹¤êµ­ì–´ DistilUSE ëª¨ë¸ (ë¹ ë¦„)",
                "language": "Multilingual",
                "size": "~540MB"
            },
            "ko-electra": {
                "name": "bongsoo/kpf-sbert-128d",
                "description": "í•œêµ­ì–´ ELECTRA ê¸°ë°˜ ê²½ëŸ‰ ëª¨ë¸",
                "language": "Korean",
                "size": "~50MB"
            }
        }

        self.loaded_embeddings = {}
        self.current_model = None
        self.hf_api_token = api_token
        print(f"embedding manager with key: {self.hf_api_token}")

    def load_embedding_model(self, model_key: str) -> HuggingFaceEmbeddings:
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        if model_key in self.loaded_embeddings:
            return self.loaded_embeddings[model_key]

        if model_key not in self.embedding_models:
            raise ValueError(f"Unknown embedding model: {model_key}")

        model_info = self.embedding_models[model_key]
        print(f"Loading embedding model: {model_info['name']} ({model_info['size']})")
        print(f"my hf api token = {self.hf_api_token}")   
        try:

            if model_key == "embedding-gemma":
                
                # # Download from the ğŸ¤— Hub
                # model = SentenceTransformer("google/embeddinggemma-300m")
                embeddings = HuggingFaceEmbeddings(
                    model_name=model_info['name'],
                    model_kwargs={'device': 'cpu', "token": self.hf_api_token}
                )
            else:
                embeddings = HuggingFaceEmbeddings(
                    model_name=model_info['name'],
                    model_kwargs={'device': 'cpu', "token": self.hf_api_token}
                )
            self.loaded_embeddings[model_key] = embeddings
            self.current_model = model_key
            return embeddings
        except Exception as e:
            print(f"Failed to load {model_key}: {e}")
            # fallback to default model
            if model_key != "paraphrase-multilingual":
                return self.load_embedding_model("paraphrase-multilingual")
            raise e

    def get_available_models(self) -> Dict[str, str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        return {key: f"{info['description']} ({info['language']}, {info['size']})"
                for key, info in self.embedding_models.items()}

    def get_current_model_info(self) -> str:
        """í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        if self.current_model:
            info = self.embedding_models[self.current_model]
            return f"ğŸ¤– {info['description']} ({info['language']}, {info['size']})"
        return "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ"

class VectorStoreManager:
    def __init__(self, embedding_model_key: str = "embedding-gemma", save_directory: str = "faiss_indexes", hf_api_token: str = None):
        self.embedding_manager = MultiEmbeddingManager(api_token=hf_api_token)
        self.embeddings = self.embedding_manager.load_embedding_model(embedding_model_key)
        self.vector_stores = {}  # ëª¨ë¸ë³„ ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
        self.current_vector_store = None
        self.current_model_key = embedding_model_key
        self.save_directory = save_directory
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.save_directory, exist_ok=True)

    def switch_embedding_model(self, model_key: str) -> str:
        """ì„ë² ë”© ëª¨ë¸ ë³€ê²½"""
        try:
            print(f"Switching to embedding model: {model_key}")
            self.embeddings = self.embedding_manager.load_embedding_model(model_key)
            self.current_model_key = model_key

            # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ê°€ ìˆë‹¤ë©´ ì‚¬ìš©, ì—†ë‹¤ë©´ ìƒˆë¡œ ìƒì„± í•„ìš”
            if model_key in self.vector_stores:
                self.current_vector_store = self.vector_stores[model_key]
                return f"âœ… ëª¨ë¸ ë³€ê²½ ì™„ë£Œ: {self.embedding_manager.get_current_model_info()}"
            else:
                return f"âœ… ëª¨ë¸ ë³€ê²½ ì™„ë£Œ (ë²¡í„° ìŠ¤í† ì–´ ì¬ìƒì„± í•„ìš”): {self.embedding_manager.get_current_model_info()}"
        except Exception as e:
            return f"âŒ ëª¨ë¸ ë³€ê²½ ì‹¤íŒ¨: {str(e)}"

    def create_vector_store(self, documents: List[Document], model_key: str = None) -> FAISS:
        """ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
        if model_key and model_key != self.current_model_key:
            self.switch_embedding_model(model_key)

        print(f"ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘... (ëª¨ë¸: {self.current_model_key})")

        try:
            vector_store = FAISS.from_documents(documents, self.embeddings)
            self.vector_stores[self.current_model_key] = vector_store
            self.current_vector_store = vector_store
            return vector_store
        except Exception as e:
            print(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: {e}")
            # fallback modelë¡œ ì¬ì‹œë„
            if self.current_model_key != "paraphrase-multilingual":
                print("ê¸°ë³¸ ëª¨ë¸ë¡œ ì¬ì‹œë„...")
                self.switch_embedding_model("paraphrase-multilingual")
                vector_store = FAISS.from_documents(documents, self.embeddings)
                self.vector_stores[self.current_model_key] = vector_store
                self.current_vector_store = vector_store
                return vector_store
            raise e

    def add_documents(self, documents: List[Document]):
        """ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ì— ë¬¸ì„œ ì¶”ê°€"""
        if self.current_vector_store:
            self.current_vector_store.add_documents(documents)
            self.vector_stores[self.current_model_key] = self.current_vector_store
        else:
            self.create_vector_store(documents)

    def get_available_models(self) -> Dict[str, str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸ ëª©ë¡"""
        return self.embedding_manager.get_available_models()

    def get_current_model_info(self) -> str:
        """í˜„ì¬ ëª¨ë¸ ì •ë³´"""
        return self.embedding_manager.get_current_model_info()
    
    def save_vector_store(self, index_name: str, model_key: str = None) -> str:
        """ë²¡í„° ìŠ¤í† ì–´ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if model_key is None:
            model_key = self.current_model_key
            
        if model_key not in self.vector_stores:
            return f"âŒ ì €ì¥í•  ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤: {model_key}"
        
        try:
            # FAISS ì¸ë±ìŠ¤ ì €ì¥ ê²½ë¡œ
            index_path = os.path.join(self.save_directory, f"{index_name}_{model_key}")
            
            # FAISS ì¸ë±ìŠ¤ ì €ì¥
            vector_store = self.vector_stores[model_key]
            vector_store.save_local(index_path)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥ (ëª¨ë¸ ì •ë³´, ìƒì„±ì¼ì‹œ ë“±)
            metadata = {
                'model_key': model_key,
                'model_name': self.embedding_manager.embedding_models[model_key]['name'],
                'index_name': index_name,
                'created_at': __import__('datetime').datetime.now().isoformat(),
                'document_count': len(vector_store.docstore._dict) if hasattr(vector_store.docstore, '_dict') else 'unknown'
            }
            
            metadata_path = os.path.join(self.save_directory, f"{index_name}_{model_key}_metadata.pkl")
            with open(metadata_path, 'wb') as f:
                pickle.dump(metadata, f)
            
            return f"âœ… ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ: {index_path}"
            
        except Exception as e:
            return f"âŒ ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì‹¤íŒ¨: {str(e)}"
    
    def load_vector_store(self, index_name: str, model_key: str = None) -> str:
        """ì €ì¥ëœ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œ"""
        if model_key is None:
            model_key = self.current_model_key
        
        try:
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ ê²½ë¡œ
            index_path = os.path.join(self.save_directory, f"{index_name}_{model_key}")
            
            if not os.path.exists(index_path):
                return f"âŒ ë²¡í„° ìŠ¤í† ì–´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {index_path}"
            
            # í•´ë‹¹ ëª¨ë¸ì˜ ì„ë² ë”©ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë¡œë“œ
            if model_key != self.current_model_key:
                self.switch_embedding_model(model_key)
            
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            vector_store = FAISS.load_local(
                index_path, 
                self.embeddings, 
                allow_dangerous_deserialization=True
            )
            
            self.vector_stores[model_key] = vector_store
            self.current_vector_store = vector_store
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ (ìˆë‹¤ë©´)
            metadata_path = os.path.join(self.save_directory, f"{index_name}_{model_key}_metadata.pkl")
            metadata_info = ""
            if os.path.exists(metadata_path):
                with open(metadata_path, 'rb') as f:
                    metadata = pickle.load(f)
                    metadata_info = f" (ìƒì„±ì¼: {metadata.get('created_at', 'Unknown')}, ë¬¸ì„œìˆ˜: {metadata.get('document_count', 'Unknown')})"
            
            return f"âœ… ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ: {index_name}_{model_key}{metadata_info}"
            
        except Exception as e:
            return f"âŒ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
    
    def list_saved_indexes(self) -> Dict[str, Dict[str, Any]]:
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ëª©ë¡ ë°˜í™˜"""
        saved_indexes = {}
        
        if not os.path.exists(self.save_directory):
            return saved_indexes
        
        for filename in os.listdir(self.save_directory):
            if filename.endswith('.faiss'):
                # íŒŒì¼ëª…ì—ì„œ ì¸ë±ìŠ¤ëª…ê³¼ ëª¨ë¸í‚¤ ì¶”ì¶œ
                base_name = filename.replace('.faiss', '')
                # ë§ˆì§€ë§‰ '_'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
                parts = base_name.rsplit('_', 1)
                if len(parts) == 2:
                    index_name, model_key = parts
                    
                    # ë©”íƒ€ë°ì´í„° ì •ë³´ ë¡œë“œ
                    metadata_path = os.path.join(self.save_directory, f"{base_name}_metadata.pkl")
                    metadata = {}
                    if os.path.exists(metadata_path):
                        try:
                            with open(metadata_path, 'rb') as f:
                                metadata = pickle.load(f)
                        except:
                            pass
                    
                    if index_name not in saved_indexes:
                        saved_indexes[index_name] = {}
                    
                    saved_indexes[index_name][model_key] = {
                        'file_path': os.path.join(self.save_directory, filename),
                        'created_at': metadata.get('created_at', 'Unknown'),
                        'document_count': metadata.get('document_count', 'Unknown'),
                        'model_name': metadata.get('model_name', 'Unknown')
                    }
        
        return saved_indexes
    
    def delete_saved_index(self, index_name: str, model_key: str) -> str:
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ì‚­ì œ"""
        try:
            index_path = os.path.join(self.save_directory, f"{index_name}_{model_key}")
            metadata_path = os.path.join(self.save_directory, f"{index_name}_{model_key}_metadata.pkl")
            
            # FAISS ê´€ë ¨ íŒŒì¼ë“¤ ì‚­ì œ
            files_to_delete = [
                f"{index_path}.faiss",
                f"{index_path}.pkl",
                metadata_path
            ]
            
            deleted_files = []
            for file_path in files_to_delete:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    deleted_files.append(os.path.basename(file_path))
            
            if deleted_files:
                return f"âœ… ì¸ë±ìŠ¤ ì‚­ì œ ì™„ë£Œ: {', '.join(deleted_files)}"
            else:
                return f"âŒ ì‚­ì œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {index_name}_{model_key}"
                
        except Exception as e:
            return f"âŒ ì¸ë±ìŠ¤ ì‚­ì œ ì‹¤íŒ¨: {str(e)}"
    
    def auto_save_after_creation(self, documents: List[Document], index_name: str, model_key: str = None) -> FAISS:
        """ë¬¸ì„œë¡œë¶€í„° ë²¡í„° ìŠ¤í† ì–´ ìƒì„± í›„ ìë™ ì €ì¥"""
        vector_store = self.create_vector_store(documents, model_key)
        save_result = self.save_vector_store(index_name, model_key)
        print(save_result)
        return vector_store


if __name__ == "__main__":
    vector_store_manager = VectorStoreManager()