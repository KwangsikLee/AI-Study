{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install torch torchvision --quiet\n",
        "\n",
        "\n",
        "# 나눔고딕 폰트 설치 및 설정\n",
        "!apt-get update -qq\n",
        "!apt-get install fonts-nanum -qq\n",
        "!fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 폰트 설정\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path, size=10)\n",
        "plt.rcParams['font.family'] = 'NanumGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFX12qHe5fDi",
        "outputId": "b1da5655-16fb-4e4b-b52b-d87b2ebd6170"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SQjLGxKtUfE",
        "outputId": "652da7e1-c936-4e00-dc17-bd0b4b1cfa1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:01<00:00, 96.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6 - Validation Acc: 0.5580\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "# 2. Data Augmentation 및 데이터 준비\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=128, shuffle=False)\n",
        "\n",
        "# 3. 더 복잡한 CNN (Residual Block + Dropout + BatchNorm)\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1, stride),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        return F.relu(out)\n",
        "\n",
        "class DeepEnsembleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.resblock1 = ResidualBlock(32, 64, stride=2)\n",
        "        self.resblock2 = ResidualBlock(64, 128, stride=2)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.resblock1(x)\n",
        "        x = self.resblock2(x)\n",
        "        x = self.pool(x).flatten(1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# 4. 앙상블 (서로 다른 랜덤시드로 모델 2개 학습)\n",
        "def train_model(model, trainloader, testloader, epochs=5, patience=2):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.6)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_acc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in trainloader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(xb)\n",
        "            loss = criterion(pred, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # EarlyStopping & Validation 평가\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in testloader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                outputs = model(xb)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += yb.size(0)\n",
        "                correct += (predicted == yb).sum().item()\n",
        "        acc = correct / total\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Validation Acc: {acc:.4f}\")\n",
        "\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping!\")\n",
        "                break\n",
        "    return model\n",
        "\n",
        "torch.manual_seed(0)\n",
        "model1 = DeepEnsembleCNN()\n",
        "model1 = train_model(model1, trainloader, testloader, epochs=6, patience=2)\n",
        "\n",
        "torch.manual_seed(2024)\n",
        "model2 = DeepEnsembleCNN()\n",
        "model2 = train_model(model2, trainloader, testloader, epochs=6, patience=2)\n",
        "\n",
        "# 5. 앙상블 예측 및 성능 측정\n",
        "def ensemble_predict(models, loader):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    all_preds = []\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for xb, _ in loader:\n",
        "                xb = xb.to(device)\n",
        "                out = F.softmax(model(xb), dim=1)\n",
        "                preds.append(out.cpu().numpy())\n",
        "        all_preds.append(np.concatenate(preds, axis=0))\n",
        "    avg_preds = np.mean(all_preds, axis=0)\n",
        "    return avg_preds\n",
        "\n",
        "ensemble_probs = ensemble_predict([model1, model2], testloader)\n",
        "ensemble_pred = np.argmax(ensemble_probs, axis=1)\n",
        "\n",
        "test_labels = np.concatenate([y.numpy() for _, y in testloader])\n",
        "acc = (ensemble_pred == test_labels).mean()\n",
        "print(f\"\\n앙상블 Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "# 6. 혼동행렬, 시각화 등\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(test_labels, ensemble_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=trainset.classes)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"앙상블 모델 Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    }
  ]
}