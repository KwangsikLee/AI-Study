{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CfxZ-RowFgXD"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ ì„¤ì¹˜ (ìµœì´ˆ 1íšŒë§Œ)\n",
        "!pip -q install -U langchain langchain-openai langsmith\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IGt6PYlFhHE"
      },
      "outputs": [],
      "source": [
        "# 1) (ì„ íƒ) OpenAI ì‚¬ìš© ì‹œ\n",
        "import os, uuid\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "# OpenAI API í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
        "OPENAPI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "LangSmith_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "# 2) LangSmith ì—°ë™ í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"      # íŠ¸ë ˆì´ì‹± í™œì„±í™”\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"]   = \"https://api.smith.langchain.com\"  # ê¸°ë³¸ê°’\n",
        "os.environ[\"LANGSMITH_PROJECT\"]    = \"llm_colab_ex_RunnablePassthrough\"                 # ìˆ˜ì—…ìš© í”„ë¡œì íŠ¸ëª…\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNauo2w4FdX4",
        "outputId": "f0a9799d-80d7-4e1d-f6ab-498f74cead2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangSmithëŠ” LLM ì•±ì„ ê´€ì°°, í‰ê°€, ë””ë²„ê¹…í•  ìˆ˜ ìˆëŠ” í”Œë«í¼ì…ë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# â”€â”€ ì„í¬íŠ¸\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "# â”€â”€ LLM ì¤€ë¹„\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# â”€â”€ í”„ë¡¬í”„íŠ¸: ì§ˆë¬¸(question)ê³¼ ì»¨í…ìŠ¤íŠ¸(context)ë¥¼ ë°›ì•„ ë‹µë³€\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"ì§ˆë¬¸: {question}\\n\"\n",
        "    \"ì»¨í…ìŠ¤íŠ¸: {context}\\n\"\n",
        "    \"ğŸ‘‰ ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°„ë‹¨íˆ ë‹µí•´ì¤˜.\"\n",
        ")\n",
        "\n",
        "# â”€â”€ RunnablePassthroughë¥¼ ì‚¬ìš©í•œ ì²´ì¸\n",
        "# questionì€ ê·¸ëŒ€ë¡œ í†µê³¼, contextëŠ” ì „ì²˜ë¦¬ í•¨ìˆ˜ ì ìš©\n",
        "chain = (\n",
        "    {\n",
        "        \"question\": RunnablePassthrough(),  # ì…ë ¥ì˜ questionì„ ê·¸ëŒ€ë¡œ ì „ë‹¬\n",
        "        \"context\": RunnableLambda(lambda x: f\"[Extra Context Attached] {x['context']}\")\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | parser\n",
        ")\n",
        "\n",
        "# â”€â”€ ì‹¤í–‰ (LangSmith íŠ¸ë˜í‚¹ íƒœê·¸/ì´ë¦„ ì¶”ê°€)\n",
        "config = {\"tags\":[\"demo\",\"passthrough\"], \"run_name\":\"passthrough_demo\"}\n",
        "\n",
        "inputs = {\"question\": \"LangSmithëŠ” ë¬´ì—‡ì¸ê°€ìš”?\", \"context\": \"LangSmithëŠ” LLM ì•±ì„ ê´€ì°°/í‰ê°€/ë””ë²„ê¹…í•  ìˆ˜ ìˆëŠ” í”Œë«í¼ì´ë‹¤.\"}\n",
        "\n",
        "print(chain.invoke(inputs, config=config))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
