{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOXOwsZ7jSKe"
      },
      "outputs": [],
      "source": [
        "# 0) 최신 버전 설치\n",
        "!pip -q install -U langchain langchain-openai langchain-community langsmith\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qP87lh-jPSx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1) (선택) OpenAI 사용 시\n",
        "import os, uuid\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "# OpenAI API 클라이언트 생성\n",
        "OPENAPI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "LangSmith_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "# 2) LangSmith 연동 필수 환경변수\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"      # 트레이싱 활성화\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"]   = \"https://api.smith.langchain.com\"  # 기본값\n",
        "os.environ[\"LANGSMITH_PROJECT\"]    = \"llm_colab_ex_1\"                 # 수업용 프로젝트명\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpdx43PfjgFY",
        "outputId": "39518d3a-1063-4ab6-9b76-c11fbe392476"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangSmith는 주로 다음과 같은 이유로 사용됩니다:\n",
            "\n",
            "1. **언어 모델 개발**: LangSmith는 언어 모델을 쉽게 개발하고 테스트할 수 있는 플랫폼을 제공합니다.\n",
            "2. **협업 기능**: 팀원들과의 협업을 통해 모델을 개선하고 피드백을 받을 수 있습니다.\n",
            "3. **데이터 관리**: 데이터셋을 효율적으로 관리하고, 다양한 형식으로 데이터를 처리할 수 있습니다.\n",
            "4. **성능 분석**: 모델의 성능을 분석하고 최적화할 수 있는 도구를 제공합니다.\n",
            "5. **사용자 친화성**: 직관적인 인터페이스로 사용자가 쉽게 접근할 수 있습니다.\n",
            "\n",
            "이러한 기능들 덕분에 LangSmith는 언어 처리 프로젝트에 유용한 도구로 자리잡고 있습니다.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # 키 필요\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a concise assistant.\"),\n",
        "    (\"user\", \"{question}\")\n",
        "])\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "print(chain.invoke({\"question\":\"LangSmith를 왜 쓰나요?\"}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6iDYbbro6nN",
        "outputId": "d2a84429-e200-4a03-c1e4-757e6cf5511e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new agent_tools_demo chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `multiply` with `{'a': 23, 'b': 7}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m161\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `now_kst` with `{}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mUTC 2025-08-28 03:07:01\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2488277466.py:16: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().strftime(\"UTC %Y-%m-%d %H:%M:%S\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m23과 7을 곱한 결과는 161입니다. 현재 시간은 2025년 8월 28일 03:07:01 UTC입니다.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': '23과 7을 곱하고, 현재 시간을 알려줘.', 'output': '23과 7을 곱한 결과는 161입니다. 현재 시간은 2025년 8월 28일 03:07:01 UTC입니다.'}\n"
          ]
        }
      ],
      "source": [
        "from langchain.tools import tool\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from datetime import datetime\n",
        "\n",
        "# === 1. 도구 정의 ===\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"두 수를 곱한다.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "@tool\n",
        "def now_kst() -> str:\n",
        "    \"\"\"현재 UTC 시간을 반환한다 (KST 아님 예제용).\"\"\"\n",
        "    return datetime.utcnow().strftime(\"UTC %Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "tools = [multiply, now_kst]\n",
        "\n",
        "# === 2. LLM ===\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# === 3. 프롬프트 (⚠️ agent_scratchpad 반드시 포함해야 함) ===\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"너는 툴-사용 에이전트야. 계산은 multiply를, 시간은 now_kst를 사용해야 한다.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\")   # ← 필수\n",
        "])\n",
        "\n",
        "# === 4. 에이전트 생성 ===\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)\n",
        "executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "# === 5. 실행 ===\n",
        "config = {\n",
        "    \"tags\": [\"demo\", \"agent\", \"tools\", \"colab\"],\n",
        "    \"metadata\": {\"lesson\":\"ls-tracing-01\"},\n",
        "    \"run_name\": \"agent_tools_demo\"\n",
        "}\n",
        "\n",
        "print(executor.invoke(\n",
        "    {\"input\": \"23과 7을 곱하고, 현재 시간을 알려줘.\"},\n",
        "    config=config\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTro3_czpRs6",
        "outputId": "de5049ed-834b-4db9-e070-5aec507975a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "단건 실행:\n",
            " LangSmith는 LLM(대형 언어 모델) 앱을 관찰, 평가, 디버깅할 수 있는 플랫폼으로, RAG(정보 검색 기반 생성)와의 관계는 RAG가 LLM의 성능을 향상시키기 위해 외부 정보를 검색하고 활용하는 방식에 있습니다. LangSmith는 이러한 LLM 앱의 작동을 분석하고 최적화하는 도구를 제공함으로써 RAG의 효과적인 구현을 지원할 수 있습니다.\n",
            "\n",
            "배치 실행 결과:\n",
            "- Q: LangChain은 무엇이며 어떤 역할을 하나요?\n",
            "  A: LangChain은 LLM(대형 언어 모델) 기반 애플리케이션을 체인 형태로 구성하여 쉽게 개발하고 운영할 수 있도록 돕는 프레임워크입니다. 이를 통해 개발자들은 복잡한 언어 모델을 효과적으로 활용하여 다양한 애플리케이션을 구축할 수 있습니다.\n",
            "\n",
            "- Q: RAG가 답변 품질에 기여하는 방식은?\n",
            "  A: RAG(검색-생성 결합 기법)는 외부 지식을 검색하여 답변의 정확성을 높이는 방식으로 답변 품질에 기여합니다. 이를 통해 사용자는 보다 신뢰할 수 있는 정보를 제공받을 수 있으며, 생성된 답변이 최신의 관련 데이터를 반영하게 됩니다. 이러한 과정은 정보의 정확성과 신뢰성을 동시에 향상시켜, 전반적인 답변 품질을 개선하는 데 중요한 역할을 합니다.\n",
            "\n",
            "- Q: LangSmith를 활용하면 개발자가 얻는 이점은?\n",
            "  A: LangSmith를 활용하면 개발자는 LLM 앱의 성능을 효과적으로 관찰하고 평가할 수 있으며, 디버깅 과정에서 발생하는 문제를 신속하게 해결할 수 있는 이점을 얻습니다. 이를 통해 개발자는 앱의 품질을 향상시키고, 사용자 경험을 개선할 수 있습니다.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langsmith import traceable\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "# === 1) 간단한 로컬 문서 코퍼스 ===\n",
        "CORPUS = [\n",
        "    {\"title\":\"LangSmith 소개\", \"text\":\"LangSmith는 LLM 앱을 관찰, 평가, 디버깅할 수 있는 플랫폼입니다.\"},\n",
        "    {\"title\":\"LangChain 소개\", \"text\":\"LangChain은 LLM 기반 앱을 체인으로 구성해 쉽게 개발/운영하도록 돕는 프레임워크입니다.\"},\n",
        "    {\"title\":\"RAG 개요\", \"text\":\"RAG는 검색과 생성 결합 기법으로, 외부 지식을 검색해 답변의 정확성을 높입니다.\"},\n",
        "]\n",
        "\n",
        "# === 2) 추출기(retriever) & 요약기(summarizer)를 traceable로 감싸기 ===\n",
        "@traceable(name=\"keyword_retriever\")\n",
        "def keyword_retrieve(query: str, topk: int = 2):\n",
        "    q = query.lower()\n",
        "    scored = []\n",
        "    for doc in CORPUS:\n",
        "        score = sum(1 for tok in q.split() if tok in doc[\"text\"].lower())\n",
        "        scored.append((score, doc))\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    return [d for s, d in scored[:topk] if s > 0] or [CORPUS[0]]\n",
        "\n",
        "@traceable(name=\"summarize_docs\")\n",
        "def summarize_docs(docs):\n",
        "    joined = \"\\n\".join([f\"- {d['title']}: {d['text']}\" for d in docs])\n",
        "    return joined\n",
        "\n",
        "# === 3) 체인 구성: retrieve -> summarize -> prompt -> LLM -> parser ===\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"질문: {question}\\n\"\n",
        "    \"관련 문서:\\n{context}\\n\"\n",
        "    \"문서 근거를 바탕으로 한 문단으로 간결히 답해줘.\"\n",
        ")\n",
        "\n",
        "def rag_pipeline(question: str):\n",
        "    docs = keyword_retrieve(question)\n",
        "    ctx = summarize_docs(docs)\n",
        "    chain = (\n",
        "        {\"question\": RunnablePassthrough(), \"context\": lambda q: ctx}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    # tags/metadata/run_name을 붙여 LangSmith에서 찾기 쉽게!\n",
        "    return chain.invoke(question, config={\n",
        "        \"tags\": [\"demo\", \"rag\", \"batch\", \"colab\"],\n",
        "        \"metadata\": {\"lesson\":\"ls-tracing-02\"},\n",
        "        \"run_name\": \"mini_rag_pipeline\"\n",
        "    })\n",
        "\n",
        "# === 4) 단건 실행 ===\n",
        "print(\"단건 실행:\\n\", rag_pipeline(\"LangSmith와 RAG의 관계는?\"))\n",
        "\n",
        "# === 5) 배치 실행 (여러 질문을 한꺼번에) ===\n",
        "questions = [\n",
        "    \"LangChain은 무엇이며 어떤 역할을 하나요?\",\n",
        "    \"RAG가 답변 품질에 기여하는 방식은?\",\n",
        "    \"LangSmith를 활용하면 개발자가 얻는 이점은?\"\n",
        "]\n",
        "\n",
        "# .batch를 쓰려면 chain을 Runnable로 구성해야 하므로, 위 람다를 조금 변경한 버전:\n",
        "def build_rag_chain():\n",
        "    # Retrieval과 Summarization은 traceable 함수로 개별 런이 생김\n",
        "    def _ctx_fn(q):\n",
        "        return summarize_docs(keyword_retrieve(q))\n",
        "    chain = (\n",
        "        {\"question\": RunnablePassthrough(), \"context\": _ctx_fn}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    return chain\n",
        "\n",
        "rag_chain = build_rag_chain()\n",
        "\n",
        "batch_outputs = rag_chain.batch(\n",
        "    questions,\n",
        "    config={\"tags\":[\"demo\",\"rag\",\"batch\"], \"metadata\":{\"lesson\":\"ls-tracing-02\"}, \"run_name\":\"mini_rag_batch\"},\n",
        "    return_exceptions=True  # 에러도 LangSmith에 기록됩니다\n",
        ")\n",
        "\n",
        "print(\"\\n배치 실행 결과:\")\n",
        "for q, ans in zip(questions, batch_outputs):\n",
        "    print(f\"- Q: {q}\\n  A: {ans}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
