{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3mtar7frMyD",
        "outputId": "39b70162-0ae3-4c5b-ab88-f13fadf0139e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m835.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# =========================================\n",
        "# 0) 설치 (Colab 최초 실행 시)\n",
        "# =========================================\n",
        "!pip -q install -U langchain langchain-openai langchain-community langsmith faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAcrz5DxrQoJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1) (선택) OpenAI 사용 시\n",
        "import os, uuid\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "# OpenAI API 클라이언트 생성\n",
        "OPENAPI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "LangSmith_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "# 2) LangSmith 연동 필수 환경변수\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"      # 트레이싱 활성화\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"]   = \"https://api.smith.langchain.com\"  # 기본값\n",
        "os.environ[\"LANGSMITH_PROJECT\"]    = \"llm_colab_ex_2\"                 # 수업용 프로젝트명\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOg2x2sorKmy",
        "outputId": "4341e4be-1382-4882-bc2a-954082e39895"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 청킹된 문서 개수: 4\n",
            "Chunk 0: LangSmith는 LLM 애플리케이션을 관찰, 평가, 디버깅할 수 있는 플랫폼입니다.\n",
            "Chunk 1: LangChain은 LLM을 체인으로 구성해 쉽게 개발/운영하도록 돕는 프레임워크입니다.\n",
            "Chunk 2: RAG(Retrieval-Augmented Generation)는 외부 지식을 검색해 답변의 정확성을 높이는 기법입니다.\n",
            "Chunk 3: 임베딩(embedding)을 사용해 텍스트를 벡터화하고, 벡터스토어에서 유사도를 기반으로 검색합니다.\n",
            "LangSmith는 LLM 애플리케이션을 관찰, 평가, 디버깅할 수 있는 플랫폼이며, RAG는 외부 지식을 검색해 답변의 정확성을 높이는 기법입니다. 두 개념은 LLM의 성능을 향상시키기 위해 함께 사용될 수 있습니다.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# =========================================\n",
        "# 2) 문서 예제 준비\n",
        "# =========================================\n",
        "docs_text = \"\"\"\n",
        "LangSmith는 LLM 애플리케이션을 관찰, 평가, 디버깅할 수 있는 플랫폼입니다.\n",
        "LangChain은 LLM을 체인으로 구성해 쉽게 개발/운영하도록 돕는 프레임워크입니다.\n",
        "RAG(Retrieval-Augmented Generation)는 외부 지식을 검색해 답변의 정확성을 높이는 기법입니다.\n",
        "임베딩(embedding)을 사용해 텍스트를 벡터화하고, 벡터스토어에서 유사도를 기반으로 검색합니다.\n",
        "\"\"\"\n",
        "\n",
        "# =========================================\n",
        "# 3) 청킹 (문서 분할)\n",
        "# =========================================\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=80, chunk_overlap=20)\n",
        "docs = splitter.create_documents([docs_text])\n",
        "\n",
        "print(\"✅ 청킹된 문서 개수:\", len(docs))\n",
        "for i, d in enumerate(docs):\n",
        "    print(f\"Chunk {i}:\", d.page_content)\n",
        "\n",
        "# =========================================\n",
        "# 4) 임베딩 & 벡터스토어 (FAISS)\n",
        "# =========================================\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# =========================================\n",
        "# 5) Retriever + LLM + Prompt → 체인\n",
        "# =========================================\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"질문: {question}\\n\"\n",
        "    \"검색된 문서:\\n{context}\\n\"\n",
        "    \"👉 위 문서 내용을 기반으로 한국어로 간결히 답해줘.\"\n",
        ")\n",
        "\n",
        "# 체인 정의 (Runnable 조합)\n",
        "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# =========================================\n",
        "# 6) LangSmith 트래킹 실행\n",
        "# =========================================\n",
        "config = {\n",
        "    \"tags\": [\"demo\", \"rag\", \"chunking\", \"embedding\"],\n",
        "    \"metadata\": {\"lesson\":\"ls-tracing-03\"},\n",
        "    \"run_name\": \"rag_with_chunking_embedding\"\n",
        "}\n",
        "\n",
        "print(rag_chain.invoke(\"LangSmith와 RAG는 어떤 관계가 있나요?\", config=config))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "langchain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
