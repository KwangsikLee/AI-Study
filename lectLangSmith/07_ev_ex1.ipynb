{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangSmith & simple evaluation\n",
        "\n",
        "# simple evaluation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "asTgozvhYF7o"
      },
      "outputs": [],
      "source": [
        "# ========== 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜ ==========\n",
        "!pip install langsmith langchain langchain-openai langchain-community chromadb tiktoken --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UTutrzhXeaf9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ========== 2. í™˜ê²½ ì„¤ì • ==========\n",
        "import os\n",
        "import getpass\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpvyoDYhedVx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# OpenAI API í‚¤ ì„¤ì • (ì‚¬ìš©ìê°€ ì…ë ¥í•´ì•¼ í•¨)\n",
        "# from google.colab import userdata\n",
        "# api_key=userdata.get('api_key')\n",
        "# os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "# OpenAI API í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
        "OPENAPI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "LangSmith_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "\n",
        "# 2) LangSmith ì—°ë™ í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"      # íŠ¸ë ˆì´ì‹± í™œì„±í™”\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"]   = \"https://api.smith.langchain.com\"  # ê¸°ë³¸ê°’\n",
        "os.environ[\"LANGSMITH_PROJECT\"]    = \"RAG_EV_ex1\"                 # ìˆ˜ì—…ìš© í”„ë¡œì íŠ¸ëª…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFvhLydRfDxu",
        "outputId": "057c9581-2002-4c8a-f577-43cb9293bdfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ìƒì„±ëœ ì²­í¬ ìˆ˜: 3\n",
            "\n",
            "===== í•©ì„± í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì‹œì‘ =====\n",
            "âœ… Simple ì§ˆë¬¸ 6ê°œ ìƒì„± ì™„ë£Œ\n",
            "âœ… Reasoning ì§ˆë¬¸ 3ê°œ ìƒì„± ì™„ë£Œ\n",
            "âœ… Multi-context ì§ˆë¬¸ 2ê°œ ìƒì„± ì™„ë£Œ\n",
            "\n",
            "ì´ 11ê°œì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„± ì™„ë£Œ!\n",
            "\n",
            "===== ìƒì„±ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¯¸ë¦¬ë³´ê¸° =====\n",
            "\n",
            "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1]\n",
            "ì§ˆë¬¸: ì¸ê³µì§€ëŠ¥(AI)ì€ ë¬´ì—‡ì„ êµ¬í˜„í•œ ì»´í“¨í„° ì‹œìŠ¤í…œì¸ê°€ìš”?\n",
            "ì •ë‹µ: ì¸ê°„ì˜ í•™ìŠµëŠ¥ë ¥, ì¶”ë¡ ëŠ¥ë ¥, ì§€ê°ëŠ¥ë ¥ì„ ì¸ê³µì ìœ¼ë¡œ êµ¬í˜„í•œ ì»´í“¨í„° ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n",
            "ìœ í˜•: simple\n",
            "ì»¨í…ìŠ¤íŠ¸ ìˆ˜: 1\n",
            "--------------------------------------------------\n",
            "\n",
            "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2]\n",
            "ì§ˆë¬¸: ë¨¸ì‹ ëŸ¬ë‹ì€ AIì˜ ì–´ë–¤ ë¶„ì•¼ì¸ê°€ìš”?\n",
            "ì •ë‹µ: AIì˜ í•œ ë¶„ì•¼ë¡œ, ë°ì´í„°ë¥¼ í†µí•´ ì»´í“¨í„°ê°€ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ë„ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
            "ìœ í˜•: simple\n",
            "ì»¨í…ìŠ¤íŠ¸ ìˆ˜: 1\n",
            "--------------------------------------------------\n",
            "\n",
            "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3]\n",
            "ì§ˆë¬¸: ë”¥ëŸ¬ë‹ì€ ë¬´ì—‡ì„ í†µí•´ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ë‚˜ìš”?\n",
            "ì •ë‹µ: ì¸ê³µì‹ ê²½ë§ì„ ì—¬ëŸ¬ ì¸µìœ¼ë¡œ ìŒ“ì•„ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
            "ìœ í˜•: simple\n",
            "ì»¨í…ìŠ¤íŠ¸ ìˆ˜: 1\n",
            "--------------------------------------------------\n",
            "\n",
            "âœ… ë°ì´í„°ì…‹ì´ LangSmithì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!\n",
            "ë°ì´í„°ì…‹ ì´ë¦„: synthetic_qa_dataset_v3\n",
            "í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìˆ˜: 11\n",
            "\n",
            "===== ë°ì´í„°ì…‹ í†µê³„ =====\n",
            "evolution_type\n",
            "simple           6\n",
            "reasoning        3\n",
            "multi_context    2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "ì§ˆë¬¸ ê¸¸ì´:\n",
            "  í‰ê· : 28.8 ê¸€ì\n",
            "  ìµœì†Œ: 18 ê¸€ì\n",
            "  ìµœëŒ€: 37 ê¸€ì\n",
            "\n",
            "ë‹µë³€ ê¸¸ì´:\n",
            "  í‰ê· : 121.7 ê¸€ì\n",
            "  ìµœì†Œ: 16 ê¸€ì\n",
            "  ìµœëŒ€: 329 ê¸€ì\n",
            "\n",
            "âœ… ë°ì´í„°ì…‹ì´ 'synthetic_qa_dataset.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "===== ë°ì´í„° í’ˆì§ˆ ê²€ì¦ =====\n",
            "âœ… ì¤‘ë³µ ì§ˆë¬¸ ì—†ìŒ\n",
            "âœ… ëª¨ë“  ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë¨\n",
            "\n",
            "===== ìƒ˜í”Œ í’ˆì§ˆ í‰ê°€ (ì²˜ìŒ 3ê°œ) =====\n",
            "\n",
            "ì¼€ì´ìŠ¤ 1:\n",
            "  ì»¨í…ìŠ¤íŠ¸ ê²¹ì¹¨: 100.00%\n",
            "  ì™„ì„±ë„: âœ…\n",
            "\n",
            "ì¼€ì´ìŠ¤ 2:\n",
            "  ì»¨í…ìŠ¤íŠ¸ ê²¹ì¹¨: 100.00%\n",
            "  ì™„ì„±ë„: âœ…\n",
            "\n",
            "ì¼€ì´ìŠ¤ 3:\n",
            "  ì»¨í…ìŠ¤íŠ¸ ê²¹ì¹¨: 100.00%\n",
            "  ì™„ì„±ë„: âœ…\n",
            "\n",
            "ğŸ‰ í•©ì„± í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!\n",
            "ì´ 11ê°œì˜ ê³ í’ˆì§ˆ QA ìŒì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# ========== 3. ìƒ˜í”Œ ë¬¸ì„œ ì¤€ë¹„ ==========\n",
        "sample_documents = [\n",
        "    \"\"\"\n",
        "    ì¸ê³µì§€ëŠ¥(AI)ì€ ì¸ê°„ì˜ í•™ìŠµëŠ¥ë ¥, ì¶”ë¡ ëŠ¥ë ¥, ì§€ê°ëŠ¥ë ¥ì„ ì¸ê³µì ìœ¼ë¡œ êµ¬í˜„í•œ ì»´í“¨í„° ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n",
        "    ë¨¸ì‹ ëŸ¬ë‹ì€ AIì˜ í•œ ë¶„ì•¼ë¡œ, ë°ì´í„°ë¥¼ í†µí•´ ì»´í“¨í„°ê°€ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ë„ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
        "    ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë°©ë²•ìœ¼ë¡œ, ì¸ê³µì‹ ê²½ë§ì„ ì—¬ëŸ¬ ì¸µìœ¼ë¡œ ìŒ“ì•„ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
        "    ìµœê·¼ì—ëŠ” GPT, BERT ê°™ì€ ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸ì´ AI ë°œì „ì„ ì£¼ë„í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    ìì—°ì–´ì²˜ë¦¬(NLP)ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
        "    ìµœê·¼ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ë“±ì¥ìœ¼ë¡œ NLP ë¶„ì•¼ëŠ” í° ë°œì „ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤.\n",
        "    BERTëŠ” ì–‘ë°©í–¥ ì¸ì½”ë” í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ë§¥ì„ ì´í•´í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.\n",
        "    GPTëŠ” ìê¸°íšŒê·€ ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì–¸ì–´ëª¨ë¸ì…ë‹ˆë‹¤.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    RAG(Retrieval-Augmented Generation)ëŠ” ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•œ AI ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
        "    ì™¸ë¶€ ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•œ í›„, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "    ì´ë¥¼ í†µí•´ LLMì˜ í™˜ê°(hallucination) ë¬¸ì œë¥¼ ì™„í™”í•˜ê³  ìµœì‹  ì •ë³´ë¥¼ ë°˜ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "]\n",
        "\n",
        "# Document ê°ì²´ë¡œ ë³€í™˜\n",
        "documents = [Document(page_content=doc, metadata={\"source\": f\"doc_{i}\"})\n",
        "             for i, doc in enumerate(sample_documents)]\n",
        "\n",
        "# ========== 4. í…ìŠ¤íŠ¸ ë¶„í•  ==========\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}\")\n",
        "\n",
        "# ========== 5. LLM ê¸°ë°˜ QA ìƒì„± í´ë˜ìŠ¤ ==========\n",
        "class SyntheticQAGenerator:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "\n",
        "    def generate_simple_questions(self, text: str, num_questions: int = 3) -> List[Dict]:\n",
        "        \"\"\"ë‹¨ìˆœ ì‚¬ì‹¤ í™•ì¸ ì§ˆë¬¸ ìƒì„±\"\"\"\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"text\", \"num_questions\"],\n",
        "            template=\"\"\"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ì½ê³  ì§ì ‘ì ì¸ ì‚¬ì‹¤ í™•ì¸ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n",
        "\n",
        "í…ìŠ¤íŠ¸:\n",
        "{text}\n",
        "\n",
        "ìš”êµ¬ì‚¬í•­:\n",
        "- {num_questions}ê°œì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì„ ìƒì„±í•˜ì„¸ìš”\n",
        "- ì§ˆë¬¸ì€ í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì°¾ì„ ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ì–´ì•¼ í•©ë‹ˆë‹¤\n",
        "- ë‹µë³€ì€ ê°„ê²°í•˜ê³  ì •í™•í•´ì•¼ í•©ë‹ˆë‹¤\n",
        "\n",
        "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:\n",
        "[\n",
        "    {{\n",
        "        \"question\": \"ì§ˆë¬¸ ë‚´ìš©\",\n",
        "        \"answer\": \"ë‹µë³€ ë‚´ìš©\"\n",
        "    }},\n",
        "    ...\n",
        "]\n",
        "\"\"\"\n",
        "        )\n",
        "\n",
        "        response = self.llm.invoke(prompt.format(text=text, num_questions=num_questions))\n",
        "\n",
        "        try:\n",
        "            # JSON íŒŒì‹±\n",
        "            qa_pairs = json.loads(response.content)\n",
        "            return [\n",
        "                {\n",
        "                    \"question\": qa[\"question\"],\n",
        "                    \"ground_truth\": qa[\"answer\"],\n",
        "                    \"contexts\": [text],\n",
        "                    \"evolution_type\": \"simple\"\n",
        "                }\n",
        "                for qa in qa_pairs\n",
        "            ]\n",
        "        except:\n",
        "            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë°©ë²•\n",
        "            return self._parse_fallback(response.content, text, \"simple\")\n",
        "\n",
        "    def generate_reasoning_questions(self, texts: List[str], num_questions: int = 2) -> List[Dict]:\n",
        "        \"\"\"ì¶”ë¡ ì´ í•„ìš”í•œ ì§ˆë¬¸ ìƒì„±\"\"\"\n",
        "        combined_text = \"\\n\\n\".join(texts)\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"text\", \"num_questions\"],\n",
        "            template=\"\"\"ë‹¤ìŒ í…ìŠ¤íŠ¸ë“¤ì„ ì½ê³  ì¶”ë¡ ì´ í•„ìš”í•œ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n",
        "\n",
        "í…ìŠ¤íŠ¸:\n",
        "{text}\n",
        "\n",
        "ìš”êµ¬ì‚¬í•­:\n",
        "- {num_questions}ê°œì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì„ ìƒì„±í•˜ì„¸ìš”\n",
        "- ì§ˆë¬¸ì€ ì—¬ëŸ¬ ì •ë³´ë¥¼ ì¢…í•©í•˜ê±°ë‚˜ ì¶”ë¡ ì´ í•„ìš”í•œ ë‚´ìš©ì´ì–´ì•¼ í•©ë‹ˆë‹¤\n",
        "- ë‹µë³€ì€ ë…¼ë¦¬ì ì´ê³  ê·¼ê±°ê°€ ëª…í™•í•´ì•¼ í•©ë‹ˆë‹¤\n",
        "\n",
        "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:\n",
        "[\n",
        "    {{\n",
        "        \"question\": \"ì¶”ë¡ ì´ í•„ìš”í•œ ì§ˆë¬¸\",\n",
        "        \"answer\": \"ë…¼ë¦¬ì ì¸ ë‹µë³€\"\n",
        "    }},\n",
        "    ...\n",
        "]\n",
        "\"\"\"\n",
        "        )\n",
        "\n",
        "        response = self.llm.invoke(prompt.format(text=combined_text, num_questions=num_questions))\n",
        "\n",
        "        try:\n",
        "            qa_pairs = json.loads(response.content)\n",
        "            return [\n",
        "                {\n",
        "                    \"question\": qa[\"question\"],\n",
        "                    \"ground_truth\": qa[\"answer\"],\n",
        "                    \"contexts\": texts,\n",
        "                    \"evolution_type\": \"reasoning\"\n",
        "                }\n",
        "                for qa in qa_pairs\n",
        "            ]\n",
        "        except:\n",
        "            return self._parse_fallback(response.content, texts, \"reasoning\")\n",
        "\n",
        "    def generate_multi_context_questions(self, texts: List[str], num_questions: int = 2) -> List[Dict]:\n",
        "        \"\"\"ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ì°¸ì¡°í•´ì•¼ í•˜ëŠ” ì§ˆë¬¸ ìƒì„±\"\"\"\n",
        "        combined_text = \"\\n\\n\".join(texts)\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"text\", \"num_questions\"],\n",
        "            template=\"\"\"ë‹¤ìŒ ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ì¢…í•©í•˜ì—¬ ë‹µí•´ì•¼ í•˜ëŠ” ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n",
        "\n",
        "í…ìŠ¤íŠ¸:\n",
        "{text}\n",
        "\n",
        "ìš”êµ¬ì‚¬í•­:\n",
        "- {num_questions}ê°œì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì„ ìƒì„±í•˜ì„¸ìš”\n",
        "- ì§ˆë¬¸ì€ ì—¬ëŸ¬ ë¬¸ì„œì˜ ì •ë³´ë¥¼ ì¢…í•©í•´ì•¼ ë‹µí•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤\n",
        "- ë‹µë³€ì€ í¬ê´„ì ì´ê³  ìƒì„¸í•´ì•¼ í•©ë‹ˆë‹¤\n",
        "\n",
        "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:\n",
        "[\n",
        "    {{\n",
        "        \"question\": \"ì¢…í•©ì ì¸ ì§ˆë¬¸\",\n",
        "        \"answer\": \"í¬ê´„ì ì¸ ë‹µë³€\"\n",
        "    }},\n",
        "    ...\n",
        "]\n",
        "\"\"\"\n",
        "        )\n",
        "\n",
        "        response = self.llm.invoke(prompt.format(text=combined_text, num_questions=num_questions))\n",
        "\n",
        "        try:\n",
        "            qa_pairs = json.loads(response.content)\n",
        "            return [\n",
        "                {\n",
        "                    \"question\": qa[\"question\"],\n",
        "                    \"ground_truth\": qa[\"answer\"],\n",
        "                    \"contexts\": texts,\n",
        "                    \"evolution_type\": \"multi_context\"\n",
        "                }\n",
        "                for qa in qa_pairs\n",
        "            ]\n",
        "        except:\n",
        "            return self._parse_fallback(response.content, texts, \"multi_context\")\n",
        "\n",
        "    def _parse_fallback(self, content: str, contexts, evolution_type: str) -> List[Dict]:\n",
        "        \"\"\"JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ íŒŒì‹±\"\"\"\n",
        "        qa_pairs = []\n",
        "        lines = content.strip().split('\\n')\n",
        "\n",
        "        current_q = None\n",
        "        for line in lines:\n",
        "            if '\"question\"' in line.lower() or 'q:' in line.lower():\n",
        "                # ì§ˆë¬¸ ì¶”ì¶œ\n",
        "                if ':' in line:\n",
        "                    current_q = line.split(':', 1)[1].strip().strip('\"').strip(',')\n",
        "            elif '\"answer\"' in line.lower() or 'a:' in line.lower():\n",
        "                # ë‹µë³€ ì¶”ì¶œ\n",
        "                if current_q and ':' in line:\n",
        "                    answer = line.split(':', 1)[1].strip().strip('\"').strip(',').strip('}')\n",
        "                    qa_pairs.append({\n",
        "                        \"question\": current_q,\n",
        "                        \"ground_truth\": answer,\n",
        "                        \"contexts\": contexts if isinstance(contexts, list) else [contexts],\n",
        "                        \"evolution_type\": evolution_type\n",
        "                    })\n",
        "                    current_q = None\n",
        "\n",
        "        return qa_pairs\n",
        "\n",
        "# ========== 6. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± ==========\n",
        "# LLM ì´ˆê¸°í™”\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
        "\n",
        "# QA ìƒì„±ê¸° ì´ˆê¸°í™”\n",
        "qa_generator = SyntheticQAGenerator(llm)\n",
        "\n",
        "print(\"\\n===== í•©ì„± í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì‹œì‘ =====\")\n",
        "\n",
        "# 1. Simple ì§ˆë¬¸ ìƒì„±\n",
        "simple_qa = []\n",
        "for chunk in chunks[:2]:\n",
        "    qa_pairs = qa_generator.generate_simple_questions(chunk.page_content, num_questions=3)\n",
        "    simple_qa.extend(qa_pairs)\n",
        "\n",
        "print(f\"âœ… Simple ì§ˆë¬¸ {len(simple_qa)}ê°œ ìƒì„± ì™„ë£Œ\")\n",
        "\n",
        "# 2. Reasoning ì§ˆë¬¸ ìƒì„±\n",
        "reasoning_qa = qa_generator.generate_reasoning_questions(\n",
        "    [chunk.page_content for chunk in chunks[:3]],\n",
        "    num_questions=3\n",
        ")\n",
        "print(f\"âœ… Reasoning ì§ˆë¬¸ {len(reasoning_qa)}ê°œ ìƒì„± ì™„ë£Œ\")\n",
        "\n",
        "# 3. Multi-context ì§ˆë¬¸ ìƒì„±\n",
        "multi_context_qa = qa_generator.generate_multi_context_questions(\n",
        "    [chunk.page_content for chunk in chunks],\n",
        "    num_questions=2\n",
        ")\n",
        "print(f\"âœ… Multi-context ì§ˆë¬¸ {len(multi_context_qa)}ê°œ ìƒì„± ì™„ë£Œ\")\n",
        "\n",
        "# ëª¨ë“  QA ê²°í•©\n",
        "all_qa = simple_qa + reasoning_qa + multi_context_qa\n",
        "test_df = pd.DataFrame(all_qa)\n",
        "\n",
        "print(f\"\\nì´ {len(test_df)}ê°œì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„± ì™„ë£Œ!\")\n",
        "\n",
        "# ========== 7. ê²°ê³¼ í™•ì¸ ==========\n",
        "print(\"\\n===== ìƒì„±ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¯¸ë¦¬ë³´ê¸° =====\")\n",
        "for idx, row in test_df.head(3).iterrows():\n",
        "    print(f\"\\n[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {idx + 1}]\")\n",
        "    print(f\"ì§ˆë¬¸: {row['question']}\")\n",
        "    print(f\"ì •ë‹µ: {row['ground_truth']}\")\n",
        "    print(f\"ìœ í˜•: {row['evolution_type']}\")\n",
        "    print(f\"ì»¨í…ìŠ¤íŠ¸ ìˆ˜: {len(row['contexts'])}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# ========== 8. LangSmithì— ë°ì´í„°ì…‹ ì—…ë¡œë“œ ==========\n",
        "from langsmith import Client\n",
        "\n",
        "try:\n",
        "    client = Client()\n",
        "\n",
        "    # ë°ì´í„°ì…‹ ìƒì„±\n",
        "    dataset_name = \"synthetic_qa_dataset_v3\"\n",
        "    dataset = client.create_dataset(\n",
        "        dataset_name=dataset_name,\n",
        "        description=\"LLMìœ¼ë¡œ ìƒì„±í•œ í•©ì„± Q&A í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹\"\n",
        "    )\n",
        "\n",
        "    # ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ LangSmithì— ì¶”ê°€\n",
        "    for idx, row in test_df.iterrows():\n",
        "        client.create_example(\n",
        "            dataset_id=dataset.id,\n",
        "            inputs={\n",
        "                \"question\": row['question'],\n",
        "                \"contexts\": row['contexts']\n",
        "            },\n",
        "            outputs={\n",
        "                \"answer\": row['ground_truth']\n",
        "            },\n",
        "            metadata={\n",
        "                \"evolution_type\": row['evolution_type']\n",
        "            }\n",
        "        )\n",
        "\n",
        "    print(f\"\\nâœ… ë°ì´í„°ì…‹ì´ LangSmithì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "    print(f\"ë°ì´í„°ì…‹ ì´ë¦„: {dataset_name}\")\n",
        "    print(f\"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìˆ˜: {len(test_df)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâš ï¸ LangSmith ì—…ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "    print(\"ë¡œì»¬ì—ë§Œ ì €ì¥í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "# ========== 9. ë°ì´í„°ì…‹ í†µê³„ ë° ë¶„ì„ ==========\n",
        "print(\"\\n===== ë°ì´í„°ì…‹ í†µê³„ =====\")\n",
        "print(test_df['evolution_type'].value_counts())\n",
        "\n",
        "# ì§ˆë¬¸/ë‹µë³€ ê¸¸ì´ ë¶„ì„\n",
        "test_df['question_length'] = test_df['question'].str.len()\n",
        "test_df['answer_length'] = test_df['ground_truth'].str.len()\n",
        "\n",
        "print(f\"\\nì§ˆë¬¸ ê¸¸ì´:\")\n",
        "print(f\"  í‰ê· : {test_df['question_length'].mean():.1f} ê¸€ì\")\n",
        "print(f\"  ìµœì†Œ: {test_df['question_length'].min()} ê¸€ì\")\n",
        "print(f\"  ìµœëŒ€: {test_df['question_length'].max()} ê¸€ì\")\n",
        "\n",
        "print(f\"\\në‹µë³€ ê¸¸ì´:\")\n",
        "print(f\"  í‰ê· : {test_df['answer_length'].mean():.1f} ê¸€ì\")\n",
        "print(f\"  ìµœì†Œ: {test_df['answer_length'].min()} ê¸€ì\")\n",
        "print(f\"  ìµœëŒ€: {test_df['answer_length'].max()} ê¸€ì\")\n",
        "\n",
        "# ========== 10. CSVë¡œ ì €ì¥ ==========\n",
        "test_df.to_csv('synthetic_qa_dataset.csv', index=False, encoding='utf-8-sig')\n",
        "print(\"\\nâœ… ë°ì´í„°ì…‹ì´ 'synthetic_qa_dataset.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# ========== 11. í’ˆì§ˆ ê²€ì¦ ==========\n",
        "print(\"\\n===== ë°ì´í„° í’ˆì§ˆ ê²€ì¦ =====\")\n",
        "\n",
        "# ì¤‘ë³µ ì§ˆë¬¸ ì²´í¬\n",
        "duplicate_questions = test_df[test_df.duplicated(['question'], keep=False)]\n",
        "if len(duplicate_questions) > 0:\n",
        "    print(f\"âš ï¸ ì¤‘ë³µ ì§ˆë¬¸ ë°œê²¬: {len(duplicate_questions)}ê°œ\")\n",
        "else:\n",
        "    print(\"âœ… ì¤‘ë³µ ì§ˆë¬¸ ì—†ìŒ\")\n",
        "\n",
        "# ë¹ˆ ê°’ ì²´í¬\n",
        "empty_questions = test_df[test_df['question'].str.strip() == '']\n",
        "empty_answers = test_df[test_df['ground_truth'].str.strip() == '']\n",
        "\n",
        "if len(empty_questions) > 0 or len(empty_answers) > 0:\n",
        "    print(f\"âš ï¸ ë¹ˆ ê°’ ë°œê²¬ - ì§ˆë¬¸: {len(empty_questions)}ê°œ, ë‹µë³€: {len(empty_answers)}ê°œ\")\n",
        "else:\n",
        "    print(\"âœ… ëª¨ë“  ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë¨\")\n",
        "\n",
        "# ========== 12. ìƒ˜í”Œ í‰ê°€ìš© í•¨ìˆ˜ (ì„ íƒì‚¬í•­) ==========\n",
        "def evaluate_qa_quality(question: str, answer: str, context: str) -> Dict:\n",
        "    \"\"\"ê°„ë‹¨í•œ QA í’ˆì§ˆ í‰ê°€\"\"\"\n",
        "    # ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•˜ëŠ”ì§€ ì²´í¬\n",
        "    context_words = set(context.lower().split())\n",
        "    answer_words = set(answer.lower().split())\n",
        "\n",
        "    # ë‹µë³€ê³¼ ì»¨í…ìŠ¤íŠ¸ì˜ ë‹¨ì–´ ê²¹ì¹¨ ë¹„ìœ¨\n",
        "    overlap_ratio = len(answer_words & context_words) / len(answer_words) if answer_words else 0\n",
        "\n",
        "    return {\n",
        "        \"question_length\": len(question),\n",
        "        \"answer_length\": len(answer),\n",
        "        \"context_overlap\": overlap_ratio,\n",
        "        \"is_complete\": '?' in question and len(answer) > 10\n",
        "    }\n",
        "\n",
        "# ìƒ˜í”Œ í‰ê°€\n",
        "print(\"\\n===== ìƒ˜í”Œ í’ˆì§ˆ í‰ê°€ (ì²˜ìŒ 3ê°œ) =====\")\n",
        "for idx, row in test_df.head(3).iterrows():\n",
        "    quality = evaluate_qa_quality(\n",
        "        row['question'],\n",
        "        row['ground_truth'],\n",
        "        row['contexts'][0] if row['contexts'] else \"\"\n",
        "    )\n",
        "    print(f\"\\nì¼€ì´ìŠ¤ {idx+1}:\")\n",
        "    print(f\"  ì»¨í…ìŠ¤íŠ¸ ê²¹ì¹¨: {quality['context_overlap']:.2%}\")\n",
        "    print(f\"  ì™„ì„±ë„: {'âœ…' if quality['is_complete'] else 'âš ï¸'}\")\n",
        "\n",
        "print(\"\\nğŸ‰ í•©ì„± í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!\")\n",
        "print(f\"ì´ {len(test_df)}ê°œì˜ ê³ í’ˆì§ˆ QA ìŒì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
