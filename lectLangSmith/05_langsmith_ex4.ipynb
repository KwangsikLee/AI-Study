{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sepibgOWJSPR",
        "outputId": "ede5b27d-6368-49e9-9116-9cfff08752de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/74.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/377.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m368.6/377.0 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# ── 설치(최초 1회) ─────────────────────────────────────────────\n",
        "!pip -q install -U langchain langchain-openai langsmith\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-NR6_RhJUfS"
      },
      "outputs": [],
      "source": [
        "# 1) (선택) OpenAI 사용 시\n",
        "import os, uuid\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "# OpenAI API 클라이언트 생성\n",
        "OPENAPI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "LangSmith_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "# 2) LangSmith 연동 필수 환경변수\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"      # 트레이싱 활성화\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"]   = \"https://api.smith.langchain.com\"  # 기본값\n",
        "os.environ[\"LANGSMITH_PROJECT\"]    = \"llm_colab_ex_4\"                 # 수업용 프로젝트명\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHJHLPTQJQiL",
        "outputId": "518f27fa-c342-4bf6-8c0d-1cde368da5e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▶ 짧은 텍스트 요약:\n",
            "LangSmith는 LLM 앱을 관찰, 평가 및 디버깅하는 플랫폼으로, 개발팀이 체인 단계별 로그를 추적하여 품질을 개선하는 데 도움을 준다.\n",
            "\n",
            "▶ 긴 텍스트 요약:\n",
            "LangSmith는 LLM 애플리케이션의 관찰, 평가, 디버깅을 지원하는 도구로, 대화형 에이전트나 RAG 파이프라인을 구축할 때 체인 단계를 기록하고 실패 지점 및 비효율적인 프롬프트/도구 호출을 추적할 수 있게 해준다. 긴 문서는 청킹하여 각 청크를 요약한 후, 이를 통합하는 방식으로 요약 품질과 추론 속도를 향상시킨다.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ── 임포트 ───────────────────────────────────────────────────\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.runnable import RunnableLambda, RunnableBranch, RunnablePassthrough\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# ── 1) 짧은 경로(원샷 요약) ───────────────────────────────────\n",
        "prompt_short = ChatPromptTemplate.from_template(\n",
        "    \"다음 텍스트를 한 문단으로 간결히 요약하라:\\n\\n{text}\"\n",
        ")\n",
        "short_chain = (\n",
        "    {\"text\": RunnableLambda(lambda x: x[\"text\"])}\n",
        "    | prompt_short\n",
        "    | llm\n",
        "    | parser\n",
        ")\n",
        "\n",
        "# ── 2) 긴 경로(Map-Reduce 요약) ──────────────────────────────\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=80)\n",
        "\n",
        "# map 단계: 청크별 요약\n",
        "prompt_map = ChatPromptTemplate.from_template(\n",
        "    \"다음 청크를 한 문장으로 요약하라:\\n\\n{chunk}\"\n",
        ")\n",
        "map_chain = prompt_map | llm | parser\n",
        "\n",
        "# reduce 단계: 부분요약들을 통합\n",
        "prompt_reduce = ChatPromptTemplate.from_template(\n",
        "    \"다음 부분 요약들을 통합해 한 문단으로 명확히 요약하라:\\n\\n{summaries}\"\n",
        ")\n",
        "reduce_chain = prompt_reduce | llm | parser\n",
        "\n",
        "# 긴 경로 체인\n",
        "long_chain = (\n",
        "    # 1) 텍스트 가져오기\n",
        "    {\"text\": RunnableLambda(lambda x: x[\"text\"])}\n",
        "    # 2) 청킹\n",
        "    | RunnableLambda(lambda d: splitter.split_text(d[\"text\"]))\n",
        "    # 3) 청크 리스트 → [{chunk: ...}, ...]로 변환\n",
        "    | RunnableLambda(lambda chunks: [{\"chunk\": c} for c in chunks])\n",
        "    # 4) 각 청크에 map_chain 적용 → 부분 요약 리스트\n",
        "    | map_chain.map()\n",
        "    # 5) 부분 요약을 합쳐 reduce 입력 만들기\n",
        "    | RunnableLambda(lambda partials: {\"summaries\": \"\\n\".join(partials)})\n",
        "    # 6) reduce 실행\n",
        "    | reduce_chain\n",
        ")\n",
        "\n",
        "# ── 3) 라우터: 길이 기준 분기 (예: 400자 이하 = 짧은 경로) ───\n",
        "router = RunnableBranch(\n",
        "    (lambda x: len(x.get(\"text\",\"\")) <= 400, short_chain),\n",
        "    long_chain  # else\n",
        ")\n",
        "\n",
        "# ── 4) 실행 ───────────────────────────────────────────────────\n",
        "short_text = \"LangSmith는 LLM 앱을 관찰/평가/디버깅하는 플랫폼이다. 개발팀은 체인 단계별 로그를 추적해 품질을 개선한다.\"\n",
        "long_text = \"\"\"\\\n",
        "LangSmith는 LLM 애플리케이션을 관찰하고 평가하며 디버깅할 수 있게 도와주는 도구다.\n",
        "대화형 에이전트나 RAG 파이프라인을 만들 때, LangSmith를 통해 체인 단계를 투명하게 기록하고,\n",
        "어디서 실패하는지, 어떤 프롬프트/도구 호출이 비효율적인지 추적할 수 있다.\n",
        "길이가 긴 문서는 보통 청킹을 해서 각 청크를 요약한 뒤, 부분 요약들을 통합하는 방식(map-reduce)으로 요약 품질과 추론 속도를 모두 확보한다.\n",
        "\"\"\"\n",
        "\n",
        "cfg = {\"tags\":[\"demo\",\"routing\",\"summarization\"], \"run_name\":\"length_based_summarizer\"}\n",
        "\n",
        "print(\"▶ 짧은 텍스트 요약:\")\n",
        "print(router.invoke({\"text\": short_text}, config=cfg))\n",
        "\n",
        "print(\"\\n▶ 긴 텍스트 요약:\")\n",
        "print(router.invoke({\"text\": long_text}, config=cfg))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
