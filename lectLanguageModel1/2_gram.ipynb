{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtcw5i4VT8Yc",
        "outputId": "6261a720-cc5a-4ee3-c8e8-ce2bf9d746ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('더', '행복할') 확률: 0.5\n",
            "('더', '많이') 확률: 0.5\n"
          ]
        }
      ],
      "source": [
        "# 예제 1: N-그램(2-gram) 언어 모델 - 한글 문장 확률 계산\n",
        "from collections import defaultdict\n",
        "\n",
        "# 예시 텍스트(작은 가사 일부)\n",
        "text = \"내일은 더 행복할 거야 오늘보다 더 많이 웃을 거야\"\n",
        "\n",
        "# 2-gram(바이그램) 모델 생성\n",
        "words = text.split()\n",
        "bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
        "\n",
        "# 바이그램 확률 딕셔너리 만들기\n",
        "bigram_counts = defaultdict(int)\n",
        "unigram_counts = defaultdict(int)\n",
        "for w1, w2 in bigrams:\n",
        "    bigram_counts[(w1, w2)] += 1\n",
        "    unigram_counts[w1] += 1\n",
        "\n",
        "# 조건부 확률 계산 함수\n",
        "def bigram_prob(w1, w2):\n",
        "    return bigram_counts[(w1, w2)] / unigram_counts[w1]\n",
        "\n",
        "# 실제로 확률 계산해보기\n",
        "print(f\"('더', '행복할') 확률:\", bigram_prob('더', '행복할'))\n",
        "print(f\"('더', '많이') 확률:\", bigram_prob('더', '많이'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-l3W3mFadhN",
        "outputId": "f9f88243-47ba-4399-87d4-4dabad1c0236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== 2-gram 기반 문장 생성 예시 ==\n",
            "행복은 있다\n",
            "\n",
            "== 다음 단어 예측 예시 ==\n",
            "'오늘도' 다음에 올 가능성 높은 단어:\n",
            "  밝은  (P=0.053)\n",
            "  떠오른다  (P=0.026)\n",
            "  노력의  (P=0.026)\n",
            "  성공은  (P=0.026)\n",
            "  한다  (P=0.026)\n",
            "'성공은' 다음에 올 가능성 높은 단어:\n",
            "  노력의  (P=0.053)\n",
            "  떠오른다  (P=0.026)\n",
            "  성공은  (P=0.026)\n",
            "  한다  (P=0.026)\n",
            "  날이다  (P=0.026)\n",
            "'고난은' 다음에 올 가능성 높은 단어:\n",
            "  성장의  (P=0.053)\n",
            "  떠오른다  (P=0.026)\n",
            "  노력의  (P=0.026)\n",
            "  성공은  (P=0.026)\n",
            "  한다  (P=0.026)\n",
            "\n",
            "== 특정 bigram 확률 ==\n",
            "P(소소한|행복은) = 0.053\n",
            "P(순간이|모든) = 0.053\n",
            "P(성장의|고난은) = 0.053\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict, Counter\n",
        "import random\n",
        "\n",
        "# [1] 예시 데이터 (다양한 뉴스/가사/명언 등 복수 문장)\n",
        "corpus = [\n",
        "    \"오늘도 밝은 태양이 떠오른다\",\n",
        "    \"희망찬 아침이 시작된다\",\n",
        "    \"슬픔은 이겨내야 한다\",\n",
        "    \"행복은 소소한 곳에 있다\",\n",
        "    \"성공은 노력의 결과이다\",\n",
        "    \"모든 순간이 기회다\",\n",
        "    \"행복을 찾는 여행을 시작해봐\",\n",
        "    \"고난은 성장의 밑거름이 된다\",\n",
        "    \"오늘은 내일의 꿈을 준비하는 날이다\",\n",
        "    \"삶은 도전과 응전의 연속이다\"\n",
        "]\n",
        "\n",
        "# [2] 2-gram 카운트/확률 테이블 구축 (Laplace smoothing)\n",
        "bigram_counts = defaultdict(lambda: defaultdict(int))\n",
        "unigram_counts = Counter()\n",
        "vocab = set()\n",
        "\n",
        "for sent in corpus:\n",
        "    tokens = sent.split()\n",
        "    vocab.update(tokens)\n",
        "    for i in range(len(tokens)-1):\n",
        "        w1, w2 = tokens[i], tokens[i+1]\n",
        "        bigram_counts[w1][w2] += 1\n",
        "        unigram_counts[w1] += 1\n",
        "    unigram_counts[tokens[-1]] += 1  # 마지막 단어\n",
        "\n",
        "vocab = list(vocab)\n",
        "V = len(vocab)  # 단어 개수\n",
        "\n",
        "def bigram_prob(w1, w2, alpha=1):\n",
        "    # Laplace smoothing\n",
        "    return (bigram_counts[w1][w2] + alpha) / (unigram_counts[w1] + alpha*V)\n",
        "\n",
        "# [3] 문장 확장/생성 함수\n",
        "def generate_sentence(seed, length=7):\n",
        "    tokens = seed.split()\n",
        "    for _ in range(length):\n",
        "        w1 = tokens[-1]\n",
        "        candidates = [(w2, bigram_prob(w1, w2)) for w2 in vocab]\n",
        "        # 확률에 따라 단어 샘플링 (softmax 아님, 실제론 분포대로 샘플링)\n",
        "        candidates = sorted(candidates, key=lambda x: -x[1])\n",
        "        w2 = random.choices([w for w, p in candidates], [p for w, p in candidates])[0]\n",
        "        tokens.append(w2)\n",
        "        if w2.endswith('다') or w2.endswith('다.'):  # 간단한 문장 마침 처리\n",
        "            break\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# [4] 다음 단어 예측 함수 (확률 순위 TOP 5)\n",
        "def predict_next(word, topk=5):\n",
        "    cands = [(w2, bigram_prob(word, w2)) for w2 in vocab]\n",
        "    cands = sorted(cands, key=lambda x: -x[1])[:topk]\n",
        "    print(f\"'{word}' 다음에 올 가능성 높은 단어:\")\n",
        "    for w2, p in cands:\n",
        "        print(f\"  {w2}  (P={p:.3f})\")\n",
        "\n",
        "# [5] 테스트\n",
        "print(\"== 2-gram 기반 문장 생성 예시 ==\")\n",
        "print(generate_sentence(\"행복은\"))\n",
        "\n",
        "print(\"\\n== 다음 단어 예측 예시 ==\")\n",
        "predict_next(\"오늘도\")\n",
        "predict_next(\"성공은\")\n",
        "predict_next(\"고난은\")\n",
        "\n",
        "# [6] 2-gram 확률 확인 예시\n",
        "print(\"\\n== 특정 bigram 확률 ==\")\n",
        "w1, w2 = \"행복은\", \"소소한\"\n",
        "print(f\"P({w2}|{w1}) =\", round(bigram_prob(w1, w2), 3))\n",
        "w1, w2 = \"모든\", \"순간이\"\n",
        "print(f\"P({w2}|{w1}) =\", round(bigram_prob(w1, w2), 3))\n",
        "w1, w2 = \"고난은\", \"성장의\"\n",
        "print(f\"P({w2}|{w1}) =\", round(bigram_prob(w1, w2), 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZzYxgmhauQj",
        "outputId": "ebefa0d6-d80a-4645-c2eb-62f2a765b0f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== 문장 자동완성 ==\n",
            "행복은 마음속에 있어요 보물입니다\n",
            "오늘도 좋은 하루 되세요 보물입니다\n",
            "고마운 마음을 잊지 마세요 보물입니다\n",
            "친구는 소중한 보물입니다\n",
            "\n",
            "== 다음 단어 추천 ==\n",
            "입력: '내일은 더' → 추천 단어:\n",
            "  나아질 (확률=0.051)\n",
            "  보물입니다 (확률=0.026)\n",
            "  사랑은 (확률=0.026)\n",
            "  모든 (확률=0.026)\n",
            "  마음속에 (확률=0.026)\n",
            "입력: '희망은' → 추천 단어:\n",
            "  포기하지 (확률=0.051)\n",
            "  보물입니다 (확률=0.026)\n",
            "  나아질 (확률=0.026)\n",
            "  사랑은 (확률=0.026)\n",
            "  모든 (확률=0.026)\n",
            "입력: '용기는 두려움을' → 추천 단어:\n",
            "  이기는 (확률=0.051)\n",
            "  보물입니다 (확률=0.026)\n",
            "  나아질 (확률=0.026)\n",
            "  사랑은 (확률=0.026)\n",
            "  모든 (확률=0.026)\n",
            "\n",
            "== 새로운 문장 자동완성 ==\n",
            "'희망은 포기하지' → 희망은 포기하지 않는 마음입니다\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict, Counter\n",
        "import random\n",
        "\n",
        "# [1] 복수 문장 데이터 (한글 뉴스/가사/명언 등)\n",
        "corpus = [\n",
        "    \"지금 이 순간을 소중히 하세요\",\n",
        "    \"행복은 마음속에 있어요\",\n",
        "    \"내일은 더 나아질 거예요\",\n",
        "    \"고마운 마음을 잊지 마세요\",\n",
        "    \"친구는 소중한 보물입니다\",\n",
        "    \"희망은 포기하지 않는 마음입니다\",\n",
        "    \"슬픔은 곧 지나갈 거예요\",\n",
        "    \"오늘도 좋은 하루 되세요\",\n",
        "    \"사랑은 모든 것을 이겨냅니다\",\n",
        "    \"용기는 두려움을 이기는 힘입니다\"\n",
        "]\n",
        "\n",
        "# [2] 2-gram & 3-gram 테이블 구축\n",
        "bigram = defaultdict(lambda: defaultdict(int))\n",
        "trigram = defaultdict(lambda: defaultdict(int))\n",
        "unigram = Counter()\n",
        "vocab = set()\n",
        "\n",
        "for sent in corpus:\n",
        "    tokens = sent.split()\n",
        "    vocab.update(tokens)\n",
        "    for i in range(len(tokens)-1):\n",
        "        w1, w2 = tokens[i], tokens[i+1]\n",
        "        bigram[w1][w2] += 1\n",
        "        unigram[w1] += 1\n",
        "    for i in range(len(tokens)-2):\n",
        "        w1, w2, w3 = tokens[i], tokens[i+1], tokens[i+2]\n",
        "        trigram[(w1, w2)][w3] += 1\n",
        "    unigram[tokens[-1]] += 1\n",
        "\n",
        "vocab = list(vocab)\n",
        "V = len(vocab)\n",
        "\n",
        "def bigram_prob(w1, w2, alpha=1):\n",
        "    return (bigram[w1][w2] + alpha) / (unigram[w1] + alpha*V)\n",
        "\n",
        "def trigram_prob(w1, w2, w3, alpha=1):\n",
        "    return (trigram[(w1, w2)][w3] + alpha) / (bigram[w1][w2] + alpha*V)\n",
        "\n",
        "# [3] 문장 자동완성/생성 함수 (N-gram 혼합, 우선 trigram → bigram)\n",
        "def complete_sentence(seed, max_length=8):\n",
        "    tokens = seed.split()\n",
        "    while len(tokens) < max_length:\n",
        "        if len(tokens) >= 2:\n",
        "            w1, w2 = tokens[-2], tokens[-1]\n",
        "            candidates = [(w3, trigram_prob(w1, w2, w3)) for w3 in vocab]\n",
        "            candidates = sorted(candidates, key=lambda x: -x[1])\n",
        "            next_word = candidates[0][0] if candidates[0][1] > 0 else None\n",
        "        else:\n",
        "            w1 = tokens[-1]\n",
        "            candidates = [(w2, bigram_prob(w1, w2)) for w2 in vocab]\n",
        "            candidates = sorted(candidates, key=lambda x: -x[1])\n",
        "            next_word = candidates[0][0] if candidates[0][1] > 0 else None\n",
        "        if not next_word:\n",
        "            break\n",
        "        tokens.append(next_word)\n",
        "        if next_word.endswith('다') or next_word.endswith('다.'):\n",
        "            break\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# [4] 자동완성 추천 함수 (n-gram 기반 다음 단어 추천 TOP 5)\n",
        "def suggest_next(seed, topk=5):\n",
        "    tokens = seed.split()\n",
        "    if len(tokens) >= 2:\n",
        "        w1, w2 = tokens[-2], tokens[-1]\n",
        "        cands = [(w3, trigram_prob(w1, w2, w3)) for w3 in vocab]\n",
        "        cands = sorted(cands, key=lambda x: -x[1])[:topk]\n",
        "    else:\n",
        "        w1 = tokens[-1]\n",
        "        cands = [(w2, bigram_prob(w1, w2)) for w2 in vocab]\n",
        "        cands = sorted(cands, key=lambda x: -x[1])[:topk]\n",
        "    print(f\"입력: '{seed}' → 추천 단어:\")\n",
        "    for w, p in cands:\n",
        "        if p > 0:\n",
        "            print(f\"  {w} (확률={p:.3f})\")\n",
        "\n",
        "# [5] 실습 테스트\n",
        "print(\"== 문장 자동완성 ==\")\n",
        "print(complete_sentence(\"행복은\"))\n",
        "\n",
        "print(complete_sentence(\"오늘도 좋은\"))\n",
        "print(complete_sentence(\"고마운 마음을\"))\n",
        "print(complete_sentence(\"친구는\"))\n",
        "\n",
        "print(\"\\n== 다음 단어 추천 ==\")\n",
        "suggest_next(\"내일은 더\")\n",
        "suggest_next(\"희망은\")\n",
        "suggest_next(\"용기는 두려움을\")\n",
        "\n",
        "print(\"\\n== 새로운 문장 자동완성 ==\")\n",
        "start = \"희망은 포기하지\"\n",
        "print(f\"'{start}' →\", complete_sentence(start))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hf_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
