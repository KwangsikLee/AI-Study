{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VS8mJfYNS5no"
   },
   "outputs": [],
   "source": [
    "# [1] 설치\n",
    "!pip install transformers torch scikit-learn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "dTZ3s_c9SmQq",
    "outputId": "400eb41d-13f0-4bfd-ef51-5498ef1ab52c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:02, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.684100</td>\n",
       "      <td>0.719197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.644300</td>\n",
       "      <td>0.814952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.564600</td>\n",
       "      <td>0.798058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.579700</td>\n",
       "      <td>0.743914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.550500</td>\n",
       "      <td>0.669864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 검증 데이터 예측 결과 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제 라벨: [1, 0]\n",
      "예측 라벨: [1, 0]\n",
      "\n",
      "--- 새로운 가사 감성 예측 ---\n",
      "\"이별의 슬픔이 내 마음을 적셔\" => 부정 (긍정 확률: 0.41)\n",
      "\"희망찬 내일이 기다리고 있어\" => 부정 (긍정 확률: 0.47)\n",
      "\"내일은 더 행복할 거야\" => 긍정 (긍정 확률: 0.61)\n",
      "\"눈물로 가득한 하루\" => 부정 (긍정 확률: 0.46)\n"
     ]
    }
   ],
   "source": [
    "# [2] 데이터셋 만들기 (간단 예시, 확장 가능)\n",
    "lyrics = [\n",
    "    \"행복했던 기억이 가득해\",        # 긍정(1)\n",
    "    \"너와의 이별이 너무 아파\",        # 부정(0)\n",
    "    \"사랑은 다시 찾아올 거야\",       # 긍정(1)\n",
    "    \"외로운 밤 혼자 울고 있어\",      # 부정(0)\n",
    "    \"햇살처럼 눈부신 하루야\",        # 긍정(1)\n",
    "    \"추억만이 남아 나를 괴롭혀\",     # 부정(0)\n",
    "    \"새로운 시작이 설레여\",         # 긍정(1)\n",
    "    \"상처뿐인 기억들이 떠올라\",      # 부정(0)\n",
    "    \"희망을 안고 달려가\",            # 긍정(1)\n",
    "    \"어둠 속에 방황하는 나\"          # 부정(0)\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1=긍정, 0=부정\n",
    "\n",
    "# [3] 토크나이저/데이터셋 준비\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# MPS 디바이스 설정 (Apple Silicon GPU 사용시 오류 방지)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    # MPS에서 발생할 수 있는 문제 방지\n",
    "    torch.mps.empty_cache()\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base-v2022\")\n",
    "\n",
    "# 토큰화 함수\n",
    "def preprocess(texts, labels):\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=32)\n",
    "    return {\n",
    "        'input_ids': torch.tensor(encodings['input_ids']),\n",
    "        'attention_mask': torch.tensor(encodings['attention_mask']),\n",
    "        'labels': torch.tensor(labels)\n",
    "    }\n",
    "\n",
    "# train/val 나누기\n",
    "X_train, X_val, y_train, y_val = train_test_split(lyrics, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_enc = preprocess(X_train, y_train)\n",
    "val_enc = preprocess(X_val, y_val)\n",
    "\n",
    "# [4] PyTorch Dataset 래퍼\n",
    "class LyricsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_dataset = LyricsDataset(train_enc)\n",
    "val_dataset = LyricsDataset(val_enc)\n",
    "\n",
    "# [5] BERT 분류 모델 불러오기 (2 클래스: 긍정/부정)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"beomi/KcELECTRA-base-v2022\", num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# [6] Trainer로 학습\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./result',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='no',\n",
    "    load_best_model_at_end=False,\n",
    "    logging_dir='./logs',\n",
    "    seed=42,\n",
    "    report_to='none', # Explicitly disable wandb reporting\n",
    "    use_mps_device=False if device.type == \"mps\" else None,  # MPS 호환성을 위해 비활성화\n",
    "    dataloader_pin_memory=False  # MPS에서 메모리 문제 방지\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# [7] 평가 및 새 가사 예측\n",
    "print(\"\\n--- 검증 데이터 예측 결과 ---\")\n",
    "outputs = trainer.predict(val_dataset)\n",
    "preds = np.argmax(outputs.predictions, axis=1)\n",
    "print(\"실제 라벨:\", y_val)\n",
    "print(\"예측 라벨:\", preds.tolist())\n",
    "\n",
    "# 새로운 가사 감정 예측\n",
    "def predict_sentiment(lyrics_list):\n",
    "    enc = tokenizer(lyrics_list, return_tensors='pt', truncation=True, padding=True, max_length=32)\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}  # 디바이스로 이동\n",
    "    with torch.no_grad():\n",
    "        out = model(**enc)\n",
    "        probs = torch.softmax(out.logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "    for lyric, pred, prob in zip(lyrics_list, preds.cpu(), probs.cpu()):\n",
    "        print(f'\"{lyric}\" => {\"긍정\" if pred==1 else \"부정\"} (긍정 확률: {prob[1]:.2f})')\n",
    "\n",
    "print(\"\\n--- 새로운 가사 감성 예측 ---\")\n",
    "test_lyrics = [\n",
    "    \"이별의 슬픔이 내 마음을 적셔\",\n",
    "    \"희망찬 내일이 기다리고 있어\",\n",
    "    \"내일은 더 행복할 거야\",\n",
    "    \"눈물로 가득한 하루\"\n",
    "]\n",
    "predict_sentiment(test_lyrics)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
