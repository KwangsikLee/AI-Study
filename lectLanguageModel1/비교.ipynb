{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dTSkDvZcOYQ",
        "outputId": "a570ebdb-64f8-43cd-cc2c-ebeb8e50affd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[알림] Mecab 사용 불가 → Okt로 진행합니다.\n",
            "사용 형태소 분석기: okt\n",
            "[whitespace] [('어제 비가 왔지만 우리는 야외 공연을 강행했다.', 0.4364), ('콘서트가 예정대로 진행됐다는 뉴스가 보도됐다.', 0.2582), ('한국어는 조사·어미 때문에 영어보다 토큰화가 어렵다.', 0.0), ('SNS 텍스트 전처리에는 이모지와 줄임말 처리가 중요하다.', 0.0), ('Mecab은 빠르고 정확하지만 설치가 까다롭다.', 0.0)]\n",
            "[regex_ko] [('어제 비가 왔지만 우리는 야외 공연을 강행했다.', 0.4364), ('콘서트가 예정대로 진행됐다는 뉴스가 보도됐다.', 0.2582), ('한국어는 조사·어미 때문에 영어보다 토큰화가 어렵다.', 0.0), ('SNS 텍스트 전처리에는 이모지와 줄임말 처리가 중요하다.', 0.0), ('Mecab은 빠르고 정확하지만 설치가 까다롭다.', 0.0)]\n",
            "[morph   ] [('콘서트가 예정대로 진행됐다는 뉴스가 보도됐다.', 0.5657), ('어제 비가 왔지만 우리는 야외 공연을 강행했다.', 0.1751), ('한국어는 조사·어미 때문에 영어보다 토큰화가 어렵다.', 0.0), ('SNS 텍스트 전처리에는 이모지와 줄임말 처리가 중요하다.', 0.0), ('Mecab은 빠르고 정확하지만 설치가 까다롭다.', 0.0)]\n",
            "[sp      ] [('콘서트가 예정대로 진행됐다는 뉴스가 보도됐다.', 0.6895), ('어제 비가 왔지만 우리는 야외 공연을 강행했다.', 0.5661), ('형태소 기반으로 명사와 동사를 중점적으로 본다.', 0.3069), ('Mecab은 빠르고 정확하지만 설치가 까다롭다.', 0.3022), ('서브워드 방식은 OOV 문제를 완화한다.', 0.2617)]\n"
          ]
        }
      ],
      "source": [
        "# --- 설치 ---\n",
        "!pip -q install konlpy mecab-python3 sentencepiece pandas scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "사용 형태소 분석기: mecab\n",
            "[whitespace] [('어제 비가 왔지만 우리는 야외 공연을 강행했다.', 0.4364), ('콘서트가 예정대로 진행됐다는 뉴스가 보도됐다.', 0.2582), ('SNS 텍스트 전처리에는 이모지와 줄임말 처리가 중요하다.', 0.0), ('한국어는 조사·어미 때문에 영어보다 토큰화가 어렵다.', 0.0), ('Mecab은 빠르고 정확하지만 설치가 까다롭다.', 0.0)]\n",
            "[regex_ko] [('어제 비가 왔지만 우리는 야외 공연을 강행했다.', 0.4364), ('콘서트가 예정대로 진행됐다는 뉴스가 보도됐다.', 0.2582), ('SNS 텍스트 전처리에는 이모지와 줄임말 처리가 중요하다.', 0.0), ('한국어는 조사·어미 때문에 영어보다 토큰화가 어렵다.', 0.0), ('Mecab은 빠르고 정확하지만 설치가 까다롭다.', 0.0)]\n",
            "[morph   ] [('콘서트가 예정대로 진행됐다는 뉴스가 보도됐다.', 0.6972), ('어제 비가 왔지만 우리는 야외 공연을 강행했다.', 0.1356), ('Mecab은 빠르고 정확하지만 설치가 까다롭다.', 0.1356), ('SNS 텍스트 전처리에는 이모지와 줄임말 처리가 중요하다.', 0.0), ('한국어는 조사·어미 때문에 영어보다 토큰화가 어렵다.', 0.0)]\n",
            "[sp      ] [('콘서트가 예정대로 진행됐다는 뉴스가 보도됐다.', 0.6895), ('어제 비가 왔지만 우리는 야외 공연을 강행했다.', 0.5661), ('형태소 기반으로 명사와 동사를 중점적으로 본다.', 0.3069), ('Mecab은 빠르고 정확하지만 설치가 까다롭다.', 0.3022), ('서브워드 방식은 OOV 문제를 완화한다.', 0.2617)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: corpus.txt\n",
            "  input_format: \n",
            "  model_prefix: sp_search\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 97\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: corpus.txt\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 7 sentences\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=184\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=88\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 7 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=41\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 96 seed sentencepieces\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 7\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 42\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 42 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=89 obj=16.2351 num_tokens=167 num_tokens/piece=1.8764\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=89 obj=16.8676 num_tokens=167 num_tokens/piece=1.8764\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: sp_search.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: sp_search.vocab\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from konlpy.tag import Okt\n",
        "import sentencepiece as spm\n",
        "import re, numpy as np, pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# --- 형태소 준비(Mecab→Okt 폴백) ---\n",
        "tokenizer_name = \"okt\"; okt = Okt(); mecab=None\n",
        "try:\n",
        "    from konlpy.tag import Mecab\n",
        "    mecab = Mecab(); tokenizer_name = \"mecab\"\n",
        "except:\n",
        "    print(\"[알림] Mecab 사용 불가 → Okt로 진행합니다.\")\n",
        "print(\"사용 형태소 분석기:\", tokenizer_name)\n",
        "\n",
        "def tok_ws(text): return text.split()\n",
        "def tok_regex_ko(text):\n",
        "    s = re.sub(r\"[^가-힣A-Za-z0-9\\s]\", \" \", text)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s.split()\n",
        "def tok_morph(text):\n",
        "    if tokenizer_name==\"mecab\": return [t for t in mecab.morphs(text) if len(t)>=2]\n",
        "    else: return [t for t in okt.morphs(text, stem=True) if len(t)>=2]\n",
        "\n",
        "# --- SentencePiece 학습(간단 Unigram) ---\n",
        "CORPUS = [\n",
        "    \"오늘은 비가 왔지만 콘서트는 예정대로 진행됐다.\",\n",
        "    \"신조어와 줄임말이 많은 SNS 텍스트는 전처리가 중요하다.\",\n",
        "    \"한국어 토큰화는 조사와 어미 때문에 어렵다.\",\n",
        "    \"메캅은 빠르고 정확하다는 평가가 많다.\",\n",
        "    \"서브워드는 OOV를 줄이는 데 유리하다.\",\n",
        "    \"텍스트 마이닝에서는 불용어 제거가 자주 사용된다.\",\n",
        "    \"형태소 분석으로 품사 정보를 활용할 수 있다.\",\n",
        "]\n",
        "open(\"corpus.txt\",\"w\",encoding=\"utf-8\").write(\"\\n\".join(CORPUS))\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input=\"corpus.txt\",\n",
        "    model_prefix=\"sp_search\",\n",
        "    model_type=\"unigram\",\n",
        "    vocab_size=97, # Reduced vocab_size\n",
        "    character_coverage=0.9995\n",
        ")\n",
        "sp = spm.SentencePieceProcessor(); sp.load(\"sp_search.model\")\n",
        "def tok_sp(text): return sp.EncodeAsPieces(text)\n",
        "\n",
        "# --- 검색 코퍼스(문서들) ---\n",
        "DOCS = [\n",
        "    \"어제 비가 왔지만 우리는 야외 공연을 강행했다.\",\n",
        "    \"SNS 텍스트 전처리에는 이모지와 줄임말 처리가 중요하다.\",\n",
        "    \"한국어는 조사·어미 때문에 영어보다 토큰화가 어렵다.\",\n",
        "    \"Mecab은 빠르고 정확하지만 설치가 까다롭다.\",\n",
        "    \"서브워드 방식은 OOV 문제를 완화한다.\",\n",
        "    \"형태소 기반으로 명사와 동사를 중점적으로 본다.\",\n",
        "    \"전처리에서 불용어를 제거하면 잡음이 줄어든다.\",\n",
        "    \"콘서트가 예정대로 진행됐다는 뉴스가 보도됐다.\",\n",
        "]\n",
        "\n",
        "# --- 토크나이저별 TF-IDF 검색 ---\n",
        "def search(query, docs, tok_fn):\n",
        "    vec = TfidfVectorizer(token_pattern=r\"[^ ]+\")\n",
        "    X = vec.fit_transform(\" \".join(tok_fn(d)) for d in docs)\n",
        "    q = vec.transform([\" \".join(tok_fn(query))])\n",
        "    sims = (q @ X.T).toarray().ravel()\n",
        "    idx = np.argsort(-sims)\n",
        "    return [(docs[i], round(float(sims[i]),4)) for i in idx[:5]]\n",
        "\n",
        "query = \"비가 왔지만 콘서트는 예정대로 진행\"\n",
        "print(\"[whitespace]\", search(query, DOCS, tok_ws))\n",
        "print(\"[regex_ko]\",  search(query, DOCS, tok_regex_ko))\n",
        "print(\"[morph   ]\",  search(query, DOCS, tok_morph))\n",
        "print(\"[sp      ]\",  search(query, DOCS, tok_sp))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hf_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
